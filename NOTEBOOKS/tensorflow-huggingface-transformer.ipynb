{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Ha77U6xSfR"
      },
      "source": [
        "# Introduction\n",
        "In this notebook we use the **transformers library**. **Transformers library** is designed by Hugging-face. Hugging Face is an NLP-focused startup with a large open-source community. **Transformers** is a python-based library that exposes an API to use many well-known transformer architectures, such as BERT, RoBERTa, GPT-2 or DistilBERT.<br><br>\n",
        "In this chapter we use the BERT hugging-face model to classify email as spam or not spam. BERT is a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.<br><br>\n",
        "**In this notebook, we show how to fine-tune a Tensorflow HuggingFace transformer.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-zVoGaMxSfZ"
      },
      "source": [
        "# Functions for preprossesing text data.\n",
        "\n",
        "In this section, we give some functions for cleaning text data:\n",
        "* One function to remove HTML tags\n",
        "* One function to remove URLs\n",
        "* One function to remove emails\n",
        "\n",
        "Theoretically, It is not necessarily good to clean the text when we practice text mining with HuggingFace transformers. But in this notebook, I decided to clean a bit the text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:01:00.585259Z",
          "iopub.status.busy": "2021-11-04T08:01:00.584761Z",
          "iopub.status.idle": "2021-11-04T08:01:00.611308Z",
          "shell.execute_reply": "2021-11-04T08:01:00.610659Z",
          "shell.execute_reply.started": "2021-11-04T08:01:00.585179Z"
        },
        "id": "lCndgdbaxSfb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "def _remove_urls(x):\n",
        "    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n",
        "\n",
        "def _remove_emails(x):\n",
        "    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k6b_K6axSfe"
      },
      "source": [
        "# Fine tuned a pretrained model using hugging face transformer with tensorflow\n",
        "In this section, we fine-tune a **BERT** model from hugging-face with **tensorflow**. We use a BERT case model to detect spam emails. First, we import the dataset. The emails text are stored in the email column. The label column is equal to 0 for the valid emails and to 1 for the spams. Run the cell below to create the email dataset:\n",
        "\n",
        "## Dataset creation\n",
        "* We first import the data as a pandas dataset\n",
        "* Secondly, we convert the pandas dataset into a hugging-face dataset\n",
        "* Last, we split the hugging-face dataset into a training set and a test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:01:00.613112Z",
          "iopub.status.busy": "2021-11-04T08:01:00.612788Z",
          "iopub.status.idle": "2021-11-04T08:01:01.737556Z",
          "shell.execute_reply": "2021-11-04T08:01:01.736661Z",
          "shell.execute_reply.started": "2021-11-04T08:01:00.613076Z"
        },
        "id": "UPFNF0rlxSfg"
      },
      "outputs": [],
      "source": [
        "# We first import the data as a pandas dataset\n",
        "import pandas as pd\n",
        "spam_ham = pd.read_csv(\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/DATA/spam_ornot_spam.csv\")\n",
        "spam_ham['email'] = spam_ham['email'].apply(lambda x:remove_html(str(x)))\n",
        "spam_ham['email'] = spam_ham['email'].apply(lambda x:_remove_urls(str(x)))\n",
        "spam_ham['email'] = spam_ham['email'].apply(lambda x:_remove_emails(str(x)))\n",
        "spam_ham.head()\n",
        "\n",
        "spam_ham['email'] = spam_ham['email'].apply(lambda x:' '.join(x.split()[0:512]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:01:11.747125Z",
          "iopub.status.busy": "2021-11-04T08:01:11.746849Z",
          "iopub.status.idle": "2021-11-04T08:01:17.936411Z",
          "shell.execute_reply": "2021-11-04T08:01:17.935678Z",
          "shell.execute_reply.started": "2021-11-04T08:01:11.747097Z"
        },
        "id": "84IepK1ExSfi",
        "outputId": "7af325c4-a171-4183-f50e-01feecfeb5df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['email', 'label'],\n",
              "        num_rows: 2550\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['email', 'label'],\n",
              "        num_rows: 450\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "!pip install datasets  #Only if you are in colab. dataset is already installed in kaggle  \n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from datasets import DatasetDict\n",
        "\n",
        "# Secondly, we convert the pandas dataset into a hugging-face dataset\n",
        "dataset = Dataset.from_pandas(spam_ham)\n",
        "\n",
        "# Last, we split the hugging-face dataset into a training set and a test set\n",
        "dataset_train_test = dataset.train_test_split(test_size=0.15)\n",
        "dataset_train_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ozv0AV6xSfq"
      },
      "source": [
        "In the above cell, dataset_train_test is a HuggingFace dataset. In the following cells, we show how\n",
        "* to access the train part of dataset_train_test\n",
        "* to access the email feature of the the train part\n",
        "* access the label feature of the the train part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T09:42:36.636592Z",
          "iopub.status.busy": "2021-10-30T09:42:36.636292Z",
          "iopub.status.idle": "2021-10-30T09:42:36.642496Z",
          "shell.execute_reply": "2021-10-30T09:42:36.641912Z",
          "shell.execute_reply.started": "2021-10-30T09:42:36.636556Z"
        },
        "id": "rOJizWEPxSfr",
        "outputId": "d865e2fc-72b7-4358-e388-c92bb51b52e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['email', 'label'],\n",
              "    num_rows: 2550\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#access the train part of dataset_train_test\n",
        "dataset_train_test['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T09:46:08.027519Z",
          "iopub.status.busy": "2021-10-30T09:46:08.027226Z",
          "iopub.status.idle": "2021-10-30T09:46:08.05722Z",
          "shell.execute_reply": "2021-10-30T09:46:08.056388Z",
          "shell.execute_reply.started": "2021-10-30T09:46:08.02749Z"
        },
        "id": "YAcmjuvrxSft",
        "outputId": "75349d27-9183-4071-ebe8-ffe295ad20f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how about this a bored forker said yawn i believe tom had it right signal not noise i ll start los angeles times september NUMBER NUMBER monday copyright NUMBER los angeles times los angeles times september NUMBER NUMBER monday home edition section main news main news part NUMBER page NUMBER national desk length NUMBER words byline dana calvo times staff writer dateline seattle body the idea came at the end of a long frustrating brown bag session at a public policy think tank here the challenge was to save the city s child care programs staring into his empty coffee cup the meeting coordinator s mind landed on an unlikely solution put a tax just a benign dime a shot on espresso that led to a petition signed by more than NUMBER NUMBER seattle residents and next year voters will decide whether the tax becomes law one that taps right into seattle s legendary addiction to coffee this is after all the town where starbucks was born and where the NUMBER pound of beans became a staple there is one starbucks for every NUMBER NUMBER residents in seattle compared to one per NUMBER NUMBER in new york seattle also has two other major coffee chains tully s coffee and seattle s best coffee as well as countless cafes and espresso carts a recent poll showed that NUMBER of seattle residents would vote for the tax for people outside of seattle who don t understand the consumption of espresso the tax proposal can be seen as crazy said john burbank the think tank s executive director but it was common sense research by his nonprofit economic opportunity institute showed that people preferred a tax on liquor or beer over one on espresso but because of the large number of lattes and cappuccinos sold a tax on espresso could be lower than one levied on alcohol burbank estimates the tax could generate NUMBER million to NUMBER million a year city council aides dispute his figures saying their research shows the tax would bring in NUMBER NUMBER million to NUMBER million a year burbank s institute is funded by foundations and labor unions the think tank s mission is to promote public policy in the interests of low income people and it has long championed child care issues burbank says the tax would restore cuts to the child care programs made earlier this year by gov gary locke he also says it would provide more low income families with subsidies for child care improve preschool programs and increase teacher salaries at bauhaus books coffee the sidewalk is dotted with tables of customers for whom coffee is a half day activity not just a drink espresso lovers like chris altman who at a dime a day would spend an extra NUMBER NUMBER a year said the investment is worth it i m ok with it said the NUMBER year old stirring his iced latte the money s got to come from somewhere hope revuelto NUMBER was cooling her regular coffee NUMBER because she brought her own',\n",
              " 'gary lawrence murphy ok but only if you also meant religious and alcoholic extremes since it was my quip i ll point out that i used the term alcoholism implying addiction i drink i m not an alcoholic most people who drink don t go to aa meetings most people who go to aa meetings do try very hard not to drink as to religion i think it is harmful and risky in almost any degree were i single i might consider potential mates who partook of the less irrational or more light hearted religions a unitarian or buddhist might be an example of the first a wiccan of the second but someone who is both irrational and serious about that irrationality strikes me as a bad choice of partner moreso than someone who was addicted to some drug alcoholics and drug addicts at least have the sense to battle their problem and to keep their children from suffering it the religious revel in their irrationality and want to raise their children in it that s a difficult difference for two parents to reconcile fortunately i am long and happily enamored of someone who has no religious tendencies _________________________________________________________________ send and receive hotmail on your mobile device URL']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#access the email feature of the the train part\n",
        "dataset_train_test['train']['email'][0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-30T09:49:23.454302Z",
          "iopub.status.busy": "2021-10-30T09:49:23.453917Z",
          "iopub.status.idle": "2021-10-30T09:49:23.481508Z",
          "shell.execute_reply": "2021-10-30T09:49:23.480546Z",
          "shell.execute_reply.started": "2021-10-30T09:49:23.45424Z"
        },
        "id": "1fbpwLfMxSfv",
        "outputId": "0bf8f88c-6538-477d-eced-5634084c9cff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#access the label feature of the the train part\n",
        "dataset_train_test['train']['label'][0:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opNQMVAaxSfw"
      },
      "source": [
        "## Tokenization and vectorization of the text\n",
        "Remind that our goal is to fine-tune a BERT cased model from the hugging-face hub. A hugging-face model has two components:\n",
        "* the tokenizer component is responsable for the text vectorization\n",
        "* the model component is a pre-trained transformer that takes the tokenizer output as an input\n",
        "\n",
        "The tokenizer and model should always be paired. In the cell below we:\n",
        "* we import the tokenizer designed for the bert-cased model\n",
        "* We vectorize the texts from the emails  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:01:31.627809Z",
          "iopub.status.busy": "2021-11-04T08:01:31.627547Z",
          "iopub.status.idle": "2021-11-04T08:01:35.686729Z",
          "shell.execute_reply": "2021-11-04T08:01:35.686049Z",
          "shell.execute_reply.started": "2021-11-04T08:01:31.627781Z"
        },
        "id": "iKFfZabBxSfy",
        "outputId": "3aa80413-96d5-482f-95fc-4ea5c7fb9d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "fb9f34e403df4d389b8aa17ff767cfc2",
            "0ab98016f88544678e2898a8bbfd4995",
            "9b1c930d39d4451e88079bc89177a656",
            "132fd6cbf5fe476a8522a12ced085846",
            "e548a293d9e94c798639701ef40146ca",
            "6618db3a563a4f4ab415255c9864df97",
            "f1f5413b5c2f4918b129feabd3b979ad",
            "da3e7724eccb428fb3afe1d297ca607e",
            "840923520fd145fd9d0020a63d620565",
            "6c1f0961939447edb6fb287a384a6ce1",
            "1b3c01fc7ae747b28ccfd1e716add414",
            "f1d7f938c2f54e44b3a3eaee7d1d848e",
            "9619c29a6370477baf9674142ff5747d",
            "f62fd5c8db00487e9d0237dc21fa93e0",
            "094aef0bf7bf403d8ef252e78bd348b9",
            "2189efe634524fdf93a82dbecea5579c",
            "b620c57853654d70bde1633a21b1ab28",
            "6c49a39d49a243c9a12247a512de6055",
            "7ff5405a9f3740b6b2cea48a463797a8",
            "62892f83f4204998bf497ff804b85c03",
            "f9192df25b6749738d5582960cdd713f",
            "d04ba6aef8f54f1482322c1f7751f99e"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb9f34e403df4d389b8aa17ff767cfc2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1d7f938c2f54e44b3a3eaee7d1d848e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# We import the tokenizer designed for the bert-cased model\n",
        "\n",
        "#Import the transformer library only in Colab\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# We vectorize the texts from the emails\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"email\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset_train_test.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:01:53.40876Z",
          "iopub.status.busy": "2021-11-04T08:01:53.408513Z",
          "iopub.status.idle": "2021-11-04T08:01:53.413954Z",
          "shell.execute_reply": "2021-11-04T08:01:53.413299Z",
          "shell.execute_reply.started": "2021-11-04T08:01:53.408733Z"
        },
        "id": "loev_UtgxSfz",
        "outputId": "d7e7805a-5516-41de-da5c-da9823f2785a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['attention_mask', 'email', 'input_ids', 'label', 'token_type_ids'],\n",
              "        num_rows: 2550\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['attention_mask', 'email', 'input_ids', 'label', 'token_type_ids'],\n",
              "        num_rows: 450\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfD9rBDIxSf0"
      },
      "source": [
        "attention_mask,  input_ids, token_type_ids are vectors features made by the bert-cased tokenizer from the email feature. The bert-cased model uses these features and the label feature during the training stage we will implement later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVfWlqBTxSf1"
      },
      "source": [
        "## Convert the hugging-face dataset in standard tf.data.Dataset \n",
        "Since we will use a hugging-face Tensorflow model, we have to convert tokenized_datasets (a hugging-face dataset) in standard tf.data.Dataset. We first make the training dataset and the test dataset from the tokenized_datasets in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:02:00.121086Z",
          "iopub.status.busy": "2021-11-04T08:02:00.120556Z",
          "iopub.status.idle": "2021-11-04T08:02:00.125981Z",
          "shell.execute_reply": "2021-11-04T08:02:00.124044Z",
          "shell.execute_reply.started": "2021-11-04T08:02:00.121049Z"
        },
        "id": "c9yRK3nHxSf2"
      },
      "outputs": [],
      "source": [
        "train_dataset = tokenized_datasets['train']\n",
        "test_dataset = tokenized_datasets['test']  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j4k73WcxSf3"
      },
      "source": [
        "Since we need the features attention_mask, input_ids, token_type_ids, label for the training stage, we get rid of the email feature. We also use the with_format(\"tensorflow\"):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:02:05.733583Z",
          "iopub.status.busy": "2021-11-04T08:02:05.733329Z",
          "iopub.status.idle": "2021-11-04T08:02:05.745128Z",
          "shell.execute_reply": "2021-11-04T08:02:05.744379Z",
          "shell.execute_reply.started": "2021-11-04T08:02:05.733554Z"
        },
        "id": "Msd7BrpmxSf4"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = train_dataset.remove_columns([\"email\"]).with_format(\"tensorflow\")\n",
        "tf_test_dataset = test_dataset.remove_columns([\"email\"]).with_format(\"tensorflow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gju8z_PoxSf5"
      },
      "source": [
        "Then we convert everything in big tensors and use the tf.data.Dataset.from_tensor_slices method. In a Kaggle notebook, you have to change \n",
        "* test_features = {x: tf_test_dataset[x] for x in tokenizer.model_input_names} into test_features = {x: tf_test_dataset[x].to_tensor() for x in tokenizer.model_input_names} \n",
        "* train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names} into train_features = {x: tf_train_dataset[x].to_tensor() for x in tokenizer.model_input_names} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:02:12.464087Z",
          "iopub.status.busy": "2021-11-04T08:02:12.463746Z",
          "iopub.status.idle": "2021-11-04T08:02:30.650399Z",
          "shell.execute_reply": "2021-11-04T08:02:30.64968Z",
          "shell.execute_reply.started": "2021-11-04T08:02:12.464042Z"
        },
        "id": "mIdg9DwVxSf5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
        "#train_features = {x: tf_train_dataset[x].to_tensor() for x in tokenizer.model_input_names} for kaggle notebook\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
        "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
        "\n",
        "test_features = {x: tf_test_dataset[x] for x in tokenizer.model_input_names}\n",
        "#test_features = {x: tf_test_dataset[x].to_tensor() for x in tokenizer.model_input_names} for kaggle notebook\n",
        "test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_features, tf_test_dataset[\"label\"]))\n",
        "test_tf_dataset = test_tf_dataset.batch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:02:35.95793Z",
          "iopub.status.busy": "2021-11-04T08:02:35.957385Z",
          "iopub.status.idle": "2021-11-04T08:02:35.964333Z",
          "shell.execute_reply": "2021-11-04T08:02:35.963504Z",
          "shell.execute_reply.started": "2021-11-04T08:02:35.957895Z"
        },
        "id": "R_fGu_fbxSf7",
        "outputId": "7d2a6383-0650-4e92-8801-f1b517c293ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ({input_ids: (None, 512), token_type_ids: (None, 512), attention_mask: (None, 512)}, (None,)), types: ({input_ids: tf.int64, token_type_ids: tf.int64, attention_mask: tf.int64}, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "test_tf_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l11ywd7xSf8"
      },
      "source": [
        "## Fine-tuning the hugging-face bert-case model with tensorflow\n",
        "To fine-tune a hugging-face model, we have to:\n",
        "* We import the bert-cased pre-trained tensorflow model from the hugging-face hub using the transformer library\n",
        "* Compile this tensorflow model\n",
        "* Fit this tensorflow model\n",
        "\n",
        "In the cell below, we import the bert-case model from hugging-face. Since we have 2 classes (spam and not spam), we have to set num_classes=2. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:02:45.113708Z",
          "iopub.status.busy": "2021-11-04T08:02:45.11344Z",
          "iopub.status.idle": "2021-11-04T08:03:05.876986Z",
          "shell.execute_reply": "2021-11-04T08:03:05.876196Z",
          "shell.execute_reply.started": "2021-11-04T08:02:45.11368Z"
        },
        "id": "-3tTXvCpxSf8",
        "outputId": "17d09e18-e319-4ec4-80c9-103d8d3c587b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "3a497aa042b7440891c640d9689a1c11",
            "a66bf4af8215485383d1d5a1f4b9d860",
            "9d609912842145bf816be0820ad054a0",
            "83433459551549d9b4e1e57e3cbcaff0",
            "77f4519f4a2c4f46b3e4746a4d38fb15",
            "2785e980163a42afab6d9080c958845f",
            "fd58acfeee194a8a88946be570b66709",
            "ebaf3d230b694a078e68047032b062f7",
            "fac33e3f5a1d4082b021d2e45e7af94f",
            "60f2b8ac787e47e0a68c378bc6758868",
            "8944efdda3134af58fa6c5b98d8eea9e"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a497aa042b7440891c640d9689a1c11",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/502M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "#!pip install transformers\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWH3ypX1xSf-"
      },
      "source": [
        "We compile the tensorflow model as we always do when we use tensorflow. Here we use the\n",
        "* the adam  optimizer with learning_rate=5e-5\n",
        "* SparseCategoricalCrossentropy loss fucntion\n",
        "* SparseCategoricalAccuracy() metrics\n",
        "\n",
        "To fine-tune properly a tensorflow hugging-face model with an adam optimizer, it recomded to set the leraning_rate to 5e-5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:04:05.518668Z",
          "iopub.status.busy": "2021-11-04T08:04:05.517951Z",
          "iopub.status.idle": "2021-11-04T08:17:32.5589Z",
          "shell.execute_reply": "2021-11-04T08:17:32.558201Z",
          "shell.execute_reply.started": "2021-11-04T08:04:05.518624Z"
        },
        "id": "gCYg9mUIxSgA",
        "outputId": "ee7c5f5e-6b92-4a3c-d13b-8ca195494955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "319/319 [==============================] - 603s 2s/step - loss: 0.1483 - sparse_categorical_accuracy: 0.9435\n",
            "Epoch 2/4\n",
            "319/319 [==============================] - 580s 2s/step - loss: 0.0405 - sparse_categorical_accuracy: 0.9898\n",
            "Epoch 3/4\n",
            "319/319 [==============================] - 580s 2s/step - loss: 0.0461 - sparse_categorical_accuracy: 0.9886\n",
            "Epoch 4/4\n",
            "319/319 [==============================] - 580s 2s/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9961\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d9023d4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#tf.keras.metrics.Precision()\n",
        "import tensorflow as tf\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "#model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3)\n",
        "model.fit(train_tf_dataset, epochs=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPnZbV7OxSgA"
      },
      "source": [
        "We evaluate the model on the test set with the SparseCategoricalAccuracy metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-02T09:59:10.059774Z",
          "iopub.status.busy": "2021-11-02T09:59:10.059498Z",
          "iopub.status.idle": "2021-11-02T09:59:34.006407Z",
          "shell.execute_reply": "2021-11-02T09:59:34.005378Z",
          "shell.execute_reply.started": "2021-11-02T09:59:10.059743Z"
        },
        "id": "Sy8-O3nUxSgB",
        "outputId": "b01eda05-d0d8-468d-83e7-455ba14db23a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 39s 620ms/step - loss: 0.0470 - sparse_categorical_accuracy: 0.9867\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04701230302453041, 0.9866666793823242]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.evaluate(test_tf_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5FwwpmTxSgC"
      },
      "source": [
        "## Prediction on the test set \n",
        "We predict the probability vectors of the test set test_tf_dataset. \n",
        "* First we call model.predict.logits on test_tf_dataset.\n",
        "* In a second step, we use a softmax function to transform to an array with probabilities and we convert in a numpy object.\n",
        "* In the third step, we the classes using the numpy function np.argmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:20:51.749444Z",
          "iopub.status.busy": "2021-11-04T08:20:51.748765Z",
          "iopub.status.idle": "2021-11-04T08:21:04.997997Z",
          "shell.execute_reply": "2021-11-04T08:21:04.997252Z",
          "shell.execute_reply.started": "2021-11-04T08:20:51.749408Z"
        },
        "id": "_on9wHBuxSgC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#First we call model.predict.logits on test_tf_dataset\n",
        "pred_test_set = model.predict(test_tf_dataset).logits\n",
        "\n",
        "#In a second step, we use a softmax function to transform to an array with probabilities and we convert in a numpy object.\n",
        "proba_test_set = tf.nn.softmax(pred_test_set, axis=1).numpy()\n",
        "\n",
        "#In the third step, we the classes using the numpy function np.argmax\n",
        "classe_test_set = np.argmax(proba_test_set,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwmOUvB9xSgD"
      },
      "source": [
        "We comput the precision. The precision is the ratio tp / (tp + fp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-02T10:54:21.324724Z",
          "iopub.status.busy": "2021-11-02T10:54:21.324443Z",
          "iopub.status.idle": "2021-11-02T10:54:21.348754Z",
          "shell.execute_reply": "2021-11-02T10:54:21.347898Z",
          "shell.execute_reply.started": "2021-11-02T10:54:21.324691Z"
        },
        "id": "S-4j__TyxSgE",
        "outputId": "ddb3d21c-d11f-4b78-b394-a51dca3b60e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9411765"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "m_precision = tf.keras.metrics.Precision()\n",
        "m_precision.update_state(test_dataset['label'],classe_test_set.tolist())\n",
        "m_precision.result().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC9nG3HxxSgE"
      },
      "source": [
        "We comput the recall. The recall is the ratio tp / (tp + fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-02T10:58:03.576449Z",
          "iopub.status.busy": "2021-11-02T10:58:03.576149Z",
          "iopub.status.idle": "2021-11-02T10:58:03.601028Z",
          "shell.execute_reply": "2021-11-02T10:58:03.599897Z",
          "shell.execute_reply.started": "2021-11-02T10:58:03.576418Z"
        },
        "id": "JPLufZRcxSgF",
        "outputId": "753c0144-9153-40fb-86e9-07db66866d11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9876543"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "m_recall = tf.keras.metrics.Recall()\n",
        "m_recall.update_state(test_dataset['label'],classe_test_set.tolist())\n",
        "m_recall.result().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rwqR5dzxSgF"
      },
      "source": [
        "We compute the AUC score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-04T08:36:52.572633Z",
          "iopub.status.busy": "2021-11-04T08:36:52.572366Z",
          "iopub.status.idle": "2021-11-04T08:36:52.594348Z",
          "shell.execute_reply": "2021-11-04T08:36:52.593712Z",
          "shell.execute_reply.started": "2021-11-04T08:36:52.572604Z"
        },
        "id": "CiDDkmtUxSgG",
        "outputId": "c2694b8f-2d7c-4191-fc15-9de69739acde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99826014"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "m_auc = tf.keras.metrics.AUC()\n",
        "m_auc.update_state(test_dataset['label'], proba_test_set[:,1])\n",
        "m_auc.result().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW1c8OmRxSgK"
      },
      "source": [
        "## How to use the fine-tuned model on a new text\n",
        "In this part, we try the fine-tuned model on a new email. Then we have to care about the tokenization of this new email. After that, we have to make this tokenization compatible with TensorFlow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-31T14:47:45.834859Z",
          "iopub.status.busy": "2021-10-31T14:47:45.833884Z",
          "iopub.status.idle": "2021-10-31T14:47:45.841855Z",
          "shell.execute_reply": "2021-10-31T14:47:45.840978Z",
          "shell.execute_reply.started": "2021-10-31T14:47:45.834805Z"
        },
        "id": "SzZc41V-xSgL"
      },
      "outputs": [],
      "source": [
        "email_test = [\"\"\"martin a posted tassos papadopoulos the greek sculptor behind \n",
        "                the plan judged that the limestone of mount kerdylio NUMBER \n",
        "                miles east of salonika and not far from the mount athos monastic \n",
        "                community was ideal for the patriotic sculpture as well as alexander s \n",
        "                granite features NUMBER ft high and NUMBER ft wide a museum a restored \n",
        "                amphitheatre and car park for admiring crowds are planned so is \n",
        "                this mountain limestone or granite if it s limestone it ll weather \n",
        "                pretty fast yahoo groups sponsor NUMBER dvds free s p join now URL \n",
        "                to unsubscribe from this group send an email to forteana unsubscribe \n",
        "                URL your use of yahoo groups is subject to URL\"\"\"]\n",
        "\n",
        "#We tokenize the new email with the tokenizer we defined earlier un this example.\n",
        "email_encoding = tokenizer(email_test, padding=\"max_length\", truncation=True)\n",
        "#token_email"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsdXdTeDxSgL"
      },
      "source": [
        "We convert the tokenizer into a tf.data.Dataset object. In a second step, we have to apply the .batch(1) method to make email_data suitable for the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-31T14:48:01.823735Z",
          "iopub.status.busy": "2021-10-31T14:48:01.823469Z",
          "iopub.status.idle": "2021-10-31T14:48:01.837941Z",
          "shell.execute_reply": "2021-10-31T14:48:01.837103Z",
          "shell.execute_reply.started": "2021-10-31T14:48:01.823706Z"
        },
        "id": "H_srrq4uxSgQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "email_data = tf.data.Dataset.from_tensor_slices((dict(email_encoding)))\n",
        "email_data_batch = email_data.batch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdO0RXRdxSgQ"
      },
      "source": [
        "Then, we can apply the fine-tuned model to the Tensorflow dataset email_data_batch. After, we convert the previous tensor into a probability vector with a softmax function.  The result is converted in a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-31T14:48:56.456811Z",
          "iopub.status.busy": "2021-10-31T14:48:56.456104Z",
          "iopub.status.idle": "2021-10-31T14:48:59.233505Z",
          "shell.execute_reply": "2021-10-31T14:48:59.23263Z",
          "shell.execute_reply.started": "2021-10-31T14:48:56.45677Z"
        },
        "id": "9HQz0rg_xSgR",
        "outputId": "1be725d1-d892-4090-f1b3-8a6ca1510497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99655104, 0.00344894]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "#predict\n",
        "pred = model.predict(email_data_batch).logits\n",
        "\n",
        "#transform to array with probabilities\n",
        "res = tf.nn.softmax(pred, axis=1).numpy()\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDMIyyXGxSgS"
      },
      "source": [
        "Now, we display the prediction of the new text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-31T14:52:57.629607Z",
          "iopub.status.busy": "2021-10-31T14:52:57.629251Z",
          "iopub.status.idle": "2021-10-31T14:52:57.637724Z",
          "shell.execute_reply": "2021-10-31T14:52:57.636876Z",
          "shell.execute_reply.started": "2021-10-31T14:52:57.629573Z"
        },
        "id": "CY_xU2XqxSgS",
        "outputId": "e1dc501f-8bc3-459e-8035-9d3b17d0f0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'valide email'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import numpy as np\n",
        "classe_email = ['valide email','spam']\n",
        "classe_email[np.argmax(res)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NttULGNtxSgT"
      },
      "source": [
        "# EXERCICE : Tensorflow Huggingface TRANSFORMER OVER fetch_20newsgroups\n",
        "Before to do this exercice I recomand you to read carfully the notebook called Transformers application with hugging face. In this notebook we fine tune a hugging face bert model over the fetch_20newsgroups dataset. We have to understand the fetch_20newsgroups dataset as a first step. You have to run the cell bellow to create the dataset. The cell execution could take a while. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcwhZJQdxSgT"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism','comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware',\\\n",
        "              'comp.sys.mac.hardware','comp.windows.x','misc.forsale','rec.autos','rec.motorcycles',\\\n",
        "              'comp.sys.mac.hardware','comp.windows.x','misc.forsale','rec.autos','rec.motorcycles',\\\n",
        "              'rec.sport.baseball','rec.sport.hockey','sci.crypt','sci.electronics','sci.med','sci.space',\\\n",
        "              'soc.religion.christian','talk.politics.guns','talk.politics.mideast','talk.politics.misc','talk.religion.misc']\n",
        "\n",
        "dataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),categories=categories)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "len(dataset['target_names'])\n",
        "target_values_to_target_labels = dict(zip(range(0,len(dataset['target_names'])),dataset['target_names']))\n",
        "\n",
        "target_values = dataset.target.tolist()\n",
        "target_labels = [target_values_to_target_labels[o] for o in target_values]\n",
        "\n",
        "dataset_fetch_20newsgroups = pd.DataFrame({'text':dataset.data,'target_labels':target_labels})\n",
        "dataset_fetch_20newsgroups.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxCSEGjbxSgU"
      },
      "source": [
        "1) Now, we work with the dataset fetch_20newsgroups object. How many unique labels does the dataframe datasets fetch_20newsgroups have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-31T14:03:22.55907Z",
          "iopub.status.busy": "2021-10-31T14:03:22.55881Z",
          "iopub.status.idle": "2021-10-31T14:03:22.563331Z",
          "shell.execute_reply": "2021-10-31T14:03:22.562629Z",
          "shell.execute_reply.started": "2021-10-31T14:03:22.559034Z"
        },
        "id": "BR_yA10MxSgV"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgPzOmxKxSgW"
      },
      "source": [
        "2) How many rows with the 'target_labels' column equal comp.sys.mac.hardware does fetch_20newsgroups object have? Print a row with a comp.sys.mac.hardware label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QvsJIkOxSgW"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmgP0OOyxSgX"
      },
      "source": [
        "3) Use the sklearn labelencoder object to encode the target labels column into a new column called target_encoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpInsuJPxSgX"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "#your code here \n",
        "dataset_fetch_20newsgroups['target_encoded'] ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcIuw9juxSgY"
      },
      "source": [
        "4) Remove the URL, the email addresses and the html tags from the column 'text' of the dataset_fetch_20newsgroups dataset. You can use the functions defined  below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-31T14:59:33.537281Z",
          "iopub.status.busy": "2021-10-31T14:59:33.536993Z",
          "iopub.status.idle": "2021-10-31T14:59:33.543464Z",
          "shell.execute_reply": "2021-10-31T14:59:33.54267Z",
          "shell.execute_reply.started": "2021-10-31T14:59:33.537248Z"
        },
        "id": "5KioX7u8xSgY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html(data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "def _remove_urls(x):\n",
        "    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n",
        "\n",
        "def _remove_emails(x):\n",
        "    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\n",
        "\n",
        "def remove_url(data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "\n",
        "def remove_skip_line(x):\n",
        "    return re.sub(r'\\r?\\n|\\\\|\\r/g',\" \", x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o076rimlxSgZ"
      },
      "source": [
        "5) The dataset library provided by huggingface allow us to perform efficient data-preprocessing and to make the data compatible for a pretrained huggingface model. The dataset dataset_fetch_20newsgroups is a pandas object.\n",
        "* Load dataset_fetch_20newsgroups as a huggingface dataset object. For this purpose use Dataset.from_pandas. \n",
        "* Store the result in dataset_fetch20. dataset_fetch20 is a huggingface dataset object. \n",
        "* Remove the column 'target_labels' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itI-cz-XxSgZ"
      },
      "outputs": [],
      "source": [
        "#your code here\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmwODMdzxSga"
      },
      "source": [
        "6) Our ultimate goal is to fine tune a bert-base-uncased model from the huggingface hub. When we use a huggingface model, the tokenizer should be cordinated with the model. \n",
        "* Import a tokenizer compatible with a bert-base-uncased model.\n",
        "* Tokenize the column 'text' from the dataset_fetch20 dataset. You have to use something like tokenized_datasets = dataset_fetch20.map(tokenize_function, batched=True). \n",
        "* Create a training and test set using the train_test_split method from the dataset package. Store the result in a huggingface dataset called train_test_dataset. You should set the argument test_size=0.15."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj_Kn4soxSga"
      },
      "outputs": [],
      "source": [
        "#your code here\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_erbKL6xSgb"
      },
      "source": [
        "7) The train_test_dataset you created in the previous question has a text column we don't need anymore. The bert model use only 'attention_mask', 'input_ids', 'target_encoded', 'text', 'token_type_ids'. 'target_encoded' is the target values. Remove the 'text' from train_test_dataset and make it compatible with tensorflow using with_format(\"tensorflow\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1pBDIeXxSgb"
      },
      "outputs": [],
      "source": [
        "#complete the code here\n",
        "train_test_dataset = train_test_dataset.remove_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiZH2r7ExSgc"
      },
      "source": [
        "8) Our goal is to fine-tune a bert-base-uncased model using tensorflow.keras. We have to convert train_test_dataset['train'] and  train_test_dataset['test'] in big tensors using the tf.data.Dataset.from_tensor_slices method. \n",
        "* You have to use the tokenizer you defined in the question 6.\n",
        "* The tensor corresponding to  train_test_dataset['train'] should be stored in train_tf_dataset\n",
        "* The tensor corresponding to  train_test_dataset['test'] should be stored in test_tf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL-XQtSWxSgc"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = train_test_dataset['train']\n",
        "tf_test_dataset = train_test_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I54QzxKIxSgd"
      },
      "outputs": [],
      "source": [
        "#you code here. Use tf_train_dataset and tf_test_dataset from the previous cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h3CYxtYxSge"
      },
      "source": [
        "9)  Import the bert-base-uncased model from the huggingface hub. Use the result of question 1 to set the value of the num_labels parameter. For this purpose use the TFAutoModelForSequenceClassification object from the transfromers library. Call your pre-trained model model_text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_Ma2FqhxSge"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "#your code here\n",
        "\n",
        "model_text = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxdEMAnWxSgf"
      },
      "source": [
        "10) Compile the model_text such that\n",
        "* You have to set optimizer argument to tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "* with the acc metric or the metric or the tf.metrics.SparseCategoricalAccuracy() metric.\n",
        "\n",
        "Train your model with train_tf_dataset you mad in question 8. Evaluate your model on test_tf_dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3LHetzbxSgf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovsH3khXxSgh"
      },
      "source": [
        "11) This part is not a question. We just show the code to use model_text to a new text to classify. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-01T09:00:56.98894Z",
          "iopub.status.busy": "2021-11-01T09:00:56.988425Z",
          "iopub.status.idle": "2021-11-01T09:00:56.992987Z",
          "shell.execute_reply": "2021-11-01T09:00:56.992048Z",
          "shell.execute_reply.started": "2021-11-01T09:00:56.988886Z"
        },
        "id": "_id-yEe8xSgh"
      },
      "outputs": [],
      "source": [
        "text = [\"\"\"\n",
        "The first thing is first. \n",
        "If you purchase a Macbook, you should not encounter performance issues that will prevent you from learning to code efficiently.\n",
        "However, in the off chance that you have to deal with a slow computer, you will need to make some adjustments. \n",
        "Having too many background apps running in the background is one of the most common causes. \n",
        "The same can be said about a lack of drive storage. \n",
        "For that, it helps if you uninstall xcode and other unnecessary applications, as well as temporary system junk like caches and old backups.\n",
        "\"\"\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxHxvWDlxSgi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#tokenize the text\n",
        "encodings = tokenizer(text, padding=\"max_length\", truncation=True)\n",
        "\n",
        "#transform to tf.Dataset\n",
        "text_data_tf = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
        "\n",
        "#predict\n",
        "preds = model_text.predict(text_data_tf.batch(1)).logits\n",
        "\n",
        "#transform to array with probabilities\n",
        "res = tf.nn.softmax(preds, axis=1).numpy()\n",
        "\n",
        "#We show the predicted class\n",
        "le.inverse_transform([np.argmax(res)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF_d9saZxSgj"
      },
      "source": [
        "12. This part is not a question. We predict the probability vectors of the test set test_tf_dataset. \n",
        "* First we call model.predict.logits on test_tf_dataset.\n",
        "* In a second step, we use a softmax function to transform to an array with probabilities and we convert in a numpy object.\n",
        "* In the third step, we the classes using the numpy function np.argmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ky72CojxSgk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "pred_test_set = model_text.predict(test_tf_dataset).logits\n",
        "\n",
        "#In a second step, we use a softmax function to transform to an array with probabilities and we convert in a numpy object.\n",
        "proba_test_set = tf.nn.softmax(pred_test_set, axis=1).numpy()\n",
        "\n",
        "#In the third step, we the classes using the numpy function np.argmax\n",
        "classe_test_set = np.argmax(proba_test_set,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_ZLzVVWxSgk"
      },
      "source": [
        "We compute the accuracy on the predictions we've just done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRhGOY2UxSgm"
      },
      "outputs": [],
      "source": [
        "m_accuracy = tf.keras.metrics.Accuracy()\n",
        "m_accuracy.update_state(tf_test_dataset['target_encoded'],classe_test_set.tolist())\n",
        "m_accuracy.result().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsf9rBclxSgn"
      },
      "source": [
        "Here, we give the class names of the prediction using the labelencoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3KQrOh4xSgn"
      },
      "outputs": [],
      "source": [
        "classe_test_set_class_text =[le.inverse_transform([text])[0] for text in classe_test_set]\n",
        "classe_test_set_class_text[0:3]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "tensorflow-huggingface-transformer.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb9f34e403df4d389b8aa17ff767cfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0ab98016f88544678e2898a8bbfd4995",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9b1c930d39d4451e88079bc89177a656",
              "IPY_MODEL_132fd6cbf5fe476a8522a12ced085846",
              "IPY_MODEL_e548a293d9e94c798639701ef40146ca"
            ]
          }
        },
        "0ab98016f88544678e2898a8bbfd4995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b1c930d39d4451e88079bc89177a656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6618db3a563a4f4ab415255c9864df97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1f5413b5c2f4918b129feabd3b979ad"
          }
        },
        "132fd6cbf5fe476a8522a12ced085846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_da3e7724eccb428fb3afe1d297ca607e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_840923520fd145fd9d0020a63d620565"
          }
        },
        "e548a293d9e94c798639701ef40146ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6c1f0961939447edb6fb287a384a6ce1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:03&lt;00:00,  1.10s/ba]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b3c01fc7ae747b28ccfd1e716add414"
          }
        },
        "6618db3a563a4f4ab415255c9864df97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1f5413b5c2f4918b129feabd3b979ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da3e7724eccb428fb3afe1d297ca607e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "840923520fd145fd9d0020a63d620565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c1f0961939447edb6fb287a384a6ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b3c01fc7ae747b28ccfd1e716add414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1d7f938c2f54e44b3a3eaee7d1d848e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9619c29a6370477baf9674142ff5747d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f62fd5c8db00487e9d0237dc21fa93e0",
              "IPY_MODEL_094aef0bf7bf403d8ef252e78bd348b9",
              "IPY_MODEL_2189efe634524fdf93a82dbecea5579c"
            ]
          }
        },
        "9619c29a6370477baf9674142ff5747d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f62fd5c8db00487e9d0237dc21fa93e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b620c57853654d70bde1633a21b1ab28",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c49a39d49a243c9a12247a512de6055"
          }
        },
        "094aef0bf7bf403d8ef252e78bd348b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ff5405a9f3740b6b2cea48a463797a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_62892f83f4204998bf497ff804b85c03"
          }
        },
        "2189efe634524fdf93a82dbecea5579c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f9192df25b6749738d5582960cdd713f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  1.30ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d04ba6aef8f54f1482322c1f7751f99e"
          }
        },
        "b620c57853654d70bde1633a21b1ab28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c49a39d49a243c9a12247a512de6055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ff5405a9f3740b6b2cea48a463797a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "62892f83f4204998bf497ff804b85c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9192df25b6749738d5582960cdd713f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d04ba6aef8f54f1482322c1f7751f99e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a497aa042b7440891c640d9689a1c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a66bf4af8215485383d1d5a1f4b9d860",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9d609912842145bf816be0820ad054a0",
              "IPY_MODEL_83433459551549d9b4e1e57e3cbcaff0",
              "IPY_MODEL_77f4519f4a2c4f46b3e4746a4d38fb15"
            ]
          }
        },
        "a66bf4af8215485383d1d5a1f4b9d860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d609912842145bf816be0820ad054a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2785e980163a42afab6d9080c958845f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd58acfeee194a8a88946be570b66709"
          }
        },
        "83433459551549d9b4e1e57e3cbcaff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ebaf3d230b694a078e68047032b062f7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 526681800,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 526681800,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fac33e3f5a1d4082b021d2e45e7af94f"
          }
        },
        "77f4519f4a2c4f46b3e4746a4d38fb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_60f2b8ac787e47e0a68c378bc6758868",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 502M/502M [00:21&lt;00:00, 34.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8944efdda3134af58fa6c5b98d8eea9e"
          }
        },
        "2785e980163a42afab6d9080c958845f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd58acfeee194a8a88946be570b66709": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ebaf3d230b694a078e68047032b062f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fac33e3f5a1d4082b021d2e45e7af94f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60f2b8ac787e47e0a68c378bc6758868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8944efdda3134af58fa6c5b98d8eea9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}