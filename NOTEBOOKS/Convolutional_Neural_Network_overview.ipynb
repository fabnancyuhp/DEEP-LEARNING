{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Convolutional-Neural-Network overview.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabnancyuhp/DEEP-LEARNING/blob/main/NOTEBOOKS/Convolutional_Neural_Network_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN definition and overview\n",
        "\n",
        "A convolutional neural network, or CNN, is a deep learning neural network designed for processing structured arrays of data such as images. Convolutional neural networks are widely used in computer vision and have become the state of the art for many visual applications such as image classification, and have also found success in natural language processing for text classification and time series prediction.<br><br>\n",
        "\n",
        "\n",
        "<center>\n",
        "<a href=\"https://waterprogramming.wordpress.com/2021/06/14/cnns-for-time-series-applications/\">\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/CNN/CNN-2.png\" width=\"80%\" />\n",
        "</a>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "UD28iBuZUlm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above figure shows a schematic of a CNN’s architecture. The architecture is primarily comprised of a series of convolution and pooling layers followed by a fully connected network. In each convolution layer are kernel matrices that are convolved with the input into the convolution layer. It is up to the user to define the number of kernels and size of the kernels, but the weights in the kernel are learned using backpropagation. A bias is added to the output of the convolution layer and then passed through an activation function, such as ReLU function to yield feature maps. The feature maps are stacked in a cuboid of a depth that equals the number of filters. If the convolution layer is followed by a pooling layer, the feature maps are down-sampled to produce a lower dimensional representation of the feature maps. The output from the final pooling or convolutional layer is flattened and fed to the fully connected layers.<br>"
      ],
      "metadata": {
        "id": "wOuvsOeiUlm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image modelization, RGB images\n",
        "\n",
        "An image is no more than many pixels (little squares) together. Each of those pixels store information. In the case of monochrome images, for example, the pixel stores two values: 1 if it is white and 0 if it is black (or vice versa). <br><br>\n",
        "A RGB image is a color image with three layers : a red (R) , a green (G) and a blue (B). Every image is made up of pixels that range from 0 to 255. The picture below gives a good understanding of how an RGB image is designed.\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/DENSENN/catRGB.jpg\" width=\"40%\" />\n",
        "</center>\n",
        "Below is the example of an input image of size 4*4 and has 3 channels i.e RGB and pixel values.\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/CNN/98920pixel%20image.png\" width=\"40%\" />\n",
        "</center>"
      ],
      "metadata": {
        "id": "BVVam91NUlm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers in a Convolutional Neural Network\n",
        "A convolution neural network has multiple hidden layers that help in extracting information from an image. The four important layers in CNN are:\n",
        "1. Convolution layer\n",
        "2. ReLU layer\n",
        "3. Pooling layer\n",
        "4. Flatten layer \n",
        "5. Fully connected layer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dt5iQvO4UlnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Layer\n",
        "\n",
        "This is the first step in the process of extracting valuable features from an image. A convolution layer has several filters that perform the convolution operation. As we mentioned earlier, every image is considered as a matrix of pixel values.<br>\n",
        "This layers convolves an image by a matrix, called Kerner or filter. The proccess is as follows:\n",
        "1. First, you overlay the kernel onto the image\n",
        "2. Then you multiply the kernel value by the image value.\n",
        "3. After that, you calculate the product of the results of the previous step.\n",
        "4. Finally, you move the kernel one pixel and repeat the process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        "<a href=\"https://anderfernandez.com/en/blog/how-to-create-convolutional-neural-network-keras/\">\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/CNN/1_ciDgQEjViWLnCbmX-EeSrA.gif\" width=\"50%\" />\n",
        "</a>\n",
        "</center>\n",
        "\n",
        "All elements in kernel matrices are trainable parameters during backpropagation algorithm."
      ],
      "metadata": {
        "id": "t3-kmpYWUlnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding in Convolutional layer \n",
        "In a convolutional layer, assuming that the input shape is $n\\times n$ and and the convolution kernel shape is  $(k\\times k)$, then the output shape will be $(n-k+1)\\times (n-k+1)$. Therefore, the output shape of the convolutional layer is determined by the shape of the input and the shape of the convolution kernel. In the above example, for an (7 x 7) image and (3 x 3) kernel, the output resulting after convolution operation would be of size (7-3+1)x (7-3+1)=(5 x 5). <br><br>\n",
        "After applying many successive convolutions, we tend to wind up with outputs that are considerably smaller than our input. If we start with a  240×240  pixel image,  10  layers of  5×5  convolutions reduce the image to  200×200  pixels, slicing off  30%  of the image and with it obliterating any interesting information on the boundaries of the original image. Padding is the most popular tool for handling this issue.<br><br>\n",
        "**Padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above.**\n",
        "\n",
        "<center>\n",
        "<a href=\"https://www.geeksforgeeks.org/cnn-introduction-to-padding/\">\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/CNN/padding.png\" width=\"30%\" />\n",
        "</a>\n",
        "</center>\n",
        "\n",
        "This prevents shrinking as, if p = number of layers of zeros added to the border of the image, then our (n x n) image becomes (n + 2p) x (n + 2p) image after padding. So, applying convolution-operation (with (f x f) filter) outputs (n + 2p – f + 1) x (n + 2p – f + 1) images. For example, adding one layer of padding to an (8 x 8) image and using a (3 x 3) filter we would get an (8 x 8) output after performing convolution operation.<br><br>\n",
        "This increases the contribution of the pixels at the border of the original image by bringing them into the middle of the padded image. Thus, information on the borders is preserved as well as the information in the middle of the image.\n",
        "1. **Valid Padding**: It implies no padding at all. The input image is left in its valid/unaltered shape. \n",
        "So, [(n x n) image ] * [(f x f) filter] --> [(n-f+1) x (n-f+1) image]\n",
        "2. **Same Padding**: In this case, we add ‘p’ padding layers such that the output image has the same dimensions as the input image. \n",
        "So, [(n +2p)x (n+2p) image ] * [(f x f) filter] --> [(n-f+1) x (n-f+1) image], which gives p = (f – 1) / 2 (because n + 2p – f + 1 = n). \n",
        "So, if we use a (the 3 x 3) filter the 1 layer of zeros must be added to the borders for the same padding. Similarly, if (5 x 5) filter is used 2 layers of zeros must be appended to the border of the image."
      ],
      "metadata": {
        "id": "1doawgbCUlnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stride in a convolutional layer\n",
        "When computing the cross-correlation, we start with the convolution window at the upper-left corner of the input tensor, and then slide it over all locations both down and to the right. In previous examples, we default to sliding one element at a time. However, sometimes, either for computational efficiency or because we wish to downsample, we move our window more than one element at a time, skipping the intermediate locations.<br><br>\n",
        "**We refer to the number of rows and columns traversed per slide as the stride.** So far, we have used strides of 1, both for height and width. Sometimes, we may want to use a larger stride. shows a two-dimensional cross-correlation operation with a stride of 3 vertically and 2 horizontally. The shaded portions are the output elements as well as the input and kernel tensor elements used for the output computation:  0×0+0×1+1×2+2×3=8 ,  0×0+6×1+0×2+0×3=6 .\n",
        "\n",
        "<center>\n",
        "<a href=\"https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html#img-conv-stride\">\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/CNN/conv-stride.svg\" width=\"40%\" />\n",
        "</a>\n",
        "</center>\n",
        "\n",
        "\n",
        "In general, \n",
        "* when the stride for the height is  $s_{h}$  and the stride for the width is  $s_{w}$ , \n",
        "* when we have a input  (n x n) image, a (k x k) filter and a p padding\n",
        "\n",
        "the output shape is $\\lfloor(n-k-p+s_{h})/s_{h}\\rfloor\\times \\lfloor(n-k-p+s_{w})/s_{w}\\rfloor$ "
      ],
      "metadata": {
        "id": "jsPQ8oXkUlnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pooling Layers\n",
        "Convolutional layers enable to detect shapes on an image keeping the image size. However, big images imply more work and slower and more difficult trainings, so we might want to reduce the size of the image at any time. Pooling layers will help us do so.<br><br>\n",
        "Pooling layers allow us to reduce the size of the image so that the neural network works faster. It basically creates a smaller image by dividing the image in several n by n matrices (say 2 by 2 matrices).<br><br>\n",
        "\n",
        "In this section, we cover two types of pooling operations: Max pooling, Average pooling. There are other pooling operations such that Global max pooling and Global average pooling.<br><br>\n",
        "\n",
        "Depending on the type of pooling layer, the way of calculating the result might vary. In Max Pooling layers, for example, the result will be the maximum value of each smaller matrix. In Average Layers, on the other hand, the result will be the average of the smaller matrix. Let’s see an example of a Max Pooling layer:\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/CNN/MaxpoolSample2.png\" width=\"50%\" />\n",
        "</center>\n",
        "\n",
        "By applying a max pooling layer we ensure that the shapes detected by the convolutional layer are maintained for the next layer. Because of this, Max Pooling layers are the most used pooling layers, as they are they usually give better results.\n",
        "Finally, pooling layers do not have any trainable parameters."
      ],
      "metadata": {
        "id": "wrHtoYrjUlnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take a look at Average pooling. The picture below gives a good understanding of how Average pooling works.\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/CNN/AvgPoolingEg.png\" width=\"50%\" />\n",
        "</center>"
      ],
      "metadata": {
        "id": "Pjl--MhWUlnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReLU activation layer\n",
        "ReLU stands for the rectified linear unit. Once the feature maps are extracted, the next step is to move them to a ReLU layer. <br>\n",
        "ReLU performs an element-wise operation and sets all the negative pixels to 0. It introduces non-linearity to the network, and the generated output is a rectified feature map. We have $R(Z)=\\max(0,Z)$.\n"
      ],
      "metadata": {
        "id": "lfH8k_v4UlnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flattening process\n",
        "Flattening process involves taking the pooled feature map that is generated in the pooling step and transforming it into a one-dimensional vector. This is done so that you can feed them as inputs to the dense layer."
      ],
      "metadata": {
        "id": "CtPxOSfXUlnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fully connected layer\n",
        "This layer is present at the tail section of the CNN model architecture as seen before. The input to the fully connected layer is the rich features that have been extracted using convolutional filters. This is then forward propagated till the output layer, where we get the probability of the input image belonging to different classes. The predicted output is the class with the highest probability that the model has predicted.<br><br>\n",
        "\n",
        "The complete process of a CNN model can be seen in the below image.\n",
        "\n",
        "<center>\n",
        "<a href=\"https://www.analyticsvidhya.com/blog/2021/08/beginners-guide-to-convolutional-neural-network-with-implementation-in-python/\">\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/CNN/full_CNN_process.png\" width=\"60%\" />\n",
        "</a>\n",
        "</center>"
      ],
      "metadata": {
        "id": "0zlDAW6AUlnW"
      }
    }
  ]
}