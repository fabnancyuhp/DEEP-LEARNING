{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "word-embedding-word-vectorization.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabnancyuhp/DEEP-LEARNING/blob/main/NOTEBOOKS/word_embedding_word_vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.<br>\n",
        "Some of NLP tasks include the following:\n",
        "* Speech recognition : converting voice data into text data\n",
        "* Part of speech tagging, also called grammatical tagging\n",
        "* Named entity recognition, or NEM : NEM identifies ‘Kentucky’ as a location or ‘Fred’ as a man's name.\n",
        "* Sentiment analysis attempts to extract Sentiments from text data.\n",
        "* ....\n",
        "\n",
        "When we deal with NLP in this cours, we're proned to handle text data. To apply algorithms such as random forests, neural networks on text data we need to convert text into vector representation. The different types of word vector representation can be broadly classified into two categories:\n",
        "* Frequency based vector \n",
        "* Prediction based vector called word Embedding\n",
        "\n",
        "We show four types of Frequency based vector:\n",
        "* one-hote vector and Count Vector\n",
        "* TF-IDF Vector\n",
        "* Co-Occurrence Vector\n",
        "\n",
        "**Frequency-based methods yield  sparse matrices.**<br>\n",
        "\n",
        "In this chapter, we show severals kind of word embedding:\n",
        "* Embedding Layer\n",
        "* Word2Vec : Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are:\n",
        "    * Continuous Bag-of-Words, or CBOW model.\n",
        "    * Continuous Skip-Gram Model.\n",
        "* GloVe : Global Vectors for Word Representation\n",
        "\n",
        "**Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space.**"
      ],
      "metadata": {
        "id": "1OLDCSy4T94R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus, dictionary and one-hote encoding\n",
        "A corpus is a set of documents. A document could be a review, a tweet, newspaper article,... . The dictionary is the set of the unique words (tokens) that appear in the corpus.<br>\n",
        "For example, \n",
        "* here we have a corpus with 3 documents.\n",
        "   * D1 'le beagle est il un bon chien de compagnie'\n",
        "   * D2 'le tour de france 2021 est maintenue'\n",
        "   * D3 \"l'euro 2020 se joue dans plusieurs pays\"\n",
        "* The dictionary is ['le', 'est', 'de', 'beagle', 'il', 'un', 'bon', 'chien', 'compagnie', 'tour', 'france', '2021', 'maintenue', \"l'euro\", '2020', 'se', 'joue', 'dans', 'plusieurs', 'pays']. \n",
        "* The vocabulary size is 20"
      ],
      "metadata": {
        "id": "Np6b_LMeT94U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For one-hot encoding, each word, or token, in a text corresponds to a vector element. O stands for one-hot. We give some one-hot encoding vectors. We have 20 words in our dictionary, then each one-hot vector has 20 components.\n",
        "* $O_{le}=O_{1}=[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]^{T}$\n",
        "* $O_{ beagle}=O_{4}=[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]^{T}$\n",
        "* $O_{compagnie} = O_{9}=[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]^{T}$\n",
        "* $O_{pays} = O_{20} = [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]^{T}$\n"
      ],
      "metadata": {
        "id": "tUesEJBjT94X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we list some drawbacks of the one-hot encoding:\n",
        "* High dimensionality : The number of dimension is equal to the number of unique words in the corpus\n",
        "* Sparse : Only 1 non-zero value\n",
        "* One-hot encoding does not catch the words meaning "
      ],
      "metadata": {
        "id": "B9gHC6-BT94Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Corpus = ['le beagle est il un bon chien de compagnie',\\\n",
        "          'le tour de france 2021 est maintenue',\\\n",
        "         \"l'euro 2020 se joue dans plusieurs pays\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T12:38:26.585159Z",
          "iopub.execute_input": "2021-06-27T12:38:26.585697Z",
          "iopub.status.idle": "2021-06-27T12:38:26.592758Z",
          "shell.execute_reply.started": "2021-06-27T12:38:26.585595Z",
          "shell.execute_reply": "2021-06-27T12:38:26.592102Z"
        },
        "trusted": true,
        "id": "y5ES6jRBT94a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf, tokenizer.word_index.keys()\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(Corpus)\n",
        "sequence = tokenizer.texts_to_sequences(['le tour de france 2021 est maintenue','le beagle est il un bon chien de compagnie'])\n",
        "#tokenizer.word_index\n",
        "print('dictionary : ',tokenizer.word_index ,'\\n','\\n', 'The vocabulary size is :', len(tokenizer.word_index.keys()),'\\n')\n",
        "print('sequence:', sequence) "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T12:38:59.610655Z",
          "iopub.execute_input": "2021-06-27T12:38:59.610998Z",
          "iopub.status.idle": "2021-06-27T12:39:05.733875Z",
          "shell.execute_reply.started": "2021-06-27T12:38:59.610968Z",
          "shell.execute_reply": "2021-06-27T12:39:05.732689Z"
        },
        "trusted": true,
        "id": "sDrGyjHUT94d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count-Vector Matrix\n",
        "\n",
        "Consider a Corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document di. "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T09:01:12.557356Z",
          "iopub.execute_input": "2021-06-27T09:01:12.557702Z",
          "iopub.status.idle": "2021-06-27T09:01:12.565264Z",
          "shell.execute_reply.started": "2021-06-27T09:01:12.557673Z",
          "shell.execute_reply": "2021-06-27T09:01:12.564088Z"
        },
        "id": "6RDF5ytWT94e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let the following Corpus of 3 documents\n",
        "\n",
        "* D1 'Boxer is a German dog', \n",
        "* D2 'Bulldog is an English dog',\n",
        "* D3 'Stellantis is a merger between PSA and FCA' \n",
        "\n",
        "This corpus has 13 unique tokens that form the following dictionary : ['Boxer', 'Bulldog','English', 'FCA', 'German', 'PSA', 'Stellantis', 'an', 'and', 'between','dog','is' 'merger'].<br><br>\n",
        "The count matrix M of size 3 X 13 will be represented as:\n",
        "$$\n",
        "\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n",
        "  \\hline\n",
        "   & boxer & Bulldog & English &  FCA & German &  PSA& Stellantis& an& and& between& dog& is& merger\\\\\n",
        "  \\hline\n",
        "  doc1 & 1& 0& 0& 0& 1& 0& 0& 0& 0& 0& 1& 1& 0 \\\\\n",
        "  \\hline\n",
        "  doc2 & 0& 1& 1& 0& 0& 0& 0& 1& 0& 0& 1& 1& 0 \\\\\n",
        "  \\hline\n",
        "  doc3 & 0& 0& 0& 1& 0& 1& 1& 0& 1& 1& 0& 1& 1\\\\\n",
        "  \\hline\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The result of a count_vector process on a corpus is a sparse matrix. Consider if you had a corpus with 20,000 unique words: a single short document in that corpus of, perhaps, 40 words would be represented by a matrix with 20,000 rows (one for each unique word) with a maximum of 40 non-zero matrix elements (and potentially far-fewer if there are a high number of non-unique words in this collection of 40 words). This leaves a lot of zeroes, and can end up taking a large amount of memory to house these spare representations."
      ],
      "metadata": {
        "id": "iDVegGf1T94g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "Corpus = ['Boxer is a German dog',\\\n",
        "         'Bulldog is an English dog',\\\n",
        "         'Stellantis is a merger between PSA and FCA']\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "vectorizer.fit(Corpus)\n",
        "print('Dictionary:',vectorizer.vocabulary_,'\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T15:08:53.624505Z",
          "iopub.execute_input": "2021-06-27T15:08:53.624905Z",
          "iopub.status.idle": "2021-06-27T15:08:53.633027Z",
          "shell.execute_reply.started": "2021-06-27T15:08:53.624847Z",
          "shell.execute_reply": "2021-06-27T15:08:53.63183Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8MLZwF7T94h",
        "outputId": "5447ebb1-459c-4d2c-b380-1762d2d26b22"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary: {'Boxer': 0, 'is': 11, 'German': 4, 'dog': 10, 'Bulldog': 1, 'an': 7, 'English': 2, 'Stellantis': 6, 'merger': 12, 'between': 9, 'PSA': 5, 'and': 8, 'FCA': 3} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count-Vector Matrix\n",
        "vectorizer.transform(Corpus).toarray()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T14:36:09.894643Z",
          "iopub.execute_input": "2021-06-27T14:36:09.89506Z",
          "iopub.status.idle": "2021-06-27T14:36:09.903404Z",
          "shell.execute_reply.started": "2021-06-27T14:36:09.895026Z",
          "shell.execute_reply": "2021-06-27T14:36:09.902018Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjS99wXdT94i",
        "outputId": "cff1d9f2-ac14-4192-ec1e-3bfc098ec9e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF vectorization\n",
        "TF-IDF stands for Term Frequency Inverse Document Frequency of records.\n",
        "TF-IDF is another method which is based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus.<br><br>\n",
        "Common words like ‘is’, ‘the’, ‘a’ etc. tend to appear quite frequently in comparison to the words which are important to a document. For example, a document A on Lionel Messi is going to contain more occurences of the word “Messi” in comparison to other documents. But common words like “the” etc. are also going to be present in higher frequency in almost every document.<br><br>\n",
        "Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.<br>\n",
        "TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document."
      ],
      "metadata": {
        "id": "iiFmJmCVT94j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the below sample table which gives the count of terms(tokens/words) in two documents.\n",
        "$$\n",
        "\\begin{array}{cccc|c|c|c|cc}\n",
        "  \\hline\n",
        "   &    & Document1 &   &   &  &  &  & Document2 & & & &  & \\\\\n",
        "  \\hline\n",
        "   &term  & & count& &  &  & term&  &count  & & & &  \\\\\n",
        "  \\hline\n",
        "  & this  & & 1& & & & this& & 1& & & &  \\\\\n",
        "  &is &  & 1& & & & is&  & 2& & & & \\\\\n",
        "  &about & & 2& & & &about & &1 & & & & \\\\\n",
        "  &Messi & & 4& & & &TF-IDF & &1 & & & & \n",
        "\\end{array}\n",
        "$$"
      ],
      "metadata": {
        "id": "jc2pQBVyT94k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us define a few terms related to TF-IDF.\n",
        "* TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
        "* TF(This, Document2)=1/5\n",
        "* IDF = log(D/n), where, D is the number of documents and n is the number of documents a term t has appeared in.\n",
        "* So, IDF(This) = log(2/2) = 0.\n",
        "* Let us compute IDF for the word ‘Messi’. IDF(Messi) = log(2/1) = 0.301.\n",
        "* TF-IDF(This,Document1) = TF(This, Document1)*IDF(This) =(1/8) * (0) = 0\n",
        "* TF-IDF(This, Document2) = TF(This, Document2)*IDF(This) = (1/5) * (0) = 0\n",
        "* TF-IDF(Messi, Document1) = TF(Messi, Document1)*IDF(Messi) = (4/8) * 0.301 = 0.15\n",
        "\n",
        "\n",
        "\n",
        "As, you can see for Document1 , TF-IDF method heavily penalises the word ‘This’ but assigns greater weight to ‘Messi’. So, this may be understood as ‘Messi’ is an important word for Document1 from the context of the entire corpus.<br>\n",
        "\n",
        "Below we have an example of computation of a TF_IDF matrix using python"
      ],
      "metadata": {
        "id": "eyqiBNZPT94l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a Corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary=[word1,word2,....,wordN]. The size of the TF-IDF matrix M will be given by D X N. Then we have\n",
        "$$M(i,j)=TF-IDF(wordj,Document_i)=TF(wordj,Document_i)*IDF(wordj)=(Number of times wordj appears in a Document_i/Number of word in Document_i)*log(D/n)$$<br>\n",
        "where n is the number of documents wordj has appeared in<br><br>\n",
        "\n",
        "**The result of a TF-IDF process on a large corpus is a sparse matrix.**\n",
        "\n",
        "Below we have an example of computation of a TF_IDF matrix using python"
      ],
      "metadata": {
        "id": "zu6i_XyiT94m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "Corpus = ['Boxer is a German dog',\\\n",
        "         'Bulldog is an English dog',\\\n",
        "         'Stellantis is a merger between PSA and FCA']\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=0, lowercase=False)\n",
        "vectorizer.fit(Corpus)\n",
        "print('Dictionary:',vectorizer.vocabulary_,'\\n')\n",
        "print('TF-IDF matrix: ')\n",
        "vectorizer.transform(Corpus).toarray()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T13:17:56.61804Z",
          "iopub.execute_input": "2021-06-28T13:17:56.618445Z",
          "iopub.status.idle": "2021-06-28T13:17:56.630194Z",
          "shell.execute_reply.started": "2021-06-28T13:17:56.618408Z",
          "shell.execute_reply": "2021-06-28T13:17:56.628715Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5MskciTT94o",
        "outputId": "c1a423ae-f78f-4fdd-a3aa-ce1b7fa6dddd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary: {'Boxer': 0, 'is': 11, 'German': 4, 'dog': 10, 'Bulldog': 1, 'an': 7, 'English': 2, 'Stellantis': 6, 'merger': 12, 'between': 9, 'PSA': 5, 'and': 8, 'FCA': 3} \n",
            "\n",
            "TF-IDF matrix: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5844829 , 0.        , 0.        , 0.        , 0.5844829 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.44451431, 0.34520502, 0.        ],\n",
              "       [0.        , 0.50461134, 0.50461134, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.50461134, 0.        , 0.        ,\n",
              "        0.38376993, 0.29803159, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.39687454, 0.        ,\n",
              "        0.39687454, 0.39687454, 0.        , 0.39687454, 0.39687454,\n",
              "        0.        , 0.2344005 , 0.39687454]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Representation/ Featurized representation\n",
        "\n",
        "\n",
        "it is obvious that one-hot encoding has drawbacks:\n",
        "* the inner product of two differents one-hot vectors is 0\n",
        "* the cosine similarity between the one-hot vectors of any two different words is 0\n",
        "* The euclidianne distance between 2 differents one-hot vectors is always the same\n",
        "\n",
        "As As consequences:\n",
        "* Since the cosine similarity between the one-hot vectors of any two different words is 0, it is difficult to use the one-hot vector to accurately represent the similarity between multiple different words.\n",
        "* one-hot encoding and bag-of-words models (i.e. using dummy variables to represent the presence or absence of a word in an observation, i.e. a sentence) do not capture information about a word's meaning or context. \n",
        "* One-hot encodings do not capture syntactic (structure) and semantic (meaning) relationships across collections of words and, therefore, represent language in a very naive way.<br>\n",
        "\n",
        "In contrast, word vectors represent words as multidimensional continuous floating point numbers where semantically similar words are mapped to proximate points in geometric space.This means that words such as wheel and engine should have similar word vectors to the word car (because of the similarity of their meanings), whereas the word banana should be quite distant. The beauty of representing words as vectors is that they lend themselves to mathematical operators. **Word vectors are simply vectors of numbers that represent the meaning of a word**. For example, we can add and subtract vectors — the canonical example here is showing that by using word vectors we can determine that:\n",
        "\n",
        "* **king - man + woman = queen**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/WORDEMBEDING/kingandkeen.png\" title=\"Title Tag Goes Here\" height=\"200\" width=\"250\" border=\"1px\">\n",
        "</center>\n",
        "\n",
        "The numbers in the word vector represent the word's distributed weight across dimensions. In a simplified sense, each dimension represents a meaning and the word's numerical weight on that dimension captures the closeness of its association with and to that meaning. Thus, the semantics of the word are embedded across the dimensions of the vector.\n",
        "\n",
        "A word vector is a featurized representation of a word. Here, we represent the words Man, Woman, King, Queen, Apple, Orange with the features Gender, Royale, Age, Food. \n",
        "\n",
        "$$\\begin{array}{c|cccccc|}\n",
        "\\hline\n",
        "&Man & Woman & King & Qween & Apple & Orange\\\\\n",
        "\\hline\n",
        "Gender & -1 & 1  &-0.95  & 0.97  &0.00  &0.01    \\\\\n",
        "\\hline\n",
        "Royal &0.01 & 0.02 & 0.93 & 0.95 & -0.01 & 0.00\\\\\n",
        "\\hline\n",
        "Age &  0.03 &0.02  &0.7  &0.69  &0.03 &-0.02      \\\\\n",
        "\\hline \n",
        "Food & 0.04 &0.01 &0.02 & 0.01 &0.95 & 0.97 \\\\\n",
        "\\hline\n",
        "\\end{array}$$\n",
        "In the above matrix each column is the vector representation of a word. This particular matrix is called embedding matrix. Each column vector is a word embedding.<br><br>\n",
        "**Given the vectors of two words, we can determine their similarity**.\n",
        "Apple and Orange are fruits then the cosine similarity between their word vectors should be close to 1. The word vectors of Apple and Orange are respectively $e_{Apple}=[0.00,-0.01,0.03,0.95]^{T}$ and $e_{Orange}=[0.01,0.00,-0.02,0.97]^{T}$. $e$ stands for embedding. We have\n",
        "$$sim(Apple,Orange)=\\frac{e_{Apple}^{T}e_{Orange}}{\\left\\lVert e_{Apple} \\right\\rVert \\left\\lVert e_{Orange} \\right\\rVert }=0.99853041412809$$\n",
        "Apple should be quit distant to King. We compute the cosine similarity between these 2 words:\n",
        "$$sim(Apple,Orange)=\\frac{e_{Apple}^{T}e_{King}}{\\left\\lVert e_{Apple} \\right\\rVert \\left\\lVert e_{King} \\right\\rVert }=0.021494708641488013$$\n",
        "These two word vectors are close to the orthogonality. The angle between these two vectors is about 88 degree."
      ],
      "metadata": {
        "id": "qKPlr_fwT94p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ea = np.array([0.0, -0.01,0.03,0.95])\n",
        "eo = np.array([0.01,0.00, -0.02,0.97])\n",
        "print('sim(Apple,Orange)=',np.sum(ea*eo)/np.sqrt((np.sum(ea*ea)*np.sum(eo*eo))))\n",
        "ek = np.array([-0.95,0.93, 0.7,0.02])\n",
        "print('sim(Apple,King)=',np.sum(ea*ek)/np.sqrt((np.sum(ea*ea)*np.sum(ek*ek))))\n",
        "print('angle between Apple and King=',np.arccos(np.sum(ea*ek)/np.sqrt((np.sum(ea*ea)*np.sum(ek*ek))))*180/np.pi,' degree')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-01T13:47:15.367164Z",
          "iopub.execute_input": "2021-07-01T13:47:15.367561Z",
          "iopub.status.idle": "2021-07-01T13:47:15.37706Z",
          "shell.execute_reply.started": "2021-07-01T13:47:15.367528Z",
          "shell.execute_reply": "2021-07-01T13:47:15.376306Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwqaKdLMT94u",
        "outputId": "3c4eebb1-cbde-40f1-fab1-0a17f7a6ff4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sim(Apple,Orange)= 0.99853041412809\n",
            "sim(Apple,King)= 0.021494708641488013\n",
            "angle between Apple and King= 88.76834905881994  degree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Are Word Embeddings?\n",
        "We give severals definitions of word embedding:\n",
        "* Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n",
        "* A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
        "* Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.\n",
        "\n",
        "Key to the approach is the idea of using a dense distributed representation for each word. Conceptually it involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension."
      ],
      "metadata": {
        "id": "Pyv1kdeZT94z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Layer : From one-hote to word embedding\n",
        "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/<br>"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-17T12:37:53.828214Z",
          "iopub.execute_input": "2021-06-17T12:37:53.828776Z",
          "iopub.status.idle": "2021-06-17T12:37:53.833238Z",
          "shell.execute_reply.started": "2021-06-17T12:37:53.828711Z",
          "shell.execute_reply": "2021-06-17T12:37:53.832561Z"
        },
        "id": "kxrwtMj6T941"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we use supervised learning to get vectors representations of words from a corpus. We Consider a corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. Each document is a review. The N tokens will form our dictionary = {word1,....,wordN}.The one-hot encoding vector of the wordi $O_{i}$ is the N-dimensional vector where each component is equal 0 except the ith one which  is equal to 1. Each document is labeled as 0 (negative) or 1 (positive). We want to represent each dictionary word with K components vector:\n",
        "* word1 vector is $e_{1}=e_{word1}=[w_{11},\\dots,w_{k1}]^{T}$\n",
        "* word2 vector is $e_{2}=e_{word1}=[w_{12},\\dots,w_{k2}]^{T}$\n",
        "* wordN vector is $e_{N}=e_{word1}=[w_{1N},\\dots,w_{kN}]^{T}$\n",
        "\n",
        "Generaly $K<N$ and the K-dimentional vector space is dense. For exemple, we could have N=10000 and K=300.\n",
        "Each word is represented with K features corresponding to the K vector components. We introduce the kxv embedding matrice E:\n",
        "$$\\begin{array}{c|ccc|}\n",
        "& word1&\\dots \\dots & wordN\\\\\n",
        "\\hline\n",
        "feature1 & w_{11}&\\dots \\dots & w_{1N}\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "featurek & w_{k1}&\\dots \\dots & w_{kN}\\\\\n",
        "\\hline\n",
        "\\end{array}$$\n",
        "The $w_{ij}s$ are trainables parameters. "
      ],
      "metadata": {
        "id": "x_yxn2wUT942"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\{O_{1},\\dots,O_{N} \\}$ be the set of the one-hot encodings of the dictionary. The one-hot encoding vector of the wordi $O_{i}$ is the N-dimensional vector where each component is equal 0 except the ith one which  is equal to 1 . The matrix multiplication between E and $O_{i}$ is equal to the word vector embedding of wordi:\n",
        "$$EO_{i}=e_{i} $$\n",
        "The matrix E is randomly initialized. We train a neural network to determin wether or not a review is positive. The matrix embedding coeficiants are fitted during the training stage of this neural network. After the neural network is trained we will get word embeddings as a side effect. So the problem for review classification is almost like a fake problem. In fact we care about word embeddings. In other words, we care about the matrix E.<br>\n",
        "In this neural network, we vertically stack all word vectors embedding into a single vector. This step is called flattering. The flattering step produces a document vector.<br> \n",
        "The documents in the corpus don't have the same length. In other words, the number of tokens differs from one document to another. To cop this problem, we use a padding step to make all vector documents same-sized.<br>\n",
        "$\\hat{Y}$ is an $\\mathbb{R}^{2}$ vector since we deal with a binary classification problem. In the case of multiclass classification problem, $\\hat{Y}$ is an $\\mathbb{R}^{n}$ vector where n is the number of classe label."
      ],
      "metadata": {
        "id": "Gj2AON3jT942"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**<br>\n",
        "An embedding layer, for lack of a better name, is a word embedding that is learned jointly with a neural network model on a specific natural language processing task, such as language modeling or document classification."
      ],
      "metadata": {
        "id": "83COkQ8jT944"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below an example of an embedding layer with Keras:"
      ],
      "metadata": {
        "id": "OUqSmjfAT945"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T12:41:11.678046Z",
          "iopub.execute_input": "2021-07-04T12:41:11.678559Z",
          "iopub.status.idle": "2021-07-04T12:41:11.68298Z",
          "shell.execute_reply.started": "2021-07-04T12:41:11.678508Z",
          "shell.execute_reply": "2021-07-04T12:41:11.68215Z"
        },
        "trusted": true,
        "id": "MhXqBi5NT946"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = ['nice food',\n",
        "          'amazing restaurant',\n",
        "          'too good',\n",
        "          'just loved it!',\n",
        "          'will go again',\n",
        "          'horrible food',\n",
        "          'never go there',\n",
        "          'poor service',\n",
        "          'poor quality',\n",
        "          'needs improvement']\n",
        "\n",
        "sentiment = np.array([1,1,1,1,1,0,0,0,0,0])  "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T13:04:20.827068Z",
          "iopub.execute_input": "2021-07-04T13:04:20.827473Z",
          "iopub.status.idle": "2021-07-04T13:04:20.832049Z",
          "shell.execute_reply.started": "2021-07-04T13:04:20.827439Z",
          "shell.execute_reply": "2021-07-04T13:04:20.831254Z"
        },
        "trusted": true,
        "id": "Gl2U8hklT946"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30\n",
        "print(\"on_hot example\",one_hot(\"amazing restaurant\",vocab_size))\n",
        "encoded_review = [one_hot(d,vocab_size) for d in reviews]\n",
        "print(encoded_review)\n",
        "\n",
        "#padding\n",
        "max_length = 3\n",
        "padding_reviews = pad_sequences(encoded_review ,maxlen=max_length,padding='post')\n",
        "#print(padding_reviews)\n",
        "\n",
        "#Embedding\n",
        "embeded_vector_size = 4\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,embeded_vector_size,input_length=max_length,name=\"embedding\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T13:35:17.077323Z",
          "iopub.execute_input": "2021-07-04T13:35:17.077642Z",
          "iopub.status.idle": "2021-07-04T13:35:17.108067Z",
          "shell.execute_reply.started": "2021-07-04T13:35:17.077614Z",
          "shell.execute_reply": "2021-07-04T13:35:17.107229Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-pfWiW8T947",
        "outputId": "b707736e-8a46-4087-c792-614ab6d6c88f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on_hot example [19, 8]\n",
            "[[12, 17], [19, 8], [18, 24], [13, 13, 8], [18, 27, 19], [2, 17], [5, 27, 12], [5, 10], [5, 26], [14, 8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padding_reviews\n",
        "Y = sentiment\n",
        "\n",
        "#Compile model\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T13:35:20.960695Z",
          "iopub.execute_input": "2021-07-04T13:35:20.961031Z",
          "iopub.status.idle": "2021-07-04T13:35:20.975214Z",
          "shell.execute_reply.started": "2021-07-04T13:35:20.960998Z",
          "shell.execute_reply": "2021-07-04T13:35:20.974417Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kImTcQrT948",
        "outputId": "a4722f0c-56a0-4487-c0a3-49a981f822f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 4)              120       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 13        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 133\n",
            "Trainable params: 133\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,Y,epochs=50,verbose=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T13:38:21.60914Z",
          "iopub.execute_input": "2021-07-04T13:38:21.609642Z",
          "iopub.status.idle": "2021-07-04T13:38:22.409029Z",
          "shell.execute_reply.started": "2021-07-04T13:38:21.609608Z",
          "shell.execute_reply": "2021-07-04T13:38:22.408256Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUuEHWJbT949",
        "outputId": "70263fea-2674-48cf-e802-e72f31c3f4a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f165b1108d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X,Y)\n",
        "accuracy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T13:39:54.245058Z",
          "iopub.execute_input": "2021-07-04T13:39:54.24542Z",
          "iopub.status.idle": "2021-07-04T13:39:54.295273Z",
          "shell.execute_reply.started": "2021-07-04T13:39:54.245386Z",
          "shell.execute_reply": "2021-07-04T13:39:54.294452Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSjbqsscT94-",
        "outputId": "627f0631-2372-4b7a-d1c3-db8b3ffca953"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6306 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have an interest in the embedding matrix. So we want to get the embedding layer weights."
      ],
      "metadata": {
        "id": "52TUm-47T94_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding_matrix model.get_layer('embedding').get_weights()[0]\n",
        "model.get_layer('embedding').get_weights()[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T13:46:36.271826Z",
          "iopub.execute_input": "2021-07-04T13:46:36.272119Z",
          "iopub.status.idle": "2021-07-04T13:46:36.279148Z",
          "shell.execute_reply.started": "2021-07-04T13:46:36.272092Z",
          "shell.execute_reply": "2021-07-04T13:46:36.278358Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6EdMt8aT94_",
        "outputId": "91a0bc2a-6504-4663-9531-33b3daabd52f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.0385818 ,  0.01196041,  0.08330416, -0.04809228],\n",
              "       [ 0.04928363,  0.03091284, -0.00218043, -0.03546967],\n",
              "       [ 0.02897159, -0.08566714,  0.06147001, -0.06250183],\n",
              "       [-0.0457347 ,  0.02563734, -0.00191243,  0.00735519],\n",
              "       [-0.03447334,  0.04667535,  0.0179052 ,  0.04737124],\n",
              "       [ 0.06568251, -0.05336633,  0.05199532, -0.00136059],\n",
              "       [-0.02487051,  0.03086315,  0.04834208,  0.00569927],\n",
              "       [-0.01147396,  0.0064138 ,  0.01900108, -0.03045193],\n",
              "       [ 0.0212338 , -0.08549266, -0.09383223,  0.01544772],\n",
              "       [ 0.0222734 ,  0.03541424, -0.04154212,  0.02096004],\n",
              "       [ 0.03723327,  0.09083919, -0.07665687,  0.03710005],\n",
              "       [ 0.01954024,  0.01567096, -0.01021005,  0.01254454],\n",
              "       [-0.01008093,  0.02703698, -0.04052323,  0.01923491],\n",
              "       [-0.01212789, -0.06453195, -0.06806654, -0.07130087],\n",
              "       [ 0.02827326, -0.08486904,  0.09327632, -0.06985669],\n",
              "       [ 0.04131815, -0.03515949, -0.0072787 ,  0.00013392],\n",
              "       [ 0.01054634, -0.04637089,  0.00918907, -0.0484867 ],\n",
              "       [-0.02545296,  0.0028329 ,  0.05190564, -0.07803503],\n",
              "       [-0.06896301,  0.0536689 , -0.05900615,  0.06012291],\n",
              "       [-0.00072147,  0.00133491, -0.00295607,  0.01325878],\n",
              "       [ 0.00849774, -0.01753539, -0.0076526 , -0.02390357],\n",
              "       [ 0.02737737, -0.04603967,  0.04535897, -0.0124467 ],\n",
              "       [-0.00599238,  0.03332135, -0.00162423, -0.04732252],\n",
              "       [-0.02981769, -0.00731677, -0.00705348, -0.03600407],\n",
              "       [-0.05770757, -0.00297162,  0.05293114, -0.0314495 ],\n",
              "       [-0.00188474,  0.02891358,  0.01999189,  0.01603521],\n",
              "       [ 0.07460233,  0.0617253 , -0.06205743,  0.0358348 ],\n",
              "       [ 0.07698561,  0.04366833, -0.06087789,  0.01205637],\n",
              "       [-0.0104318 ,  0.04098563,  0.03113243, -0.00234402],\n",
              "       [ 0.0207291 , -0.00430768, -0.02951194, -0.00010825]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.get_layer('embedding').get_weights()[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T13:49:30.385738Z",
          "iopub.execute_input": "2021-07-04T13:49:30.386102Z",
          "iopub.status.idle": "2021-07-04T13:49:30.394043Z",
          "shell.execute_reply.started": "2021-07-04T13:49:30.386067Z",
          "shell.execute_reply": "2021-07-04T13:49:30.393295Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UqmsfZtT95B",
        "outputId": "35b828c7-eb1f-4b86-ad2a-1308c8f12298"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word embedding of each word**: we make a python dictionnary with the unique word "
      ],
      "metadata": {
        "id": "uMxHcaMWT95B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique word of the reviews\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "tokenizer.word_index.keys()\n",
        "one_hot_distinct_word = [one_hot(d,vocab_size) for d in tokenizer.word_index.keys()]\n",
        "print(\"unique tokens:\",tokenizer.word_index.keys())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T14:22:48.347561Z",
          "iopub.execute_input": "2021-07-04T14:22:48.347898Z",
          "iopub.status.idle": "2021-07-04T14:22:48.353524Z",
          "shell.execute_reply.started": "2021-07-04T14:22:48.347866Z",
          "shell.execute_reply": "2021-07-04T14:22:48.352819Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kgZ_lzDT95C",
        "outputId": "d0f9037b-6098-4430-8586-fc0a1c899baf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique tokens: dict_keys(['food', 'go', 'poor', 'nice', 'amazing', 'restaurant', 'too', 'good', 'just', 'loved', 'it', 'will', 'again', 'horrible', 'never', 'there', 'service', 'quality', 'needs', 'improvement'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One encoding of unique words\n",
        "one_hot_distinct_word = [one_hot(d,vocab_size) for d in tokenizer.word_index.keys()]\n",
        "print(one_hot_distinct_word )\n",
        "one_hot_distinct_word_bis = [o[0] for o in one_hot_distinct_word]\n",
        "print(one_hot_distinct_word_bis) "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T14:28:56.934255Z",
          "iopub.execute_input": "2021-07-04T14:28:56.934563Z",
          "iopub.status.idle": "2021-07-04T14:28:56.939885Z",
          "shell.execute_reply.started": "2021-07-04T14:28:56.934535Z",
          "shell.execute_reply": "2021-07-04T14:28:56.939044Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlCQ6SZxT95D",
        "outputId": "acfd8f3c-4d90-409a-ef4a-d0b518a5f850"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[17], [27], [5], [12], [19], [8], [18], [24], [13], [13], [8], [18], [19], [2], [5], [12], [10], [26], [14], [8]]\n",
            "[17, 27, 5, 12, 19, 8, 18, 24, 13, 13, 8, 18, 19, 2, 5, 12, 10, 26, 14, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_token = list(tokenizer.word_index.keys())\n",
        "Embedding_matrix = model.get_layer('embedding').get_weights()[0]\n",
        "[(unique_token[i],Embedding_matrix[one_hot_distinct_word_bis[i]]) for i in range(0,len(one_hot_distinct_word_bis))]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T14:42:07.46357Z",
          "iopub.execute_input": "2021-07-04T14:42:07.464084Z",
          "iopub.status.idle": "2021-07-04T14:42:07.476599Z",
          "shell.execute_reply.started": "2021-07-04T14:42:07.464039Z",
          "shell.execute_reply": "2021-07-04T14:42:07.475682Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2O3JF8hT95E",
        "outputId": "62bf1f91-a13d-4976-ca4c-6a8e867a54fd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('food',\n",
              "  array([-0.02545296,  0.0028329 ,  0.05190564, -0.07803503], dtype=float32)),\n",
              " ('go',\n",
              "  array([ 0.07698561,  0.04366833, -0.06087789,  0.01205637], dtype=float32)),\n",
              " ('poor',\n",
              "  array([ 0.06568251, -0.05336633,  0.05199532, -0.00136059], dtype=float32)),\n",
              " ('nice',\n",
              "  array([-0.01008093,  0.02703698, -0.04052323,  0.01923491], dtype=float32)),\n",
              " ('amazing',\n",
              "  array([-0.00072147,  0.00133491, -0.00295607,  0.01325878], dtype=float32)),\n",
              " ('restaurant',\n",
              "  array([ 0.0212338 , -0.08549266, -0.09383223,  0.01544772], dtype=float32)),\n",
              " ('too',\n",
              "  array([-0.06896301,  0.0536689 , -0.05900615,  0.06012291], dtype=float32)),\n",
              " ('good',\n",
              "  array([-0.05770757, -0.00297162,  0.05293114, -0.0314495 ], dtype=float32)),\n",
              " ('just',\n",
              "  array([-0.01212789, -0.06453195, -0.06806654, -0.07130087], dtype=float32)),\n",
              " ('loved',\n",
              "  array([-0.01212789, -0.06453195, -0.06806654, -0.07130087], dtype=float32)),\n",
              " ('it',\n",
              "  array([ 0.0212338 , -0.08549266, -0.09383223,  0.01544772], dtype=float32)),\n",
              " ('will',\n",
              "  array([-0.06896301,  0.0536689 , -0.05900615,  0.06012291], dtype=float32)),\n",
              " ('again',\n",
              "  array([-0.00072147,  0.00133491, -0.00295607,  0.01325878], dtype=float32)),\n",
              " ('horrible',\n",
              "  array([ 0.02897159, -0.08566714,  0.06147001, -0.06250183], dtype=float32)),\n",
              " ('never',\n",
              "  array([ 0.06568251, -0.05336633,  0.05199532, -0.00136059], dtype=float32)),\n",
              " ('there',\n",
              "  array([-0.01008093,  0.02703698, -0.04052323,  0.01923491], dtype=float32)),\n",
              " ('service',\n",
              "  array([ 0.03723327,  0.09083919, -0.07665687,  0.03710005], dtype=float32)),\n",
              " ('quality',\n",
              "  array([ 0.07460233,  0.0617253 , -0.06205743,  0.0358348 ], dtype=float32)),\n",
              " ('needs',\n",
              "  array([ 0.02827326, -0.08486904,  0.09327632, -0.06985669], dtype=float32)),\n",
              " ('improvement',\n",
              "  array([ 0.0212338 , -0.08549266, -0.09383223,  0.01544772], dtype=float32))]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Embedding : CBOW model and Skip-Gram Model"
      ],
      "metadata": {
        "id": "iSj7gbX9T95F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec is a statistical method for efficiently learning a standalone word embedding from a text corpus. Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are:\n",
        "* **Continuous Bag-of-Words, or CBOW model.**\n",
        "* **Continuous Skip-Gram Model.**\n",
        "\n",
        "In the following image, the word in the blue box is called the target word and the words in the white boxes are called context words in a size 5 window.<br>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/WORDEMBEDING/Capture-de%CC%81cran-2020-09-18-a%CC%80-09.27.50.png.webp\" title=\"Title Tag Goes Here\" height=\"300\" width=\"450\" border=\"1px\">\n",
        "</center>\n",
        "\n",
        "* The **CBOW model** learns the embedding by predicting the current word based on its context. The continuous skip-gram model learns by predicting the surrounding words given a current word.\n",
        "\n",
        "* The continuous **skip-gram** model learns by predicting the surrounding words given a current word.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/WORDEMBEDING/Word2Vec-Training-Models.webp\" title=\"Title Tag Goes Here\" height=\"300\" width=\"450\" border=\"1px\">\n",
        "</center>\n",
        "<center>\n",
        "<title> Word2Vec Training Models<br>\n",
        "Taken from “Efficient Estimation of Word Representations in Vector Space”, 2013 </title>          \n",
        "</center>"
      ],
      "metadata": {
        "id": "QO2fZXH1T95F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip-Gram model mathematical description**<br>\n",
        "We have a corpus that leads to a N-size dictionary. In other words, We have N words in our vocabulary.<br>\n",
        "We go deeper to explain the skip-gram algorithm where the context is just one randomly picked nearby word.<br>\n",
        "\n",
        "Let the following sentence : \n",
        "* Machiavelli was an Italian Renaissance politician\n",
        "\n",
        "First we randomly pick up a word to be the context. Let's say we choose the word Politician. We also randomly pick up an other word to be the target. After, We randomly choose an other word within a window (+- n words of the context word) to be the target word. Let's say we choose the word Italian. We set c=Politician and t=Italian. We do it again in order to have new couples of context-target:\n",
        "$$\\begin{array}{c|c}\n",
        "Context=c & Target=t\\\\\n",
        "\\hline\n",
        "Italian  & Politician\\\\\n",
        "Italian  & Renaissance\\\\\n",
        "Renaissance & was\\\\\n",
        "politician & Machiavelli\n",
        "\\end{array}$$\n",
        "\n",
        "We set up supervised learning problem where given the context word, we are asked to predict the target word. The goal of setting up this supervised learning problem is not to do well on the supervised learning problem. **In fact, we want to use this learning problem to learn good word embeddings.**."
      ],
      "metadata": {
        "id": "r_9FWuA7T95G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to learn a mapping from some context to some target: $Context=c\\longrightarrow  Target=t$. Here the details of the model:<br>\n",
        "\n",
        "* $o_{c}\\longrightarrow E\\longrightarrow Eo_{c}=e_{c}\\longrightarrow \\underbrace{\\theta}_{Softmax}\\longrightarrow \\hat{y}$\n",
        "    * $o_{c}$ is the one_hote encoding for the input context vector\n",
        "    * $E$ is an embedding matrix\n",
        "    * $e_{c}$ is the embedding vector for the input context word\n",
        "    * $\\hat{y}$ is a N_sized probability vector where N is the vocabulary size. ($\\hat{y}\\in \\mathbb{R}^{N}$)\n",
        "    \n",
        "   \n",
        "The softmax model estimates the probability of  different target words t given the input context word c as\n",
        "$$P(t|c)=\\frac{\\exp(\\theta_{t}^{T}e_{c})}{\\sum_{j=1}^{N}\\exp(\\theta_{j}^{T}e_{c})}$$\n",
        "where $\\theta_{t}$ is the parameter associated with the output target word t. $P(t|c)$ is the chance of a particular word t being the label given the context word c.<br>\n",
        "The loss function for the softmax is the negative log-likelihood  \n",
        "$$\\mathcal{L}(\\hat{y},y)=-\\sum_{i=1}^{N}y_{i}\\log(\\hat{y}_{i})$$\n",
        "where $\\hat{y}_{i}=\\frac{\\exp(\\theta_{i}^{T}e_{c})}{\\sum_{j=1}^{N}\\exp(\\theta_{j}^{T}e_{c})}$ and $y$ is a one_hot vector."
      ],
      "metadata": {
        "id": "euJ97A1BT95H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GloVe word vectors\n",
        "GloVe stands for Global Vectors for Word Representation. GloVe is based on the word-word co-occurrence matrix denoted by $X$.\n",
        "* $X_{i,j}$ tabulate the number of times word j occurs in the context of word i.\n",
        "* $X_{i}=\\sum_{k}X_{i,k}$ is the number of times any word appears in the context of word i.\n",
        "\n",
        "The probability that word j appears in the context of word i is $P_{i,j}=P(j|i)=\\frac{X_{i,j}}{X_{i}}$"
      ],
      "metadata": {
        "id": "8dG2Ao3jT95I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind the word-word co-occurrence matrix is that Similar words tend to occur together and will have similar context. For example apple and mango are fruits. Apple and mango tend to have a similar context i.e fruit.We have to clarify two concepts : co-occurrence and Context.<br><br>\n",
        "**Co-occurrence:** For a given corpus, the co-occurrence of a pair of words is the number of times they have appeared together in a Context.<br>\n",
        "<br> **Context:**  \n",
        "In the following sentence, the green words are in a size 5 context window for the word ‘Fox’ and for calculating the co-occurrence only these words will be counted.\n",
        "<h1><font color=\"green\">Quick Brown</font><font color=\"red\"> Fox </font><font color=\"green\">Jump Over </font>The Lazy Dog</h1>\n",
        "\n",
        "\n",
        "Let us see context window for the word ‘Over’.\n",
        "\n",
        "<h1>Quick Brown<font color=\"green\"> Fox Jump</font><font color=\"red\"> Over </font><font color=\"green\">The Lazy </font>Dog</h1>"
      ],
      "metadata": {
        "id": "xIaWKVMBT95J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us take an example corpus to calculate a word-word co-occurrence matrix. Corpus = He is not lazy. He is intelligent. He is smart.\n",
        "$$\\begin{array}{c|c|c|c|c|c|c|}\n",
        "\\hline\n",
        " &He & is & not & lazy & intelligent & smart\\\\\n",
        " \\hline\n",
        " He & 0 & 4 & 2 & 1 & 2 & 1 \\\\\n",
        " \\hline\n",
        " is & 4& 0 & 1 & 2 & 2 & 1\\\\\n",
        " \\hline\n",
        " not & 2& 1 & 0 & 1 &0 &0\\\\\n",
        " \\hline\n",
        " lazy &1 &2 &1 & 0 &0 &0\\\\\n",
        " \\hline\n",
        " intelligent &2 &2 & 0 & 0 & 0 & 0 \\\\\n",
        " \\hline\n",
        " smart & 1& 1 &0 &0 & 0& 0 \\\\\n",
        " \\hline\n",
        "\\end{array}\n",
        "$$"
      ],
      "metadata": {
        "id": "E8miPxbkT95J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and\n",
        "that we are free to exchange the two roles.**\n",
        "\n",
        "The ratio $\\frac{P_{i,k}}{P_{j,k}}$ depends on three words i, j, and k, the most general model takes the form, \n",
        "$$F(w_{i},w_{j},\\tilde{w}_{k})=\\frac{P_{i,k}}{P_{j,k}}$$\n",
        "where $w \\in \\mathbb{R}^{d}$ are word vectors and $\\tilde{w} \\in \\mathbb{R}^{d}$ are separate context word vectors. In this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified parameters. We would like F to encode the information present the ratio $\\frac{P_{i,k}}{P_{j,k}}$ in the word vector space. Since vector spaces are inherently linear structures, the most natural way to do this is with vector differences. This leads to the following equation:\n",
        "$$F(w_{i},w_{j},\\tilde{w}_{k})=F(w_{i}-w_{j},\\tilde{w}_{k})=\\frac{P_{i,k}}{P_{j,k}}$$\n",
        "Next, we note that the arguments of F in are vectors while the right-hand side is a scalar. Then, we can first\n",
        "take the dot product of the arguments:\n",
        "$$F(w_{i}-w_{j},\\tilde{w}_{k})=F\\left((w_{i}-w_{j})^{T}\\tilde{w}_{k}\\right)=\\frac{P_{i,k}}{P_{j,k}}$$"
      ],
      "metadata": {
        "id": "QbMdeDP2T95K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<svg width=\"20\" height=\"20\">\n",
        "<rect width=\"20\" height=\"20\" style=\"fill:#E9E612;stroke-width:3;stroke:rgb(0,0,0)\" />\n",
        "</svg>"
      ],
      "metadata": {
        "id": "JnCBwMRWT95L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and\n",
        "that we are free to exchange the two roles. To do so consistently, we must not only exchange $w\\leftrightarrow \\tilde{w}$ but also $X\\leftrightarrow  X^{T}$ . We require that F be a homomorphism between the groups $(\\mathbb{R},+)$ and $(\\mathbb{R}_{>0},\\times)$,\n",
        "$$F\\left((w_{i}-w_{j})^{T}\\tilde{w}_{k}\\right)=\\frac{F(w_{i}^{T}\\tilde{w}_{k})}{F(w_{j}^{T}\\tilde{w}_{k})}=\\frac{P_{i,k}}{P_{j,k}}$$\n",
        "then \n",
        "$$F(w_{i}^{T}\\tilde{w}_{k})=P_{i,k}=\\frac{X_{i,k}}{X_{i}}$$\n",
        "Since $\\exp((w_{i}-w_{j})^{T}\\tilde{w}_{k})=\\frac{\\exp(w_{i}^{T}\\tilde{w}_{k})}{\\exp(w_{j}^{T}\\tilde{w}_{k})}$, we conclude that $F=\\exp$ and\n",
        "$$\\exp(w_{i}^{T}\\tilde{w}_{k})=P_{i,k}=\\frac{X_{i,k}}{X_{i}} \\Leftrightarrow w_{i}^{T}\\tilde{w}_{k}=\\ln(P_{i,k})=\\ln(X_{i,k})-\\ln(X_{i})$$\n",
        "\n",
        "We note that the above equation would exhibit the exchange symmetry if not for the $\\ln(X_{i})$ on the right-hand side. However, this term is independent of k so it can be absorbed into a bias $b_{i}$ for $w_{i}$. Finally, adding an additional bias $\\tilde{b_{k}}$ for $\\tilde{w_{k}}$ restores the symmetry,\n",
        "$$w_{i}^{T}\\tilde{w}_{k}+b_{i}+\\tilde{b_{k}}=\\ln(X_{i,k}) $$"
      ],
      "metadata": {
        "id": "V4JCD2FiT95M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can cast the above equation as least squared problem and we introduce a weighting function $f(X_{i,j})$ into the cost function:\n",
        "$$J=\\sum_{i,j=1}^{V}\\left(w_{i}^{T}\\tilde{w}_{j}+b_{i}+\\tilde{b_{j}}-\\ln(X_{i,j})\\right)^{2} $$\n",
        "* The goal of the weighting function f is to punish high frequency\n",
        "* V is the size of the vocabulary\n",
        "\n",
        "we defined $f$ as follow:\n",
        "$$f(x)=\\left\\{\\begin{array}{cc}\n",
        "(x/x_{max})^{\\alpha} & \\text{if $x<x_{max}$}\\\\\n",
        "1 & \\text{otherwise}\n",
        "\\end{array}\n",
        "\\right.$$\n",
        "\n",
        "In conclusion, GloVe algorithm finds a set of word vectors $\\{w_{1},\\dots,w_{n}\\}$ that minimize the cost function $\\sum_{i,j=1}^{V}\\left(w_{i}^{T}\\tilde{w}_{j}+b_{i}+\\tilde{b_{j}}-\\ln(X_{i,j})\\right)^{2}$."
      ],
      "metadata": {
        "id": "dc4MaWcHT95M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice"
      ],
      "metadata": {
        "id": "rYTANnPlYqF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/DATA/text_for_embedding.parquet.brotli\"\n",
        "text_for_embedding = pd.read_parquet(url)\n",
        "\n",
        "text_for_embedding.head(7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "_azaLv0umKtT",
        "outputId": "a71b912b-e54b-46b8-8cc0-d7be5deeab1c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bda333c1-e707-4c4a-a154-1e0c5605faa4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12775</th>\n",
              "      <td>1</td>\n",
              "      <td>Common sense is prevailing in Brexit negotiati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>930</th>\n",
              "      <td>1</td>\n",
              "      <td>Paul Manafort, the indicted former campaign ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4467</th>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Representative Mark Walker said after a m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8653</th>\n",
              "      <td>1</td>\n",
              "      <td>Hungarian Prime Minister Viktor Orban on Satur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21544</th>\n",
              "      <td>0</td>\n",
              "      <td>Way to go Granny! Perfect timing for your anno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13398</th>\n",
              "      <td>0</td>\n",
              "      <td>The DNC Action Committee announced on Facebook...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14975</th>\n",
              "      <td>0</td>\n",
              "      <td>The old reliable stone most commonly used by a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bda333c1-e707-4c4a-a154-1e0c5605faa4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bda333c1-e707-4c4a-a154-1e0c5605faa4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bda333c1-e707-4c4a-a154-1e0c5605faa4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       class                                               text\n",
              "12775      1  Common sense is prevailing in Brexit negotiati...\n",
              "930        1  Paul Manafort, the indicted former campaign ma...\n",
              "4467       1  U.S. Representative Mark Walker said after a m...\n",
              "8653       1  Hungarian Prime Minister Viktor Orban on Satur...\n",
              "21544      0  Way to go Granny! Perfect timing for your anno...\n",
              "13398      0  The DNC Action Committee announced on Facebook...\n",
              "14975      0  The old reliable stone most commonly used by a..."
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dh6hdg1Qo1l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We lowercase, remove the digit and remove the puntuation."
      ],
      "metadata": {
        "id": "LbkiZg3Ype_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(x):\n",
        "    punct_tag=re.compile(r'[^\\w\\s]')\n",
        "    new_text=punct_tag.sub(r'',x)\n",
        "    new_text = re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , new_text)\n",
        "    new_text = re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", new_text)\n",
        "    new_text = re.sub(r'[0-9]', '', new_text)\n",
        "    return(new_text.lower())\n",
        "\n",
        "text_for_embedding['text'] = text_for_embedding['text'].apply(lambda x:preprocess_text(x))\n"
      ],
      "metadata": {
        "id": "1fNP23CspwpC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sV3iiMpss-vG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}