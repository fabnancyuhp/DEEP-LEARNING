{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "word-embedding-word-vectorization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabnancyuhp/DEEP-LEARNING/blob/main/NOTEBOOKS/word_embedding_word_vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.<br>\n",
        "Some of NLP tasks include the following:\n",
        "* Speech recognition : converting voice data into text data\n",
        "* Part of speech tagging, also called grammatical tagging\n",
        "* Named entity recognition, or NEM : NEM identifies ‘Kentucky’ as a location or ‘Fred’ as a man's name.\n",
        "* Sentiment analysis attempts to extract Sentiments from text data.\n",
        "* ....\n",
        "\n",
        "When we deal with NLP in this cours, we're proned to handle text data. To apply algorithms such as random forests, neural networks on text data we need to convert text into vector representation. The different types of word vector representation can be broadly classified into two categories: **Frequency based vector ,Prediction based vector called word Embedding.**\n",
        "\n",
        "\n",
        "In this chapter, we show severals kind of word embedding:Embedding Layer,Word2Vec, GloVe : Global Vectors for Word Representation\n",
        "\n",
        "**Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space.**"
      ],
      "metadata": {
        "id": "1OLDCSy4T94R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus, dictionary and one-hote encoding\n",
        "A corpus is a set of documents. A document could be a review, a tweet, newspaper article,... . The dictionary is the set of the unique words (tokens) that appear in the corpus.<br>\n",
        "For example, \n",
        "* here we have a corpus with 3 documents.\n",
        "   * D1 'le beagle est il un bon chien de compagnie'\n",
        "   * D2 'le tour de france 2021 est maintenue'\n",
        "   * D3 \"l'euro 2020 se joue dans plusieurs pays\"\n",
        "* The dictionary is ['le', 'est', 'de', 'beagle', 'il', 'un', 'bon', 'chien', 'compagnie', 'tour', 'france', '2021', 'maintenue', \"l'euro\", '2020', 'se', 'joue', 'dans', 'plusieurs', 'pays']. \n",
        "* The vocabulary size is 20"
      ],
      "metadata": {
        "id": "Np6b_LMeT94U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For one-hot encoding, each word, or token, in a text corresponds to a vector element. O stands for one-hot. We give some one-hot encoding vectors. We have 20 words in our dictionary, then each one-hot vector has 20 components.\n",
        "* $O_{le}=O_{1}=[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]^{T}$\n",
        "* $O_{ beagle}=O_{4}=[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]^{T}$\n",
        "* $O_{compagnie} = O_{9}=[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]^{T}$\n",
        "* $O_{pays} = O_{20} = [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]^{T}$\n"
      ],
      "metadata": {
        "id": "tUesEJBjT94X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we list some drawbacks of the one-hot encoding:\n",
        "* High dimensionality : The number of dimension is equal to the number of unique words in the corpus\n",
        "* Sparse : Only 1 non-zero value\n",
        "* One-hot encoding does not catch the words meaning "
      ],
      "metadata": {
        "id": "B9gHC6-BT94Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count-Vector Matrix\n",
        "\n",
        "Consider a Corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document di. "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-27T09:01:12.557356Z",
          "iopub.execute_input": "2021-06-27T09:01:12.557702Z",
          "iopub.status.idle": "2021-06-27T09:01:12.565264Z",
          "shell.execute_reply.started": "2021-06-27T09:01:12.557673Z",
          "shell.execute_reply": "2021-06-27T09:01:12.564088Z"
        },
        "id": "6RDF5ytWT94e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let the following Corpus of 3 documents\n",
        "\n",
        "* D1 'Boxer is a German dog', \n",
        "* D2 'Bulldog is an English dog',\n",
        "* D3 'Stellantis is a merger between PSA and FCA' \n",
        "\n",
        "This corpus has 13 unique tokens that form the following dictionary : ['Boxer', 'Bulldog','English', 'FCA', 'German', 'PSA', 'Stellantis', 'an', 'and', 'between','dog','is' 'merger'].<br><br>\n",
        "The count matrix M of size 3 X 13 will be represented as:\n",
        "$$\n",
        "\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n",
        "  \\hline\n",
        "   & boxer & Bulldog & English &  FCA & German &  PSA& Stellantis& an& and& between& dog& is& merger\\\\\n",
        "  \\hline\n",
        "  doc1 & 1& 0& 0& 0& 1& 0& 0& 0& 0& 0& 1& 1& 0 \\\\\n",
        "  \\hline\n",
        "  doc2 & 0& 1& 1& 0& 0& 0& 0& 1& 0& 0& 1& 1& 0 \\\\\n",
        "  \\hline\n",
        "  doc3 & 0& 0& 0& 1& 0& 1& 1& 0& 1& 1& 0& 1& 1\\\\\n",
        "  \\hline\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "The result of a count_vector process on a corpus is a sparse matrix. Consider if you had a corpus with 20,000 unique words: a single short document in that corpus of, perhaps, 40 words would be represented by a matrix with 20,000 rows (one for each unique word) with a maximum of 40 non-zero matrix elements (and potentially far-fewer if there are a high number of non-unique words in this collection of 40 words). This leaves a lot of zeroes, and can end up taking a large amount of memory to house these spare representations."
      ],
      "metadata": {
        "id": "iDVegGf1T94g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Representation/ Featurized representation\n",
        "\n",
        "\n",
        "it is obvious that one-hot encoding has drawbacks:\n",
        "* the inner product of two differents one-hot vectors is 0\n",
        "* the cosine similarity between the one-hot vectors of any two different words is 0\n",
        "* The euclidianne distance between 2 differents one-hot vectors is always the same\n",
        "\n",
        "As As consequences:\n",
        "* Since the cosine similarity between the one-hot vectors of any two different words is 0, it is difficult to use the one-hot vector to accurately represent the similarity between multiple different words.\n",
        "* one-hot encoding and bag-of-words models (i.e. using dummy variables to represent the presence or absence of a word in an observation, i.e. a sentence) do not capture information about a word's meaning or context. \n",
        "* One-hot encodings do not capture syntactic (structure) and semantic (meaning) relationships across collections of words and, therefore, represent language in a very naive way.<br>\n",
        "\n",
        "In contrast, word vectors represent words as multidimensional continuous floating point numbers where semantically similar words are mapped to proximate points in geometric space.This means that words such as wheel and engine should have similar word vectors to the word car (because of the similarity of their meanings), whereas the word banana should be quite distant. The beauty of representing words as vectors is that they lend themselves to mathematical operators. **Word vectors are simply vectors of numbers that represent the meaning of a word**. For example, we can add and subtract vectors — the canonical example here is showing that by using word vectors we can determine that:\n",
        "\n",
        "* **king - man + woman = queen**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/WORDEMBEDING/kingandkeen.png\" title=\"Title Tag Goes Here\" height=\"200\" width=\"250\" border=\"1px\">\n",
        "</center>\n",
        "\n",
        "The numbers in the word vector represent the word's distributed weight across dimensions. In a simplified sense, each dimension represents a meaning and the word's numerical weight on that dimension captures the closeness of its association with and to that meaning. Thus, the semantics of the word are embedded across the dimensions of the vector.\n",
        "\n",
        "A word vector is a featurized representation of a word. Here, we represent the words Man, Woman, King, Queen, Apple, Orange with the features Gender, Royale, Age, Food. \n",
        "\n",
        "$$\\begin{array}{c|cccccc|}\n",
        "\\hline\n",
        "&Man & Woman & King & Qween & Apple & Orange\\\\\n",
        "\\hline\n",
        "Gender & -1 & 1  &-0.95  & 0.97  &0.00  &0.01    \\\\\n",
        "\\hline\n",
        "Royal &0.01 & 0.02 & 0.93 & 0.95 & -0.01 & 0.00\\\\\n",
        "\\hline\n",
        "Age &  0.03 &0.02  &0.7  &0.69  &0.03 &-0.02      \\\\\n",
        "\\hline \n",
        "Food & 0.04 &0.01 &0.02 & 0.01 &0.95 & 0.97 \\\\\n",
        "\\hline\n",
        "\\end{array}$$\n",
        "In the above matrix each column is the vector representation of a word. This particular matrix is called embedding matrix. Each column vector is a word embedding.<br><br>\n",
        "**Given the vectors of two words, we can determine their similarity**.\n",
        "Apple and Orange are fruits then the cosine similarity between their word vectors should be close to 1. The word vectors of Apple and Orange are respectively $e_{Apple}=[0.00,-0.01,0.03,0.95]^{T}$ and $e_{Orange}=[0.01,0.00,-0.02,0.97]^{T}$. $e$ stands for embedding. We have\n",
        "$$sim(Apple,Orange)=\\frac{e_{Apple}^{T}e_{Orange}}{\\left\\lVert e_{Apple} \\right\\rVert \\left\\lVert e_{Orange} \\right\\rVert }=0.99853041412809$$\n",
        "Apple should be quit distant to King. We compute the cosine similarity between these 2 words:\n",
        "$$sim(Apple,Orange)=\\frac{e_{Apple}^{T}e_{King}}{\\left\\lVert e_{Apple} \\right\\rVert \\left\\lVert e_{King} \\right\\rVert }=0.021494708641488013$$\n",
        "These two word vectors are close to the orthogonality. The angle between these two vectors is about 88 degree."
      ],
      "metadata": {
        "id": "qKPlr_fwT94p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Are Word Embeddings?\n",
        "We give severals definitions of word embedding:\n",
        "* Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n",
        "* A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
        "* Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.\n",
        "\n",
        "Key to the approach is the idea of using a dense distributed representation for each word. Conceptually it involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension."
      ],
      "metadata": {
        "id": "Pyv1kdeZT94z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Layer : From one-hote to word embedding\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-17T12:37:53.828214Z",
          "iopub.execute_input": "2021-06-17T12:37:53.828776Z",
          "iopub.status.idle": "2021-06-17T12:37:53.833238Z",
          "shell.execute_reply.started": "2021-06-17T12:37:53.828711Z",
          "shell.execute_reply": "2021-06-17T12:37:53.832561Z"
        },
        "id": "kxrwtMj6T941"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we use supervised learning to get vectors representations of words from a corpus. We Consider a corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. Each document is a review. The N tokens will form our dictionary = {word1,....,wordN}.The one-hot encoding vector of the wordi $O_{i}$ is the N-dimensional vector where each component is equal 0 except the ith one which  is equal to 1. Each document is labeled as 0 (negative) or 1 (positive). We want to represent each dictionary word with K components vector:\n",
        "* word1 vector is $e_{1}=e_{word1}=[w_{11},\\dots,w_{k1}]^{T}$\n",
        "* word2 vector is $e_{2}=e_{word1}=[w_{12},\\dots,w_{k2}]^{T}$\n",
        "* wordN vector is $e_{N}=e_{word1}=[w_{1N},\\dots,w_{kN}]^{T}$\n",
        "\n",
        "Generaly $K<N$ and the K-dimentional vector space is dense. For exemple, we could have N=10000 and K=300.\n",
        "Each word is represented with K features corresponding to the K vector components. We introduce the kxv embedding matrice E:\n",
        "$$\\begin{array}{c|ccc|}\n",
        "& word1&\\dots \\dots & wordN\\\\\n",
        "\\hline\n",
        "feature1 & w_{11}&\\dots \\dots & w_{1N}\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "featurek & w_{k1}&\\dots \\dots & w_{kN}\\\\\n",
        "\\hline\n",
        "\\end{array}$$\n",
        "The $w_{ij}s$ are trainables parameters. "
      ],
      "metadata": {
        "id": "x_yxn2wUT942"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\{O_{1},\\dots,O_{N} \\}$ be the set of the one-hot encodings of the dictionary. The one-hot encoding vector of the wordi $O_{i}$ is the N-dimensional vector where each component is equal 0 except the ith one which  is equal to 1 . The matrix multiplication between E and $O_{i}$ is equal to the word vector embedding of wordi:\n",
        "$$EO_{i}=e_{i} $$\n",
        "The matrix E is randomly initialized. We train a neural network to determin wether or not a review is positive. The matrix embedding coeficiants are fitted during the training stage of this neural network. After the neural network is trained we will get word embeddings as a side effect. So the problem for review classification is almost like a fake problem. In fact we care about word embeddings. In other words, we care about the matrix E.<br>\n",
        "In this neural network, we vertically stack all word vectors embedding into a single vector. This step is called flattering. The flattering step produces a document vector.<br> \n",
        "The documents in the corpus don't have the same length. In other words, the number of tokens differs from one document to another. To cop this problem, we use a padding step to make all vector documents same-sized.<br>\n",
        "$\\hat{Y}$ is an $\\mathbb{R}^{2}$ vector since we deal with a binary classification problem. In the case of multiclass classification problem, $\\hat{Y}$ is an $\\mathbb{R}^{n}$ vector where n is the number of classe label."
      ],
      "metadata": {
        "id": "Gj2AON3jT942"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**<br>\n",
        "An embedding layer, for lack of a better name, is a word embedding that is learned jointly with a neural network model on a specific natural language processing task, such as language modeling or document classification."
      ],
      "metadata": {
        "id": "83COkQ8jT944"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Embedding : CBOW model and Skip-Gram Model"
      ],
      "metadata": {
        "id": "iSj7gbX9T95F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec is a statistical method for efficiently learning a standalone word embedding from a text corpus. Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are:\n",
        "* **Continuous Bag-of-Words, or CBOW model.**\n",
        "* **Continuous Skip-Gram Model.**\n",
        "\n",
        "\n",
        "In the following sentence, the word in red is called the target word and the words in green are called context words in a size 5 window.\n",
        "\n",
        "<h3><font color=\"green\"> The Quick</font><font color=\"red\"> Brown </font><font color=\"green\">Fox Jumps </font>Over The Lazy Dog</h3>\n",
        "\n",
        "<h3>The<font color=\"green\"> Quick Brown</font><font color=\"red\"> Fox</font><font color=\"green\">Jumps Over</font>The Lazy Dog</h3>\n",
        "\n",
        "<h3>The Quick <font color=\"green\"> Brown Fox</font><font color=\"red\"> Jumps</font><font color=\"green\"> Over The</font> Lazy Dog</h3>\n",
        "\n",
        "* The **CBOW model** learns the embedding by predicting the current word based on its context. The continuous skip-gram model learns by predicting the surrounding words given a current word.\n",
        "\n",
        "* The continuous **skip-gram** model learns by predicting the surrounding words given a current word.\n"
      ],
      "metadata": {
        "id": "QO2fZXH1T95F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip-Gram model mathematical description**<br>\n",
        "We have a corpus that leads to a N-size dictionary. In other words, We have N words in our vocabulary.<br>\n",
        "We go deeper to explain the skip-gram algorithm where the context is just one randomly picked nearby word.<br>\n",
        "\n",
        "Let the following sentence : \n",
        "* Machiavelli was an Italian Renaissance politician\n",
        "\n",
        "First we randomly pick up a word to be the context. Let's say we choose the word Politician. We also randomly pick up an other word to be the target. After, We randomly choose an other word within a window (+- n words of the context word) to be the target word. Let's say we choose the word Italian. We set c=Politician and t=Italian. We do it again in order to have new couples of context-target:\n",
        "$$\\begin{array}{c|c}\n",
        "Context=c & Target=t\\\\\n",
        "\\hline\n",
        "Italian  & Politician\\\\\n",
        "Italian  & Renaissance\\\\\n",
        "Renaissance & was\\\\\n",
        "politician & Machiavelli\n",
        "\\end{array}$$\n",
        "\n",
        "We set up supervised learning problem where given the context word, we are asked to predict the target word. The goal of setting up this supervised learning problem is not to do well on the supervised learning problem. **In fact, we want to use this learning problem to learn good word embeddings.**."
      ],
      "metadata": {
        "id": "r_9FWuA7T95G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to learn a mapping from some context to some target: $Context=c\\longrightarrow  Target=t$. Here the details of the model:<br>\n",
        "\n",
        "* $o_{c}\\longrightarrow E\\longrightarrow Eo_{c}=e_{c}\\longrightarrow \\underbrace{\\theta}_{Softmax}\\longrightarrow \\hat{y}$\n",
        "    * $o_{c}$ is the one_hote encoding for the input context vector\n",
        "    * $E$ is an embedding matrix\n",
        "    * $e_{c}$ is the embedding vector for the input context word\n",
        "    * $\\hat{y}$ is a N_sized probability vector where N is the vocabulary size. ($\\hat{y}\\in \\mathbb{R}^{N}$)\n",
        "    \n",
        "   \n",
        "The softmax model estimates the probability of  different target words t given the input context word c as\n",
        "$$P(t|c)=\\frac{\\exp(\\theta_{t}^{T}e_{c})}{\\sum_{j=1}^{N}\\exp(\\theta_{j}^{T}e_{c})}$$\n",
        "where $\\theta_{t}$ is the parameter associated with the output target word t. $P(t|c)$ is the chance of a particular word t being the label given the context word c.<br>\n",
        "The loss function for the softmax is the negative log-likelihood  \n",
        "$$\\mathcal{L}(\\hat{y},y)=-\\sum_{i=1}^{N}y_{i}\\log(\\hat{y}_{i})$$\n",
        "where $\\hat{y}_{i}=\\frac{\\exp(\\theta_{i}^{T}e_{c})}{\\sum_{j=1}^{N}\\exp(\\theta_{j}^{T}e_{c})}$ and $y$ is a one_hot vector."
      ],
      "metadata": {
        "id": "euJ97A1BT95H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GloVe word vectors\n",
        "GloVe stands for Global Vectors for Word Representation. GloVe is based on the word-word co-occurrence matrix denoted by $X$.\n",
        "* $X_{i,j}$ tabulate the number of times word j occurs in the context of word i.\n",
        "* $X_{i}=\\sum_{k}X_{i,k}$ is the number of times any word appears in the context of word i.\n",
        "\n",
        "The probability that word j appears in the context of word i is $P_{i,j}=P(j|i)=\\frac{X_{i,j}}{X_{i}}$"
      ],
      "metadata": {
        "id": "8dG2Ao3jT95I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind the word-word co-occurrence matrix is that Similar words tend to occur together and will have similar context. For example apple and mango are fruits. Apple and mango tend to have a similar context i.e fruit.We have to clarify two concepts : co-occurrence and Context.<br><br>\n",
        "**Co-occurrence:** For a given corpus, the co-occurrence of a pair of words is the number of times they have appeared together in a Context.<br>\n",
        "<br> **Context:**  \n",
        "In the following sentence, the green words are in a size 5 context window for the word ‘Fox’ and for calculating the co-occurrence only these words will be counted.\n",
        "<h1><font color=\"green\">Quick Brown</font><font color=\"red\"> Fox </font><font color=\"green\">Jump Over </font>The Lazy Dog</h1>\n",
        "\n",
        "\n",
        "Let us see context window for the word ‘Over’.\n",
        "\n",
        "<h3>Quick Brown<font color=\"green\"> Fox Jump</font><font color=\"red\"> Over </font><font color=\"green\">The Lazy </font>Dog</h3>"
      ],
      "metadata": {
        "id": "xIaWKVMBT95J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and\n",
        "that we are free to exchange the two roles.**\n",
        "\n",
        "The ratio $\\frac{P_{i,k}}{P_{j,k}}$ depends on three words i, j, and k, the most general model takes the form, \n",
        "$$F(w_{i},w_{j},\\tilde{w}_{k})=\\frac{P_{i,k}}{P_{j,k}}$$\n",
        "where $w \\in \\mathbb{R}^{d}$ are word vectors and $\\tilde{w} \\in \\mathbb{R}^{d}$ are separate context word vectors. In this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified parameters. We would like F to encode the information present the ratio $\\frac{P_{i,k}}{P_{j,k}}$ in the word vector space. Since vector spaces are inherently linear structures, the most natural way to do this is with vector differences. This leads to the following equation:\n",
        "$$F(w_{i},w_{j},\\tilde{w}_{k})=F(w_{i}-w_{j},\\tilde{w}_{k})=\\frac{P_{i,k}}{P_{j,k}}$$\n",
        "Next, we note that the arguments of F in are vectors while the right-hand side is a scalar. Then, we can first\n",
        "take the dot product of the arguments:\n",
        "$$F(w_{i}-w_{j},\\tilde{w}_{k})=F\\left((w_{i}-w_{j})^{T}\\tilde{w}_{k}\\right)=\\frac{P_{i,k}}{P_{j,k}}$$"
      ],
      "metadata": {
        "id": "QbMdeDP2T95K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<svg width=\"20\" height=\"20\">\n",
        "<rect width=\"20\" height=\"20\" style=\"fill:#E9E612;stroke-width:3;stroke:rgb(0,0,0)\" />\n",
        "</svg>"
      ],
      "metadata": {
        "id": "JnCBwMRWT95L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and\n",
        "that we are free to exchange the two roles. To do so consistently, we must not only exchange $w\\leftrightarrow \\tilde{w}$ but also $X\\leftrightarrow  X^{T}$ . We require that F be a homomorphism between the groups $(\\mathbb{R},+)$ and $(\\mathbb{R}_{>0},\\times)$,\n",
        "$$F\\left((w_{i}-w_{j})^{T}\\tilde{w}_{k}\\right)=\\frac{F(w_{i}^{T}\\tilde{w}_{k})}{F(w_{j}^{T}\\tilde{w}_{k})}=\\frac{P_{i,k}}{P_{j,k}}$$\n",
        "then \n",
        "$$F(w_{i}^{T}\\tilde{w}_{k})=P_{i,k}=\\frac{X_{i,k}}{X_{i}}$$\n",
        "Since $\\exp((w_{i}-w_{j})^{T}\\tilde{w}_{k})=\\frac{\\exp(w_{i}^{T}\\tilde{w}_{k})}{\\exp(w_{j}^{T}\\tilde{w}_{k})}$, we conclude that $F=\\exp$ and\n",
        "$$\\exp(w_{i}^{T}\\tilde{w}_{k})=P_{i,k}=\\frac{X_{i,k}}{X_{i}} \\Leftrightarrow w_{i}^{T}\\tilde{w}_{k}=\\ln(P_{i,k})=\\ln(X_{i,k})-\\ln(X_{i})$$\n",
        "\n",
        "We note that the above equation would exhibit the exchange symmetry if not for the $\\ln(X_{i})$ on the right-hand side. However, this term is independent of k so it can be absorbed into a bias $b_{i}$ for $w_{i}$. Finally, adding an additional bias $\\tilde{b_{k}}$ for $\\tilde{w_{k}}$ restores the symmetry,\n",
        "$$w_{i}^{T}\\tilde{w}_{k}+b_{i}+\\tilde{b_{k}}=\\ln(X_{i,k}) $$"
      ],
      "metadata": {
        "id": "V4JCD2FiT95M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can cast the above equation as least squared problem and we introduce a weighting function $f(X_{i,j})$ into the cost function:\n",
        "$$J=\\sum_{i,j=1}^{V}f(X_{i,j})\\left(w_{i}^{T}\\tilde{w}_{j}+b_{i}+\\tilde{b_{j}}-\\ln(X_{i,j})\\right)^{2} $$\n",
        "* The goal of the weighting function f is to punish high frequency\n",
        "* V is the size of the vocabulary\n",
        "\n",
        "we defined $f$ as follow:\n",
        "$$f(x)=\\left\\{\\begin{array}{cc}\n",
        "(x/x_{max})^{\\alpha} & \\text{if $x<x_{max}$}\\\\\n",
        "1 & \\text{otherwise}\n",
        "\\end{array}\n",
        "\\right.$$\n",
        "\n",
        "In conclusion, GloVe algorithm finds a set of word vectors $\\{w_{1},\\dots,w_{n}\\}$ that minimize the cost function $J$."
      ],
      "metadata": {
        "id": "dc4MaWcHT95M"
      }
    }
  ]
}