{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INTRODUCTION AND SOME DEFINITIONS\nhttps://ledatascientist.com/amp/a-la-decouverte-du-transformer/<br>\nhttp://peterbloem.nl/blog/transformers<br>\nhttps://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&utm_medium=comprehensive-guide-attention-mechanism-deep-learning<br>\nhttps://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape<br>\nhttps://mail.google.com/mail/u/0/?pli=1#label/DEEP+LEARNING/QgrcJHrhstXrnCBhtNtRjFPhWQHbDbqwpJb?projector=1<br>\nhttps://www.d2l.ai/chapter_attention-mechanisms/index.html<br>\nhttps://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html<br>\nhttps://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.YMyvEvkzZPY","metadata":{}},{"cell_type":"markdown","source":"In this section, we define some key definitions useful to understand the concepts we'll show in this chapter.\n* **Seq2seq Model** : A model which transforms a sequence to an other sequence is called a Sequence-to-Sequence Model. For example, translation of English sentences to German sentences is a sequence-to-sequence task.<br>\n* **Pre Trained model** : Pre Trained models are machine learning models that are trained, developed and made available by other developers. They are generally used to solve problems based on deep learning and are always trained on a very large dataset.<br>\n* **Pre Trained model** : Simply put, a pre-trained model is a model created by some one else to solve a similar problem. Instead of building a model from scratch to solve a similar problem, you use the model trained on other problem as a starting point.<br>\n* **Fine-tuning** Fine-tuning means taking weights of a trained neural network and use it as initialization for a new model being trained on data from the same domain. It is used to speed up the training and overcome small dataset size. There are various strategies, such as training the whole initialized network or \"freezing\" some of the pre-trained weights (usually whole layers).<br>\n* **Attention (from Wikipedia)** : In the context of neural networks, attention is a technique that mimics cognitive attention. The effect enhances the important parts of the input data and fades out the rest -- the thought being that the network should devote more computing power on that small but important part of the data. Which part of the data is more important than others depends on the context and is learned through training data. We go deeper the attention machanism later in this chapter.<br>\n* **Word Embedding**: In natural language processing (NLP), Word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.","metadata":{}},{"cell_type":"markdown","source":"The Transformer’s architecture does not contain any recurrence or convolution andhence has no notion of word order. All the words of the input sequence are fed to thenetwork with no special order or position as they all flow simultaneously through theEncoder and decoder stack.\nTo understand the meaning of a sentence, it is essential to understand the position andthe order of words.","metadata":{}},{"cell_type":"markdown","source":"**Transformer:** A transformer is a deep learning model that adopts the mechanism of attention, weighing the influence of different parts of the input data. It is used primarily in the field of natural language processing (NLP).<br>\n\nTransformers are designed to handle sequential input data, such as natural language, for tasks such as translation and text summarization.<br>\nUnlike RNNs, transformers do not require that the sequential data be processed in order. This fact allows transformer to be trained using parallelizing computation.<br>\n**In this course, transformers are pre-trained models on a huge amount of data using parallelizing computing. We use Transformers for fine-tuning purposes.**\n","metadata":{}},{"cell_type":"markdown","source":"# Positional encoding\nhttps://kazemnejad.com/blog/transformer_architecture_positional_encoding/<br>\nhttp://peterbloem.nl/blog/transformers<br>\nWhen we process text data we usually convert the text into vectors using word embedding. Position and order of words are the essential parts of any language. Unfortunately Word embedding doesn't care position and order of words. To cope this issus we use positional encoding. The aim of positional coding is to create positional embedding vectors and to add it to the current embedding vectors <br>\nLet $x^{<1>},\\dots,x^{<n>}$ be word embeddings of n words from a sentence. Then $x^{<1>}\\in \\mathbb{R}^{d},\\dots,x^{<n>}\\in \\mathbb{R}^{d}$ are d-dimensional vectors that represent the sentence words.\nPositional encoding yields a set of vectors $p^{<1>}\\in \\mathbb{R}^{d},\\dots,p^{<1>}\\in \\mathbb{R}^{d}$ satisfying the following criteria:\n* It should output a unique encoding for each time-step (word’s position in a sentence)\n* Distance between any two time-steps should be consistent across sentences with different lengths.\n* Our model should generalize to longer sentences without any efforts. Its values should be bounded.\n* It must be deterministic.\n\nThe i th component of $p^{<t>}$ is given by\n$$p^{<t>}_{i}=\\left\\{\\begin{array}{l} \n\\sin(w_{k}t), \\text{ if i=2k}\\\\\n\\cos(w_{k}t), \\text{ if i=2k+1}\n\\end{array}\\right.$$\nwhere \n$$w_{k}=\\frac{1}{10000^{2k/d}}$$\nWe add the positional encoding to the input embedding to get new word vectors representations:\n$x^{<t>}\\leftarrow x^{<t>}+p^{<t>}$ for each time step t.","metadata":{}},{"cell_type":"markdown","source":"# Self-attention\n## Introduction\nThe fundamental operation of any transformer architecture is the self-attention operation. The concept of attention can be modeled in machine learning where attention is a simple weighting of data. In the attention mechanism, the more informative parts of data are given larger weights for the sake of more attention.<br>\nTake a look at the following sentence:<br>\nThis cat is lazy, he is always sleeping. Let $X_{this},X_{cat},X_{is},X_{lazy},X_{he},X_{is},X_{always},X_{sleeping}$ be word vectors. We want to understand how the word cate relates to other. The dot product between the two words vectors $X_{cat}$ and $X_{he}$ gives you a score that measure the link between that tow words. Since there is a strong relation between cat and he, $X_{cat}^{T}X_{he}$ should be a positive term. On the other hand, the dot product $X_{cat}^{T}X_{always}$ should be close to 0 or negative.\n## Mechanism of Self-attention \n$X^{<1>}\\in\\mathbb{R}^{d},\\dots,X^{<n>}\\in\\mathbb{R}^{d}$ are word vectors from the sum of word embedding and positional encoding vectors. The similarity between $X^{<t>}$ and $X^{<t^{\\prime}>}$ is given by $s^{<t,t^{\\prime}>}=X^{<t>T}X^{<t^{\\prime}>}$.<br>\nThe dot product gives us a value anywhere between negative and positive infinity, so we apply a softmax to map the values to [0,1] and to ensure that they sum to 1 over the whole sequence:\n$$\\alpha^{<t,t^{\\prime}>}=\\frac{\\exp(s^{<t,t^{\\prime}>})}{\\sum_{t^{\\prime}=1}^{n}\\exp(s^{<t,t^{\\prime}>})}$$\n$\\alpha^{<t,t^{\\prime}>}$ measures how much the word $X^{<t>}$ is related to the word $X^{<𝑡^{\\prime}>}$. \nIn other words, $\\alpha^{<t,t^{\\prime}>}$ is the amount of self attention between the words $X^{<t>}$ and $X^{<𝑡^{\\prime}>}$.\nFor all t, the self attention operation simply takes a weighted average over all the input vectors to produce the attention at time step t $Y^{<t>}$ such that:\n$$Y^{<t>}=\\sum_{t^{\\prime}=1}^{n}\\alpha^{<t,t^{\\prime}>}X^{<t^{\\prime}>}$$\n$Y^{<t>}$ is the final embedding at time step t. The mechanism of Self-attention yields new word embedding representations.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Self-attention used in modern transformers: \nIn this part, we show how an attention layer works, in modern transformers. \nIn this section, $X^{<1>}\\in\\mathbb{R}^{d},\\dots,X^{<n>}\\in\\mathbb{R}^{d}$ are word vectors from the sum of word embedding and positional encoding vectors.<br>\nThe actual self-attention used in modern transformers relies on 3 different representations:\n* For the first represntation, the model will read the word as the work being processed.\n* For the second representation, the model will read the word as the word being compared to the main processed word.\n* For the third representation, the model will read the word as a way to evaluate what information it should keep from that word to build the final representation of the main word.\n\n**An attention layer yields new word embedding representations.**\n\nGiven a vector word, we can use it as the main word being processed, we can use it as a word being processed with a main word, and we can use it as a way to value how much this other word should influence the new representation of the main word.\n\nIn other words the 3 different representations are:\n* **The query** is the main representation of the word for which we're building a new representation (the main word)\n* **The key** is the representation of all words when these words are being compared with a main word\n* **The value** is the representation of all words--including the main word--we use to produce the final embedding of the main word.","metadata":{}},{"cell_type":"markdown","source":"The query, key,and value are projection of the words into p-dimensional,p-dimensional, and r-dimensional  subspaces,respectively:<br>\n$$\\begin{eqnarray*}\nq^{<t>}&=&W_{Q}^{T}X^{<t>}\\in \\mathbb{R}^{p}\\\\\nk^{<t>}&=&W_{k}^{T}X^{<t>}\\in \\mathbb{R}^{p}\\\\\nv^{<t>}&=&W_{V}^{T}X^{<t>}\\in \\mathbb{R}^{r}\n\\end{eqnarray*}$$\nWhere $W_{Q}\\in \\mathbb{R}^{d\\times p},W_{K}\\in \\mathbb{R}^{d\\times p},W_{V}\\in \\mathbb{R}^{d\\times r}$ are the projection matrices into the low-dimensional query, key, and value subspaces, respectively. The similarity\nmeasures between $X^{<t>}$ and $X^{<t^{\\prime}>}$ is given by the scaled inner product:\n$$s^{<t,t^{\\prime}>}=\\frac{q^{<t>T}k^{<t^{\\prime}>}}{\\sqrt{p}}=\\frac{X^{<t>T}W_{Q}W_{K}^{T}X^{<t^{\\prime}>}}{\\sqrt{p}}$$\nThe attention weights are given by:\n$$\\alpha^{<t,t^{\\prime}>}=\\frac{\\exp(s^{<t,t^{\\prime}>})}{\\sum_{t^{\\prime}=1}^{n}\\exp(s^{<t,t^{\\prime}>})}$$\n$\\alpha^{<t,t^{\\prime}>}$ is the amount of self attention between the words $X^{<t>}$ and $X^{<𝑡^{\\prime}>}$.<br>\nWe have to notice that:\n$$(\\alpha^{<t,1>},\\dots,\\alpha^{<t,n>})=\\mathrm{softmax}\\left(s^{<t,1>},\\dots,s^{<t,n>}\\right)  $$\nThe attention vector at time step t is given by:\n$$Y^{<t>}=\\sum_{t^{\\prime}=1}^{n}\\alpha^{<t,t^{\\prime}>}v^{<t^{\\prime}>}$$","metadata":{}},{"cell_type":"markdown","source":"<p style=\"float: right; clear: right\"><img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/TRANSFORMERS/SCALDE.png\" height=\"300\" width=\"250\" border=\"1px\"></p>\nNow, let introduce:\n\n* $X=[X^{<1>},\\dots,X^{<n>}]\\in\\mathbb{R}^{d\\times n}$\n* $Q=[q^{<1>},\\dots,q^{<n>}]\\in\\mathbb{R}^{p\\times n}$\n* $K=[k^{<1>},\\dots,k^{<n>}]\\in\\mathbb{R}^{p\\times n}$\n* $V=[v^{<1>},\\dots,v^{<n>}]\\in\\mathbb{R}^{r\\times n}$\n* $Y=[Y^{<1>},\\dots,Y^{<n>}]\\in\\mathbb{R}^{r\\times n}$ is the attention values of all the words.\nThe attention for the whole sequence can be written as :\n$$Y=Attention(Q;K;V)=V\\mathrm(\\frac{1}{\\sqrt{p}}Q^{T}K)$$\n$Y=Attention(Q;K;V)$ is called the Scaled dot-product attention because the dot products are scaled down by $\\sqrt{p}$.\n","metadata":{}},{"cell_type":"markdown","source":"# MULTIHEAD ATTENTION WITH SELF-ATTENTION\nAfter positional encoding, data are fed to a multihead attention module with self-attention.  This module applies the attention mechanism for h times. The first attention determines how much every word attends other words. The second repeat of attention calculates how much every pair of words attends other pairs of words. Likewise, the third repeat of attention sees how much every pair of pairs of words attends other pairs of pairs of words; and so on.<br>\nthe data, which include positional encoding, are passed from linear layers for obtaining the queries, values, and keys. These linear layers model linear projection. We have h of these linear layers to generate h set of queries, value, and\nkeys such as:\n\n* $Q_{i}=[q_{i}^{<1>},\\dots,q_{i}^{<n>}]=[W_{Q,i}^{T}X^{<1>},\\dots,W_{Q,i}^{T}X^{<n>}]=W_{Q,i}^{T}X\\in\\mathbb{R}^{p\\times n}, \\forall i\\in\\{1,\\dots,h\\}$\n* $K_{i}=[k_{i}^{<1>},\\dots,k_{i}^{<n>}]=[W_{K,i}^{T}X^{<1>},\\dots,W_{K,i}^{T}X^{<n>}]=W_{K,i}^{T}X\\in\\mathbb{R}^{p\\times n}, \\forall i\\in\\{1,\\dots,h\\}$\n* $V_{i}=[v_{i}^{<1>},\\dots,v_{i}^{<n>}]=[W_{V,i}^{T}X^{<1>},\\dots,W_{V,i}^{T}X^{<n>}]=W_{V,i}^{T}X\\in\\mathbb{R}^{r\\times n}, \\forall i\\in\\{1,\\dots,h\\}$\n\nwhere $X=[X^{<1>},\\dots,X^{<n>}]$. We generate the h attention values\n\n* $Y_{1}=Attention(Q_{1};K_{1};V_{1})=V_{1}\\mathrm(\\frac{1}{\\sqrt{p}}Q_{1}^{T}K_{1}),\\dots,Y_{i}=Attention(Q_{i};K_{i};V_{i})=V_{i}\\mathrm(\\frac{1}{\\sqrt{p}}Q_{i}^{T}K_{i}),\\dots,Y_{n}=Attention(Q_{n};K_{n};V_{n})=V_{n}\\mathrm(\\frac{1}{\\sqrt{p}}Q_{n}^{T}K_{n})$\n\nThen, by a linear layer, which is a linear projection, the total attention value, $Y_{tot}$, is obtained:\n$$Y_{tot}=W_{o}\\mathrm{concat}(Y_{1},\\dots,Y_{n})$$\nCheck out the below illustration to understand the inner workings of an elementary self-attention block:","metadata":{}},{"cell_type":"markdown","source":"<center>\n<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/TRANSFORMERS/MultiHead.png\" title=\"Title Tag Goes Here\" height=\"500\" width=\"450\" border=\"1px\">\n</center>\n<center>\n<title> <h1>Multi-Head Attention</h1>  </title>          \n</center>","metadata":{}},{"cell_type":"markdown","source":"# Overall transformer architecture\nTransformer employs an encoder-decoder architecture. In transformers network the input sequence can be passed in parallel. We deal with encoder blocks and decoder blocks in the next section.<br>\nTransformers networks are a stack of encoder blocks and a stack of decoder blocks. Both the encoder stack and the decoder stack have the same number of units.<br>\nlet assume we handle translating from French to English.\nThe first encoder takes the Positional encodings of the french sentence words as an input. The first decoder takes the Positional encodings of the english sentence words as an input.<br>\nThe input of one encoder is the output of the previous one. The input of a decoder is the output of the previous decoder as well as the words already encoded.\n<center>\n<img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/TRANSFORMERS/The_transformer_encoder_decoder_stack.png\" title=\"Title Tag Goes Here\" height=\"500\" width=\"450\" border=\"1px\">\n</center>\n<center>\n<title> <h1>transformer architecture </h1>  </title>          \n</center>","metadata":{}},{"cell_type":"markdown","source":"# Encoder Transformer block and Decodeur Transformer block\n<p style=\"float: right; clear: right\"><img src=\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/IMAGE/TRANSFORMERS/Screenshot-from-2019-06-17-20-01-32.webp\" height=\"500\" width=\"450\" border=\"1px\"></p>\nWe have seen in the previous section that encoder and decoder blocks are the elementary units of a transformer. Now, we explain how encoder and decoder work\n\nThe Encoder block applies in sequence:\n* Multi-Head attention layer, \n* Normalization layer \n* a feed forward layer (a single MLP applied independently to each attention vectors). These feed forward nets are used in practice to transform the attention vectors into a form that is digestible by the next encoder block\n* Normalization layer\n\nThe Decoder block applies in sequence:\n* Multi-Head attention layer\n* Add and Norm layer\n* Multi-Head attention layer conected to the encoder\n* Add and Norm layer\n* a feed forward layer (a single MLP applied independently to each attention vectors). These feed forward nets are used in practice to transform the attention vectors into a form that is digestible by the next decoder block\n* Add and Norm layer\n\nThe normalization layer could be:\n* batch normalization making the same normalization for each training examples inside the batch \n* A layer normalization making the normalization across each feature instead of each sample\n\nAbove the decoder block we have:\n* a linear layer which is a feed forward layer used to expand the input vector dimensions into the size of the output vector space. When we translate from english to french, this feed forward layer expand the dimensions into the number of words in the french vocabulary\n* The sofmax layer takes the new vector outputted by the previous linear layer in order to transform it into a probability distribution","metadata":{}},{"cell_type":"markdown","source":"# Modern transformers\nref : http://peterbloem.nl/blog/transformers<br>\nIn the context of this cours, we mainly use pre-trained transformers model from tensorflow_hub and from hugging-face. We present just few of them. ","metadata":{}},{"cell_type":"markdown","source":"## BERT\nBERT was one of the first models to show that transformers could reach human-level performance on a variety of language based tasks: question answering, sentiment classification or classifying whether two sentences naturally follow one another.<br>\nBERT consists of a simple stack of transformer blocks, of the type we’ve described above. This stack is pre-trained on a large general-domain corpus consisting of 800M words from English books (modern work, from unpublished authors), and 2.5B words of text from English Wikipedia articles (without markup).<br>\nPretraining is done through two tasks:\n* Masking : A certain number of words in the input sequence are: masked out, replaced with a random word or kept as is. The model is then asked to predict, for these words, what the original words were. Note that the model doesn't need to predict the entire denoised sentence, just the modified words. Since the model doesn't know which words it will be asked about, it learns a representation for every word in the sequence.\n* Next sequence classification : Two sequences of about 256 words are sampled that either (a) follow each other directly in the corpus, or (b) are both taken from random places. The model must then predict whether a or b is the case.\n\nThe largest BERT model uses 24 transformer blocks, an embedding dimension of 1024 and 16 attention heads, resulting in 340M parameters.","metadata":{}},{"cell_type":"markdown","source":"## GPT-2\nGPT-2 is the first transformer model that actually made it into the mainstream news, after the controversial decision by OpenAI not to release the full model.<br>\nThe reason was that GPT-2 could generate sufficiently believable text that large-scale fake news campaigns of the kind seen in the 2016 US presidential election would become effectively a one-person job.<br>\nThe first trick that the authors of GPT-2 employed was to create a new high-quality dataset. While BERT used high-quality data, their sources (lovingly crafted books and well-edited wikipedia articles) had a certain lack of diversity in the writing style. To collect more diverse data without sacrificing quality the authors used links posted on the social media site Reddit to gather a large collection of writing with a certain minimum level of social support (expressed on Reddit as karma).<br>\nGPT2 is fundamentally a language generation model, so it uses masked self-attention like we did in our model above. It uses byte-pair encoding to tokenize the language, which , like the WordPiece encoding breaks words up into tokens that are slightly larger than single characters but less than entire words.<br>\nGPT2 is built very much like our text generation model above, with only small differences in layer order and added tricks to train at greater depths. The largest model uses 48 transformer blocks, a sequence length of 1024 and an embedding dimension of 1600, resulting in 1.5B parameters.","metadata":{}}]}