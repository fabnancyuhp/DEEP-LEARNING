{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, we work a news database. We will solve 2 problems:\n* We first design a model to predict if a news is a fake one or not.\n* In a second application, we buid a model that able to detect the topic of a news text \n\nWe provide some useful functions to clean text databases.","metadata":{}},{"cell_type":"markdown","source":"# Data cleaning functions\nhere, we provide some useful functions to clean text data. We call these functions during preprocessing text data.","metadata":{}},{"cell_type":"code","source":"import re\n\n#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n\n#Removes URL data\ndef _remove_urls(x):\n    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n\n\n#Removes dups char\ndef _remove_dups_char(x):\n    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n    return x\n\n\n#Removes emails\ndef _remove_emails(x):\n    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\n\n\ndef _remove_rt(x):\n    return re.sub(r'\\brt\\b', '', x).strip()\n\ndef _remove_special_chars(x):\n    x = re.sub(r'[^\\w ]+', \"\", x)\n    x = ' '.join(x.split())\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:31:05.376489Z","iopub.execute_input":"2021-11-21T08:31:05.377786Z","iopub.status.idle":"2021-11-21T08:31:05.387148Z","shell.execute_reply.started":"2021-11-21T08:31:05.377727Z","shell.execute_reply":"2021-11-21T08:31:05.386162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example 1 : Fake news detection using LSTM \nIn this section, we want to make a model to predict fake news. Our dataset corresponds to a corpus of fake and real news. We first import some useful libraries.<br>\nWe will be using an LSTM to evaluate whether or not a text correspond to a fake news. We start by importing the dataset:","metadata":{"execution":{"iopub.status.busy":"2021-11-18T12:54:37.268943Z","iopub.execute_input":"2021-11-18T12:54:37.269626Z","iopub.status.idle":"2021-11-18T12:54:37.291889Z","shell.execute_reply.started":"2021-11-18T12:54:37.269518Z","shell.execute_reply":"2021-11-18T12:54:37.290774Z"}}},{"cell_type":"code","source":"import pandas as pd\nFake = pd.read_parquet(\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/DATA/Fake.parquet.brotli\")\nReal = pd.read_parquet(\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/DATA/Real.parquet.brotli\")\nFake = Fake.loc[~(Fake['text'].str.strip().str.len()==0)]\nReal = Real.loc[~(Real['text'].str.strip().str.len()==0)]\nReal['text'] = Real['title']+ \" \" +Real['text']\nFake['text'] = Fake['title']+ \" \" +Fake['text']\nReal['class'] = 1\nFake['class'] = 0\nReal = Real[['class','text']]\nFake = Fake[['class','text']]\nreal_fake_news = pd.concat([Real,Fake]).sample(frac=1.0)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:31:55.520511Z","iopub.execute_input":"2021-11-19T08:31:55.521049Z","iopub.status.idle":"2021-11-19T08:31:57.624362Z","shell.execute_reply.started":"2021-11-19T08:31:55.521014Z","shell.execute_reply":"2021-11-19T08:31:57.623626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First step, We draw 2 wordclouds to compare fake news and real news\nIt's hard to distinguish the wordcloud from the fake news and from the real news.","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:33:05.601576Z","iopub.execute_input":"2021-11-18T13:33:05.601881Z","iopub.status.idle":"2021-11-18T13:33:05.610364Z","shell.execute_reply.started":"2021-11-18T13:33:05.601831Z","shell.execute_reply":"2021-11-18T13:33:05.609717Z"}}},{"cell_type":"code","source":"import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\nfrom wordcloud import WordCloud\n\ntext_fake = ' '.join(Fake['text'].tolist())\nwordcloud_fake = WordCloud(width=1920,height=1080).generate(text_fake)\ntext_real = ' '.join(Real['text'].tolist())\nwordcloud_real = WordCloud(width=1920,height=1080).generate(text_real)\nplt.figure(figsize=(20,20))\nplt.subplot(121)\nplt.imshow(wordcloud_fake)\nplt.tight_layout(pad=0)\nplt.title(\"fake_news\",fontsize =30)\n\nplt.subplot(122)\nplt.imshow(wordcloud_real)\nplt.tight_layout(pad=0)\nplt.title(\"real_news\",fontsize =30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T13:43:10.189009Z","iopub.execute_input":"2021-11-18T13:43:10.189292Z","iopub.status.idle":"2021-11-18T13:44:37.899109Z","shell.execute_reply.started":"2021-11-18T13:43:10.189251Z","shell.execute_reply":"2021-11-18T13:44:37.898399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2 :preprocessing the text data\n* We convert the text in lowercase\n* We remove html tags, url tags, emails, ponctuations and emails.","metadata":{}},{"cell_type":"code","source":"real_fake_news['text'] = real_fake_news['text'].apply(lambda x:str(x).lower()).replace('_', ' ') #.replace('\\\\', '').replace('_', ' '))\nreal_fake_news['text'] = real_fake_news['text'].apply(lambda x:remove_html(str(x)))\nreal_fake_news['text'] = real_fake_news['text'].apply(lambda x:_remove_urls(str(x)))\nreal_fake_news['text'] = real_fake_news['text'].apply(lambda x:remove_punctuations(str(x)))\nreal_fake_news['text'] = real_fake_news['text'].apply(lambda x:_remove_emails(str(x)))\nreal_fake_news['text'] = real_fake_news['text'].apply(lambda x:_remove_rt(str(x)))\nreal_fake_news['text'] = real_fake_news['text'].apply(lambda x:_remove_special_chars(str(x)))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:32:02.482879Z","iopub.execute_input":"2021-11-19T08:32:02.483445Z","iopub.status.idle":"2021-11-19T08:32:17.90594Z","shell.execute_reply.started":"2021-11-19T08:32:02.483384Z","shell.execute_reply":"2021-11-19T08:32:17.905176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: text Tokenization\nTokenization is the process of breaking up a given text into units called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks may be discarded. The tokens usually become the input for the processes like parsing and text mining.<br>\nIn the cell below, \n* X is a list of tokens list.\n* XBIS is a list of sequences. A sequence is a list of number","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\nX = [d.split() for d in list(real_fake_news['text'])]\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)\nXBIS = tokenizer.texts_to_sequences(X)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:32:40.540344Z","iopub.execute_input":"2021-11-19T08:32:40.540985Z","iopub.status.idle":"2021-11-19T08:33:04.057342Z","shell.execute_reply.started":"2021-11-19T08:32:40.540946Z","shell.execute_reply":"2021-11-19T08:33:04.056463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X[1])\nprint(XBIS[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T15:04:11.639104Z","iopub.execute_input":"2021-11-19T15:04:11.639427Z","iopub.status.idle":"2021-11-19T15:04:11.64411Z","shell.execute_reply.started":"2021-11-19T15:04:11.639381Z","shell.execute_reply":"2021-11-19T15:04:11.643239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We display a part of the word index we created with tensorflow.keras.preprocessing.text.Tokenizer. This index is a python dictionary.","metadata":{}},{"cell_type":"code","source":"index_word  = tokenizer.word_index\nfor x in list(index_word)[0:3]:\n    print (\"{}, {} \".format(x,  index_word[x]))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:56:08.209906Z","iopub.execute_input":"2021-11-19T08:56:08.210589Z","iopub.status.idle":"2021-11-19T08:56:08.233612Z","shell.execute_reply.started":"2021-11-19T08:56:08.21054Z","shell.execute_reply":"2021-11-19T08:56:08.232616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We study the length of text data. We draw a histogram of the size (count of words) of the news.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist([len(x) for x in X],bins=700)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T09:03:55.905697Z","iopub.execute_input":"2021-11-19T09:03:55.905982Z","iopub.status.idle":"2021-11-19T09:03:57.369884Z","shell.execute_reply.started":"2021-11-19T09:03:55.905951Z","shell.execute_reply":"2021-11-19T09:03:57.36909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We truncate all texts which have more than 1000 words using pad_sequences. After we run the cell below, all texts are modelized in  1000 components vectors.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nXTIER = pad_sequences(XBIS,maxlen=1000)\nXTIER","metadata":{"execution":{"iopub.status.busy":"2021-11-19T09:30:26.36031Z","iopub.execute_input":"2021-11-19T09:30:26.360586Z","iopub.status.idle":"2021-11-19T09:30:28.460278Z","shell.execute_reply.started":"2021-11-19T09:30:26.360559Z","shell.execute_reply":"2021-11-19T09:30:28.459469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The vocabulary size is equal to len(tokenizer.word_index)+1 to take into account the unknown words.","metadata":{"execution":{"iopub.status.busy":"2021-11-19T09:29:02.643181Z","iopub.execute_input":"2021-11-19T09:29:02.643887Z","iopub.status.idle":"2021-11-19T09:29:02.650967Z","shell.execute_reply.started":"2021-11-19T09:29:02.64385Z","shell.execute_reply":"2021-11-19T09:29:02.650163Z"}}},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index)+1\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2021-11-19T09:38:18.965994Z","iopub.execute_input":"2021-11-19T09:38:18.966581Z","iopub.status.idle":"2021-11-19T09:38:18.971755Z","shell.execute_reply.started":"2021-11-19T09:38:18.966544Z","shell.execute_reply":"2021-11-19T09:38:18.970973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Splitting the dataset into a training set and a test set\nWe use the train_test_split method to make a training set and a test set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(XTIER,real_fake_news['class'].values,test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T10:03:02.686353Z","iopub.execute_input":"2021-11-19T10:03:02.687204Z","iopub.status.idle":"2021-11-19T10:03:03.166175Z","shell.execute_reply.started":"2021-11-19T10:03:02.687163Z","shell.execute_reply":"2021-11-19T10:03:03.165352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Building the model with tensorflow.keras\nWe want to predict the fake newsWe build a model with the following layers:\n* an Embedding layer with input_dim=len(tokenizer.word_index)+1, input_length=1000. Since we want the dimension of the dense embedding is 100, we have to set output_dim=100.\n* a LSTM layer with 128 units \n* a Dense layer. Since we have only 2 classes, in the last dense layer we choose the sigmoid activation function.\n\nWe compile this model with an adam optimizer, a binary_crossentropy loss function and the accuracy metric. We fit the model ","metadata":{"execution":{"iopub.status.busy":"2021-11-19T09:38:08.703174Z","iopub.execute_input":"2021-11-19T09:38:08.703873Z","iopub.status.idle":"2021-11-19T09:38:08.71037Z","shell.execute_reply.started":"2021-11-19T09:38:08.703823Z","shell.execute_reply":"2021-11-19T09:38:08.709535Z"}}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Embedding, LSTM\nfrom tensorflow.keras.models import Sequential\nvocab_size = len(tokenizer.word_index)+1\n\n#model_fake_single = Sequential([Embedding(input_dim=vocab_size,output_dim=100),\\\n                                #LSTM(units=128),Dense(1,activation='sigmoid')])\n\nmodel_fake_single = Sequential()\nmodel_fake_single.add(Embedding(input_dim=vocab_size,output_dim=100))\nmodel_fake_single.add(LSTM(units=128))\nmodel_fake_single.add(Dense(1,activation='sigmoid'))\n\nmodel_fake_single.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n\nmodel_fake_single.fit(X_train,y_train,epochs=6)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T11:07:55.322191Z","iopub.execute_input":"2021-11-19T11:07:55.322834Z","iopub.status.idle":"2021-11-19T13:05:38.503191Z","shell.execute_reply.started":"2021-11-19T11:07:55.322795Z","shell.execute_reply":"2021-11-19T13:05:38.501586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fake_single.evaluate(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:21:49.250405Z","iopub.execute_input":"2021-11-19T13:21:49.251552Z","iopub.status.idle":"2021-11-19T13:24:03.972102Z","shell.execute_reply.started":"2021-11-19T13:21:49.251507Z","shell.execute_reply":"2021-11-19T13:24:03.971445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We test the model on a new text\nThe class of real news is 1. The Class of fake news is 0. We aplly model_fake_single.predict to a new vectorized text to get the probability those text is a real news. If the result is lower than 0.5, we classify this text as a fake news else as a real news.\n* handle fake news provides the vectorization of a text like we did during the preprocessing and the vectorization of the text database\n* Then, we apply the predict method to this vectorization to get the probability this text correspond to a real news","metadata":{}},{"cell_type":"code","source":"def handle_fake_news(text):\n    text1 = str(text).lower().replace('_',' ')\n    text1 = remove_html(str(text1))\n    text1 = _remove_urls(text1)\n    text1 = remove_punctuations(text1)\n    text1 = _remove_emails(text1)\n    text1 = _remove_rt(text1)\n    text1 = _remove_special_chars(text1)\n    text1 = tokenizer.texts_to_sequences([text1.split()])\n    text1 = pad_sequences(text1,maxlen=1000)\n    return(text1)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:43:40.576144Z","iopub.execute_input":"2021-11-19T13:43:40.576463Z","iopub.status.idle":"2021-11-19T13:43:40.582282Z","shell.execute_reply.started":"2021-11-19T13:43:40.57643Z","shell.execute_reply":"2021-11-19T13:43:40.58149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_text = \" Tone Deaf Trump: Congrats Rep. Scalise On Losing Weight After You Almost Died Donald \\\n             Trump just signed the GOP tax scam into law. Of course, that meant that he invited all \\\n             of his craven, cruel GOP sycophants down from their perches on Capitol Hill to celebrate\\\n             in the Rose Garden at the White House.\"\nproba_real = model_fake_single.predict(handle_fake_news(new_text))\n\nif proba_real<0.5:\n    print(\"This text probably correspond to a fake news\")\nelse :\n    print(\"This text probably correspond to a real news\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:36:31.27166Z","iopub.execute_input":"2021-11-19T14:36:31.271969Z","iopub.status.idle":"2021-11-19T14:36:31.49946Z","shell.execute_reply.started":"2021-11-19T14:36:31.271936Z","shell.execute_reply":"2021-11-19T14:36:31.498399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example : Topic prediction using an LSTM\n* In this example, we want to build a neural network to predict the subjects of texts\n* We use the news dataset from the previous example. We keep the following subjects: 'politics News', 'world news', 'News', 'politics'. We merge 'politics News' and 'politics' into 'politics'.\n* The text data is the input of our neural network\n\nRun the cell below to get the database.","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np\nFake = pd.read_parquet(\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/DATA/Fake.parquet.brotli\")\nReal = pd.read_parquet(\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/DATA/Real.parquet.brotli\")\n\nReal['text'] = Real['title']+ \" \" +Real['text']\nFake['text'] = Fake['title']+ \" \" +Fake['text']\n\nReal = Real[['subject','text']]\nFake = Fake[['subject','text']]\n\ntopic_news = pd.concat([Real,Fake])\n\ntopic_news = topic_news.loc[topic_news['subject'].isin(['politicsNews','worldnews','News','politics'])]\ntopic_news['subject'] = np.where(topic_news['subject'].isin(['politicsNews','politics']),\\\n                                 'politics',topic_news['subject'])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:10:09.102708Z","iopub.execute_input":"2021-11-21T09:10:09.103581Z","iopub.status.idle":"2021-11-21T09:10:14.282943Z","shell.execute_reply.started":"2021-11-21T09:10:09.103535Z","shell.execute_reply":"2021-11-21T09:10:14.281938Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"We just have three topics in our database: 'politics', 'worldnews', 'News'.","metadata":{}},{"cell_type":"code","source":"topic_news['subject'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:55:21.814253Z","iopub.execute_input":"2021-11-19T14:55:21.814928Z","iopub.status.idle":"2021-11-19T14:55:21.824126Z","shell.execute_reply.started":"2021-11-19T14:55:21.814883Z","shell.execute_reply":"2021-11-19T14:55:21.823211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Topics distribution\nWe draw the topics distribution with a histogram. The topic distribution is enough balanced to build a LSTM network that predict the topic of a text.","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\nimport nltk, re\nfrom wordcloud import WordCloud\n \nplt.figure(figsize=(10,6))\nsns.countplot(x='subject',data=topic_news)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T15:00:58.364404Z","iopub.execute_input":"2021-11-19T15:00:58.364727Z","iopub.status.idle":"2021-11-19T15:00:58.58942Z","shell.execute_reply.started":"2021-11-19T15:00:58.364695Z","shell.execute_reply":"2021-11-19T15:00:58.588553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encodage of the text topics\nSince we will construct a tensorflow.keras model, we have to encode the text topics. The subject column corresponds to the target values. We use the LabelEncoder from sklearn to encode the topics.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(topic_news['subject'].unique())\n\ntopic_news['subject'] = le.transform(topic_news['subject'])\nle.inverse_transform([0,1,2])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:30:38.76748Z","iopub.execute_input":"2021-11-21T08:30:38.767779Z","iopub.status.idle":"2021-11-21T08:30:39.659351Z","shell.execute_reply.started":"2021-11-21T08:30:38.767746Z","shell.execute_reply":"2021-11-21T08:30:39.658502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"topic_news['subject'] is now an encoded column.","metadata":{}},{"cell_type":"markdown","source":"## Text data cleaning\nIn this section we preprocess the texts to remove the URLs, the HTML tags, the emails... We apply the cleaning functions we defined at the begining of this notebook.","metadata":{}},{"cell_type":"code","source":"topic_news['text'] = topic_news['text'].apply(lambda x:str(x).lower().replace('_', ' '))\ntopic_news['text'] = topic_news['text'].apply(lambda x:remove_html(str(x)))\ntopic_news['text'] = topic_news['text'].apply(lambda x:_remove_urls(str(x)))\ntopic_news['text'] = topic_news['text'].apply(lambda x:remove_punctuations(str(x)))\ntopic_news['text'] = topic_news['text'].apply(lambda x:_remove_emails(str(x)))\ntopic_news['text'] = topic_news['text'].apply(lambda x:_remove_rt(str(x)))\ntopic_news['text'] = topic_news['text'].apply(lambda x:_remove_special_chars(str(x)))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:31:30.966073Z","iopub.execute_input":"2021-11-21T08:31:30.966983Z","iopub.status.idle":"2021-11-21T08:31:43.586558Z","shell.execute_reply.started":"2021-11-21T08:31:30.966936Z","shell.execute_reply":"2021-11-21T08:31:43.585519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Tokenization, indexation\n* Tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).\n* Followed the tokenization we create an index of words. In our context, an index  is a python dictionary built from the set of the unique words of our corpus. The corpus is the set of text data stored in topic_news['text'].\n* Concretely in this step, we convert all text into a list of integers. X is a list of list of words. XBIS is a list of list of integers.\n* We use pad_sequences to pad sequences to the same length. All list of integers in XTIER have the same length.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer()\n\nX = [text.split() for text in list(topic_news['text'])]\ntokenizer.fit_on_texts(X)\n\nXBIS = tokenizer.texts_to_sequences(X)\nXTIER = pad_sequences(XBIS,maxlen=1000)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:32:33.638865Z","iopub.execute_input":"2021-11-21T08:32:33.639166Z","iopub.status.idle":"2021-11-21T08:33:00.343888Z","shell.execute_reply.started":"2021-11-21T08:32:33.639136Z","shell.execute_reply":"2021-11-21T08:33:00.342953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The index of the corpus is given by tokenizer.word_index .The vocabulary size is given by len(tokenizer.word_index)+1. We add 1 to the length of the index to take into account unknown words. The keys of this dictionary are the set of unique words we get from the corpus.","metadata":{}},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index)+1\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:33:04.517206Z","iopub.execute_input":"2021-11-21T08:33:04.517487Z","iopub.status.idle":"2021-11-21T08:33:04.522516Z","shell.execute_reply.started":"2021-11-21T08:33:04.517457Z","shell.execute_reply":"2021-11-21T08:33:04.521439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Splitting the dataset into a training set and a test set\nWe use the train_test_split method to make a training set and a test set. We have a 3 classes problem, topic_news['subject'].values must be converted into a 3 dimentional vectors using tf.keras.utils.to_categorical.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\nY_CAT = to_categorical(topic_news['subject'].values,num_classes=3)\nY_CAT","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:33:40.307627Z","iopub.execute_input":"2021-11-21T08:33:40.307907Z","iopub.status.idle":"2021-11-21T08:33:40.32458Z","shell.execute_reply.started":"2021-11-21T08:33:40.307879Z","shell.execute_reply":"2021-11-21T08:33:40.32349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(XTIER,Y_CAT,test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:33:47.758303Z","iopub.execute_input":"2021-11-21T08:33:47.758595Z","iopub.status.idle":"2021-11-21T08:33:47.892617Z","shell.execute_reply.started":"2021-11-21T08:33:47.758566Z","shell.execute_reply":"2021-11-21T08:33:47.891714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RNN MODEL WITH A SINGLE LSTM LAYER\n* In the following network the first layer is an embedding layer that embedds each word from the vocabulary into a 100 sized dense vector. Some importants arguments of the tf.keras.layers.Embedding are: \n   * input_dim : Size of the vocabulary+1\n   * output_dim: Integer. Dimension of the dense embedding.\n   * input_length: Length of input sequences. The lenght of our input sequences is set with pad_sequences.\n* We want to predict if the topic of a text is about politic,worldnews or news. In other words, we are in the case of a 3 classes problem. Then, the output layer shoud be a dense layer with 3 units. The output layer shoud logically be a dense layer with 3 units with a softmax activation function. We choose categorical_crossentropy loss function. \n* LSTM(units=128) means the dimensionality of the output space is 128. 128 is not the number of elementary LSTM cells we have in our LSTM. We have 1000 elementary LSTM cells in our LSTM Neural network.\n* The input $X^{<1>}\\in \\mathbb{R}^{100}$ for the first elementary LSTM cell is a dense vector. The input for the last elementary LSTM cell $X^{<1000>}\\in \\mathbb{R}^{100}$ is a dense vector. The LSTM we build has 1000 elementary cells.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, LSTM, Embedding\nfrom tensorflow.keras.models import Sequential\n\ndim = 100\nvocab_size = len(tokenizer.word_index)+1\n\ntopic_single = Sequential()\ntopic_single.add(Embedding(input_dim=vocab_size,output_dim=100))\ntopic_single.add(LSTM(units=128))\ntopic_single.add(Dense(3,activation='softmax'))\n\ntopic_single.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:34:06.366259Z","iopub.execute_input":"2021-11-21T08:34:06.36683Z","iopub.status.idle":"2021-11-21T08:34:06.718281Z","shell.execute_reply.started":"2021-11-21T08:34:06.366756Z","shell.execute_reply":"2021-11-21T08:34:06.717532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We fit the topic_single model and evaluate it.","metadata":{}},{"cell_type":"code","source":"topic_single.fit(X_train,y_train,epochs=1)\ntopic_single.evaluate(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T08:34:46.482318Z","iopub.execute_input":"2021-11-21T08:34:46.4832Z","iopub.status.idle":"2021-11-21T08:55:30.041577Z","shell.execute_reply.started":"2021-11-21T08:34:46.483155Z","shell.execute_reply":"2021-11-21T08:55:30.040914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We test the model on unknown texts","metadata":{}},{"cell_type":"markdown","source":"First, we have to make a function able to preprocess and tokenizer unknown texts like we did in the text data cleaning section and the Tokenization section. We call the tokenizer we defined earlier in this example.","metadata":{}},{"cell_type":"code","source":"new_text = \" WATCH STANLEY CUP CHAMPS VISIT White Houseâ€¦Give Trump Coveted \\\nGift Video President Trump greeted the NHL s Stanley Cup winning \\\nPittsburgh Penguins at the White House today. The President and First Lady were \\\non hand to greet the team and coaches. The president was given a small \\\nreplica of the Stanley Cup Very cool!President Trump welcomed \\\nthe Pittsburgh Penguins to the White House on Tuesday to congratulate\\\nthem on their Stanley Cup win.\" \n\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndef handle_topic(text):\n    text1 = str(text).lower().replace('_', ' ')\n    text1 = remove_html(str(text1))\n    text1 = _remove_urls(str(text1))\n    text1 = remove_punctuations(str(text1))\n    text1 = _remove_emails(str(text1))\n    text1 = _remove_rt(str(text1))\n    text1 = _remove_special_chars(str(text1))\n    text1 = [text1.split()]\n    text1 = tokenizer.texts_to_sequences(text1)\n    text1 = pad_sequences(text1,maxlen=1000)\n    return(text1)\n\ntext_vect = handle_topic(new_text)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:31:29.537396Z","iopub.execute_input":"2021-11-21T09:31:29.538319Z","iopub.status.idle":"2021-11-21T09:31:29.548200Z","shell.execute_reply.started":"2021-11-21T09:31:29.538257Z","shell.execute_reply":"2021-11-21T09:31:29.547273Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Now, we can apply the model we've just fitted and we get a probability vector. Then, we apply the label encoder we defined earlier to know the predicted label of the new text.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n#we compute the probability vector of the subject\nproba_predict = topic_single.predict(text_vect)\n\n#The predicted subject\nprint(\"the predicted subject is: \"+str(le.inverse_transform([np.argmax(proba_predict)])[0]))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:46:17.274004Z","iopub.execute_input":"2021-11-21T09:46:17.274871Z","iopub.status.idle":"2021-11-21T09:46:17.510685Z","shell.execute_reply.started":"2021-11-21T09:46:17.274825Z","shell.execute_reply":"2021-11-21T09:46:17.509812Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## NEURAL NETWORK WITH MULTIPLE LSTM LAYER\nIn, this section we built neural network with 3 LSTM LAYERS. We don't use this model, We train it with only one epoch.  When we stack multiple LSTM layers we have to set return_sequences to True for all LSTM layers except the last one. You can put a Dropout layer between LSTM LAYERS.","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:11:02.953250Z","iopub.execute_input":"2021-11-21T09:11:02.954142Z","iopub.status.idle":"2021-11-21T09:11:02.960460Z","shell.execute_reply.started":"2021-11-21T09:11:02.954086Z","shell.execute_reply":"2021-11-21T09:11:02.959598Z"}}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Sequential\n\n\nvocab_size = len(tokenizer.word_index)+1\n\n\nmulti_lstm = Sequential()\nmulti_lstm.add(Embedding(input_dim=vocab_size,output_dim=100))\nmulti_lstm.add(LSTM(50,return_sequences=True))\nmulti_lstm.add(Dropout(0.2))\nmulti_lstm.add(LSTM(32,return_sequences=True))\nmulti_lstm.add(BatchNormalization())\nmulti_lstm.add(LSTM(16))\nmulti_lstm.add(Dense(3,activation='softmax'))\n\nmulti_lstm.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:58:55.317292Z","iopub.execute_input":"2021-11-21T09:58:55.317610Z","iopub.status.idle":"2021-11-21T09:58:56.135217Z","shell.execute_reply.started":"2021-11-21T09:58:55.317579Z","shell.execute_reply":"2021-11-21T09:58:56.134513Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"multi_lstm.fit(X_train,y_train,epochs=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:59:05.609266Z","iopub.execute_input":"2021-11-21T09:59:05.610116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NEURAL NETWORK WITH BIDIRECTIONAL LSTM LAYER","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Sequential\n\nDIM = 100 #the size of the emdedding vectors\nvocab_size = len(tokenizer.word_index)+1\n\n\nbidirect_lstm = Sequential()\nbidirect_lstm.add(Embedding(input_dim=vocab_size,output_dim=100,input_length=1000))\nbidirect_lstm.add(Bidirectional(LSTM(128)))\nbidirect_lstm.add(Dense(3,activation='softmax'))\n\nbidirect_lstm.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","metadata":{},"execution_count":null,"outputs":[]}]}