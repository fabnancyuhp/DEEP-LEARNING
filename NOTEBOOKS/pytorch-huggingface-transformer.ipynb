{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\nref : https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python<br>\nIn this notebook we use the **transformers library**. **Transformers library** is designed by Hugging-face. Hugging Face is an NLP-focused startup with a large open-source community. **Transformers** is a python-based library that exposes an API to use many well-known transformer architectures, such as BERT, RoBERTa, GPT-2 or DistilBERT.<br><br>\nIn this chapter we use the BERT hugging-face model to classify email as spam or not spam. BERT is a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.<br><br>\n**In this notebook, we show how to fine-tune a PyTorch HuggingFace transformer.**","metadata":{}},{"cell_type":"markdown","source":"# Functions for preprossesing text data.\n\nIn this section, we give some functions for cleaning text data:\n* One function to remove HTML tags\n* One function to remove URLs\n* One function to remove emails\n\nTheoretically, It is not necessarily good to clean the text when we practice text mining with HuggingFace transformers. But in this notebook, I decided to clean a bit the text data.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\ndef _remove_urls(x):\n    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n\ndef _remove_emails(x):\n    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T14:53:57.685221Z","iopub.execute_input":"2021-11-02T14:53:57.685623Z","iopub.status.idle":"2021-11-02T14:53:57.714661Z","shell.execute_reply.started":"2021-11-02T14:53:57.685524Z","shell.execute_reply":"2021-11-02T14:53:57.713888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning in PyTorch with the Trainer API\nIn this section, we fine-tune a **BERT** model from hugging-face with **PyTorch**. We use a BERT case model to detect spam emails. First, we import the dataset. The emails text are stored in the email column. The label column is equal to 0 for the valid emails and to 1 for the spams. Run the cell below to create the email dataset:\n\n## Dataset creation\n* We first import the data as a pandas dataset\n* Secondly, we convert the pandas dataset into a hugging-face dataset\n* Last, we split the hugging-face dataset into a training set and a test set","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nspam_ham = pd.read_csv(\"https://raw.githubusercontent.com/fabnancyuhp/DEEP-LEARNING/main/DATA/spam_ornot_spam.csv\")\nspam_ham['email'] = spam_ham['email'].apply(lambda x:remove_html(str(x)))\nspam_ham['email'] = spam_ham['email'].apply(lambda x:_remove_urls(str(x)))\nspam_ham['email'] = spam_ham['email'].apply(lambda x:_remove_emails(str(x)))\nspam_ham.head()\n\nspam_ham['email'] = spam_ham['email'].apply(lambda x:' '.join(x.split()[0:512]))","metadata":{"execution":{"iopub.status.busy":"2021-11-02T15:05:09.654548Z","iopub.execute_input":"2021-11-02T15:05:09.655381Z","iopub.status.idle":"2021-11-02T15:05:10.689895Z","shell.execute_reply.started":"2021-11-02T15:05:09.655342Z","shell.execute_reply":"2021-11-02T15:05:10.689044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to use the hugging-face pytorch Trainer API, you have to set up WANDB_DISABLED to true. Wandb means weights & Biases integration.  We set WANDB_DISABLED to “true” to disable wandb entirely. ","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2021-11-02T15:05:13.015386Z","iopub.execute_input":"2021-11-02T15:05:13.016197Z","iopub.status.idle":"2021-11-02T15:05:13.019393Z","shell.execute_reply.started":"2021-11-02T15:05:13.016147Z","shell.execute_reply":"2021-11-02T15:05:13.018825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom datasets import Dataset\nfrom datasets import DatasetDict\n\n# Secondly, we convert the pandas dataset into a hugging-face dataset\ndataset = Dataset.from_pandas(spam_ham)\n\n# Last, we split the hugging-face dataset into a training set and a test set\ndataset_train_test = dataset.train_test_split(test_size=0.15)\ndataset_train_test","metadata":{"execution":{"iopub.status.busy":"2021-11-02T15:06:43.594978Z","iopub.execute_input":"2021-11-02T15:06:43.595709Z","iopub.status.idle":"2021-11-02T15:06:43.617715Z","shell.execute_reply.started":"2021-11-02T15:06:43.595674Z","shell.execute_reply":"2021-11-02T15:06:43.616845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization and vectorization of the text\nRemind that our goal is to fine-tune a BERT cased model from the hugging-face hub. A hugging-face model has two components:\n* the tokenizer component is responsable for the text vectorization\n* the model component is a pre-trained transformer that takes the tokenizer output as an input\n\nThe tokenizer and model should always be paired. In the cell below we:\n* we import the tokenizer designed for the bert-cased model\n* We vectorize the texts from the emails  ","metadata":{}},{"cell_type":"code","source":"# We import the tokenizer designed for the bert-cased model\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# We vectorize the texts from the emails\ndef tokenize_function(examples):\n    return tokenizer(examples[\"email\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = dataset_train_test.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T15:07:35.729918Z","iopub.execute_input":"2021-11-02T15:07:35.730887Z","iopub.status.idle":"2021-11-02T15:07:39.901851Z","shell.execute_reply.started":"2021-11-02T15:07:35.730777Z","shell.execute_reply":"2021-11-02T15:07:39.900719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2021-11-02T15:07:45.191219Z","iopub.execute_input":"2021-11-02T15:07:45.191551Z","iopub.status.idle":"2021-11-02T15:07:45.1977Z","shell.execute_reply.started":"2021-11-02T15:07:45.191516Z","shell.execute_reply":"2021-11-02T15:07:45.196844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"attention_mask,  input_ids, token_type_ids are vectors features made by the bert-cased tokenizer from the email feature. The bert-cased model uses these features and the label feature during the training stage we will implement later. We get the training set and the test set from the hugging-face dataset tokenized_datasets.","metadata":{}},{"cell_type":"code","source":"train_set = tokenized_datasets['train']\ntest_set = tokenized_datasets['test']","metadata":{"execution":{"iopub.status.busy":"2021-11-02T15:14:20.625268Z","iopub.execute_input":"2021-11-02T15:14:20.625582Z","iopub.status.idle":"2021-11-02T15:14:20.630695Z","shell.execute_reply.started":"2021-11-02T15:14:20.625548Z","shell.execute_reply":"2021-11-02T15:14:20.629739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training stage with PyTorch Trainer\n* We have to load the transfromer model we want to use. \n* We have to set the TrainingArguments\n* We set the Trainer \n\n**We load the transfromer model we want to use**<br>\nThe model we load is paired with the tokenizer we loaded earlier in this exemple. The model in the cell below is a Pytorch model. In the case of a pretrained pytorch model we use AutoModelForSequenceClassification instead of TFAutoModelForSequenceClassification.  ","metadata":{}},{"cell_type":"code","source":"#We have to load the transfromer model we want to use. Model is a Pytorch model.\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T15:22:11.274775Z","iopub.execute_input":"2021-11-02T15:22:11.275524Z","iopub.status.idle":"2021-11-02T15:22:30.887907Z","shell.execute_reply.started":"2021-11-02T15:22:11.275485Z","shell.execute_reply":"2021-11-02T15:22:30.88715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We have to set the TrainingArguments\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\"test_trainer\")\n          #num_train_epochs=3,              # total number of training epochs\n          #per_device_train_batch_size=16,  # 16 batch size per device during training\n          #per_device_eval_batch_size=20,   #20\n          #output_dir='./results',\n          #weight_decay=0.01)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T15:22:49.614846Z","iopub.execute_input":"2021-11-02T15:22:49.615146Z","iopub.status.idle":"2021-11-02T15:22:49.637641Z","shell.execute_reply.started":"2021-11-02T15:22:49.615112Z","shell.execute_reply":"2021-11-02T15:22:49.636758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We set the Trainer and begin the training stage with Trainer.train().**<br>\nThe Trainer object takes some arguments such that:\n* model : a huggingface pretrained PyTorch model\n* args : a TrainingArguments defined earlier \n* train_dataset : a huggingface dataset made from a huggingface tokenizer step  \n\nThe target value of a huggingface dataset used by the Trainer should always be named label else the Train API doesn't work.","metadata":{}},{"cell_type":"code","source":"#We set the Trainer and begin the training stage with Trainer.train().\n#import torch\n#torch.cuda.empty_cache()\n\nfrom transformers import Trainer\n\ntrainer = Trainer(model=model,args=training_args,train_dataset=train_set)\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we save the tokenizer and the pytorch model model.","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"./\")\ntokenizer.save_pretrained(\"./\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply the PyTorch fine-tuned model on a new text\nNow, we try the model on the following email : ","metadata":{}},{"cell_type":"code","source":"email_test = \"\"\"martin a posted tassos papadopoulos the greek sculptor behind \n                the plan judged that the limestone of mount kerdylio NUMBER \n                miles east of salonika and not far from the mount athos monastic \n                community was ideal for the patriotic sculpture as well as alexander s \n                granite features NUMBER ft high and NUMBER ft wide a museum a restored \n                amphitheatre and car park for admiring crowds are planned so is \n                this mountain limestone or granite if it s limestone it ll weather \n                pretty fast yahoo groups sponsor NUMBER dvds free s p join now URL \n                to unsubscribe from this group send an email to forteana unsubscribe \n                URL your use of yahoo groups is subject to URL\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-11-02T15:30:24.360678Z","iopub.execute_input":"2021-11-02T15:30:24.360973Z","iopub.status.idle":"2021-11-02T15:30:24.36607Z","shell.execute_reply.started":"2021-11-02T15:30:24.360946Z","shell.execute_reply":"2021-11-02T15:30:24.365436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the Trainer step, model is a fine-tuned transformer model. Then, we can apply model on a new email. We have to\n* vectorize the new email\n* Apply the pytorch on the vectorized new text \n* use softmax function to get a PyTorch tensor probability vector\n* convert the previous PyTorch tensor into a numpy array","metadata":{}},{"cell_type":"code","source":"token_input = tokenizer(email_test, padding=\"max_length\", truncation=True,return_tensors=\"pt\").to(\"cuda\")\n#outputs = model(**token_input) #model is a PyTorch model\n#probs = outputs[0].softmax(1)  #  \nprobs = model(**token_input)[0].softmax(1).cpu().detach().numpy()\nprint(\"proba=\"+ str(probs))\ntarget_names = ['email valid','spam']\ntarget_names[probs.argmax()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction on the test set with the fine-tuned model\nA huggingFace PyTorch is GPU-memory consuming. When we want to predict on a whole dataset with it, we have to clean the memory with the gc package (garbage collection).<br><br>\nIn the above cell, we do a loop over the whole dataset.  At each step of this loop we call the tokenizer and the model. At each step of the loop, We also convert a PyTorch tensor object into a numpy array to release some GPU memory.<br><br>\n**CUDA** (or Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing unit (GPU) for general purpose processing.<br><br>\n\nY_test_proba in the above cell is a list of numpy probability vectors.","metadata":{}},{"cell_type":"code","source":"Y_true = test_set['label']\nY_test_email = test_set['email']\n\nimport gc\n\nY_test_proba = []\n\nfor email in test_set['email']:\n    token_output = tokenizer(email, padding=\"max_length\", truncation=True,return_tensors=\"pt\").to(\"cuda\")\n    Y_test_proba.append(model(**token_output)[0].softmax(1).cpu().detach().numpy())\n    del token_output\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we use the np.argmax function to compute the predicted classes of the test set:","metadata":{}},{"cell_type":"markdown","source":"# EXERCICE : PyTorch Huggingface TRANSFORMER OVER fetch_20newsgroups\nIn this exercice, we fine-tune a hugging face bert model over the fetch_20newsgroups dataset. We have to understand the fetch_20newsgroups dataset as a first step. You have to run the cell bellow to create the dataset. The cell execution could take a while. ","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\ncategories = ['alt.atheism','comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware',\\\n              'comp.sys.mac.hardware','comp.windows.x','misc.forsale','rec.autos','rec.motorcycles',\\\n              'comp.sys.mac.hardware','comp.windows.x','misc.forsale','rec.autos','rec.motorcycles',\\\n              'rec.sport.baseball','rec.sport.hockey','sci.crypt','sci.electronics','sci.med','sci.space',\\\n              'soc.religion.christian','talk.politics.guns','talk.politics.mideast','talk.politics.misc','talk.religion.misc']\n\ndataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),categories=categories)\n\nimport numpy as np\nimport pandas as pd\nlen(dataset['target_names'])\ntarget_values_to_target_labels = dict(zip(range(0,len(dataset['target_names'])),dataset['target_names']))\n\ntarget_values = dataset.target.tolist()\ntarget_labels = [target_values_to_target_labels[o] for o in target_values]\n\ndataset_fetch_20newsgroups = pd.DataFrame({'text':dataset.data,'target_labels':target_labels})\n\ntarget_class_to_encoder = dict(zip(dataset_fetch_20newsgroups['target_labels'].unique().tolist(),list(range(0,len(dataset_fetch_20newsgroups['target_labels'].unique())))))\n\ndataset_fetch_20newsgroups['class_encoded'] = dataset_fetch_20newsgroups['target_labels'].apply(lambda x:target_class_to_encoder[x])\ndataset_fetch_20newsgroups.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:30:35.657782Z","iopub.execute_input":"2021-11-03T14:30:35.658327Z","iopub.status.idle":"2021-11-03T14:30:37.675036Z","shell.execute_reply.started":"2021-11-03T14:30:35.658288Z","shell.execute_reply":"2021-11-03T14:30:37.674345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we show some texts labeled rec.autos:","metadata":{}},{"cell_type":"code","source":"dataset_fetch_20newsgroups.loc[dataset_fetch_20newsgroups['target_labels']=='rec.autos']['text'].reset_index().iloc[3,1]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:19:43.634295Z","iopub.execute_input":"2021-11-03T09:19:43.634813Z","iopub.status.idle":"2021-11-03T09:19:43.647057Z","shell.execute_reply.started":"2021-11-03T09:19:43.634774Z","shell.execute_reply":"2021-11-03T09:19:43.646462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_fetch_20newsgroups.loc[dataset_fetch_20newsgroups['target_labels']=='rec.autos']['text'].reset_index().iloc[10,1]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:20:19.061441Z","iopub.execute_input":"2021-11-03T09:20:19.06199Z","iopub.status.idle":"2021-11-03T09:20:19.074168Z","shell.execute_reply.started":"2021-11-03T09:20:19.061949Z","shell.execute_reply":"2021-11-03T09:20:19.073459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1) Remove the URL, the email addresses and the html tags from the column 'text' of the dataset_fetch_20newsgroups dataset. You can use the functions defined  below.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\ndef _remove_urls(x):\n    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n\ndef _remove_emails(x):\n    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\n\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ndef remove_skip_line(x):\n    return re.sub(r'\\r?\\n|\\\\|\\r/g',\" \", x)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:30:46.237376Z","iopub.execute_input":"2021-11-03T14:30:46.237643Z","iopub.status.idle":"2021-11-03T14:30:46.243759Z","shell.execute_reply.started":"2021-11-03T14:30:46.237612Z","shell.execute_reply":"2021-11-03T14:30:46.243036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_fetch_20newsgroups['text'] = dataset_fetch_20newsgroups['text'].apply(lambda x:remove_html(str(x)))\ndataset_fetch_20newsgroups['text'] = dataset_fetch_20newsgroups['text'].apply(lambda x:_remove_urls(str(x)))\ndataset_fetch_20newsgroups['text'] = dataset_fetch_20newsgroups['text'].apply(lambda x:_remove_emails(str(x)))\ndataset_fetch_20newsgroups['text'] = dataset_fetch_20newsgroups['text'].apply(lambda x:remove_skip_line(str(x)))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:30:51.839239Z","iopub.execute_input":"2021-11-03T14:30:51.839811Z","iopub.status.idle":"2021-11-03T14:30:54.051024Z","shell.execute_reply.started":"2021-11-03T14:30:51.839754Z","shell.execute_reply":"2021-11-03T14:30:54.050239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2) In this exercise, we show how to fine-tune a bert-model with the Trainer API. Behind the scene, the Trainer API works with PyTorch.<br>\nThe Trainer API takes a PyTorch hugging-face model as an input and some arguments such that \n* the number of epochs, \n* the weight decay, \n* the fact to load the best model during the training stage\n* ...\n\nIn order to use the hugging-face pytorch Trainer API, you have to set up WANDB_DISABLED to true. Wandb means weights & Biases integration.  We set WANDB_DISABLED to “true” to disable wandb entirely. Run the cell below.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:30:59.226429Z","iopub.execute_input":"2021-11-03T14:30:59.226999Z","iopub.status.idle":"2021-11-03T14:30:59.230838Z","shell.execute_reply.started":"2021-11-03T14:30:59.22696Z","shell.execute_reply":"2021-11-03T14:30:59.229977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Trainer API takes also a **hugging-face dataset** as an input. As As a consequence, we have to convert the pandas dataset dataset_fetch_20newsgroups into a hugging-face dataset.<br>\nThe target value of a hugging-face dataset should always be named label else the Train API doesn't work. \n* rename the 'class_encoded' column of dataset_fetch_20newsgroups into label. \n* After that, you have to convert the dataset_fetch_20newsgroups into a huggin-face dataset using Dataset.from_pandas method. Call this hugging-face dataset dataset_fetch_20_for_Trainer_API.","metadata":{}},{"cell_type":"code","source":"#your code here\n#we rename ''class_encoded' 'label'  'class_encoded'\ndataset_fetch_20newsgroups = dataset_fetch_20newsgroups.rename(columns={'class_encoded': 'label'})\n\n\nfrom datasets import Dataset\n#convert dataset_fetch_20newsgroups into a hugging-face dataset\ndataset_fetch_20_for_Trainer_API = Dataset.from_pandas(dataset_fetch_20newsgroups)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:31:06.322282Z","iopub.execute_input":"2021-11-03T14:31:06.322545Z","iopub.status.idle":"2021-11-03T14:31:11.841392Z","shell.execute_reply.started":"2021-11-03T14:31:06.322515Z","shell.execute_reply":"2021-11-03T14:31:11.840708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3) Split dataset_fetch_20_for_Trainer_API into a train set and a test set. Set the parameter test_size=0.15. In this question, you have to call the train_test_split method for a hugging-face dataset. In this question, we don't use the train_test_split from sklearn.","metadata":{}},{"cell_type":"code","source":"#we create the training and the test dataset. the test_size is 15% of the whole dataset\ntrain_test_dataset = dataset_fetch_20_for_Trainer_API.train_test_split(test_size=0.15)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:31:18.575174Z","iopub.execute_input":"2021-11-03T14:31:18.575461Z","iopub.status.idle":"2021-11-03T14:31:18.593933Z","shell.execute_reply.started":"2021-11-03T14:31:18.575432Z","shell.execute_reply":"2021-11-03T14:31:18.593203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4) the goal of this exercise is to fin-tuned a Bert cased model with Train API. To preprocess our data, we will need a tokenizer. A tokenizer splits the data in tokens (these can be characters, words, part of words). For this task, we are using the tokenizer from the pre-trained model we selected (bert-base-cased).<br>\n* Import the tokenizer designed for the bert-base-case model from hugging-face hub. Call this tokenizer tokenizer.\n* Use the tokenizer you've just imported to preprocess dataset_fetch_20_for_Trainer_API. Store the preprocessed dataset into train_test_dataset_tokenize.\n* Extract the training set from train_test_dataset_tokenize into train_dataset.\n* Extract the test set from train_test_dataset_tokenize into test_dataset.","metadata":{}},{"cell_type":"code","source":"#we call the tokenizer from the huggingface hub. After that \nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ntrain_test_dataset_tokenize = train_test_dataset.map(tokenize_function, batched=True)\n\ntrain_dataset = train_test_dataset_tokenize['train']\ntest_dataset = train_test_dataset_tokenize['test']","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:31:27.08335Z","iopub.execute_input":"2021-11-03T14:31:27.083629Z","iopub.status.idle":"2021-11-03T14:31:42.555264Z","shell.execute_reply.started":"2021-11-03T14:31:27.083597Z","shell.execute_reply":"2021-11-03T14:31:42.554544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most importants features made during the tokenize step  are 'attention_mask', 'input_ids', 'label', 'token_type_ids'. Now, we don't need the text feature anymore. As a consequence, you could remove the text columns from train_dataset and  test_datasetlater.","metadata":{}},{"cell_type":"markdown","source":"5) Call the transformer model paired with the tokenizer. We have to set num_labels=20 since we have 20 classes in the label. This transformer model should be suitable with PyTorch, then we have to use AutoModelForSequenceClassification in this case. Call this transformer model.","metadata":{}},{"cell_type":"code","source":"#we call the transformer model paired with the tokenizer. We have to set num_labels to \nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=20)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:31:42.556886Z","iopub.execute_input":"2021-11-03T14:31:42.557295Z","iopub.status.idle":"2021-11-03T14:32:12.885616Z","shell.execute_reply.started":"2021-11-03T14:31:42.557256Z","shell.execute_reply":"2021-11-03T14:32:12.884424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6) The Trainer API uses some training arguments we call with a TrainingArguments object. You can use the default configuration with TrainingArguments(\"test_trainer\"). Else, you can set the following arguments:\n* num_train_epochs\n* weight_decay=0.01\n* load_best_model_at_end","metadata":{}},{"cell_type":"code","source":"#we set up the TrainingArguments\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\"test_trainer\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:44.240966Z","iopub.execute_input":"2021-11-03T14:32:44.241385Z","iopub.status.idle":"2021-11-03T14:32:44.29769Z","shell.execute_reply.started":"2021-11-03T14:32:44.241345Z","shell.execute_reply":"2021-11-03T14:32:44.296736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7) Call the Trainer API with your model, the arguments you defined in question 6 and train_dataset from question 4. Train your model and test it with test_dataset from question 4.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T14:32:47.714135Z","iopub.execute_input":"2021-11-03T14:32:47.716948Z","iopub.status.idle":"2021-11-03T15:06:25.546874Z","shell.execute_reply.started":"2021-11-03T14:32:47.716902Z","shell.execute_reply":"2021-11-03T15:06:25.546145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8) In this question we test our fine-tune model on a new text. To achieve this purpose, we have to tokenize this new text with the tokenizer we defined in the question 4. After that we have to apply our model to this preprocessed text to get a probability vector modelized stored in a torch.Tensor object.","metadata":{}},{"cell_type":"code","source":"text = \"\"\"\nThe first thing is first. \nIf you purchase a Macbook, you should not encounter performance issues that will prevent you from learning to code efficiently.\nHowever, in the off chance that you have to deal with a slow computer, you will need to make some adjustments. \nHaving too many background apps running in the background is one of the most common causes. \nThe same can be said about a lack of drive storage. \nFor that, it helps if you uninstall xcode and other unnecessary applications, as well as temporary system junk like caches and old backups.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:11:29.117076Z","iopub.execute_input":"2021-11-03T15:11:29.117339Z","iopub.status.idle":"2021-11-03T15:11:29.12231Z","shell.execute_reply.started":"2021-11-03T15:11:29.117309Z","shell.execute_reply":"2021-11-03T15:11:29.121516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenize this new text \ntoken_input = tokenizer(text, padding=\"max_length\", truncation=True,return_tensors=\"pt\").to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:12:19.513266Z","iopub.execute_input":"2021-11-03T15:12:19.513513Z","iopub.status.idle":"2021-11-03T15:12:19.521268Z","shell.execute_reply.started":"2021-11-03T15:12:19.513485Z","shell.execute_reply":"2021-11-03T15:12:19.52051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above cell we apply the model on the tokenized text and we get a probability vector modelized with a torch.Tensor object ","metadata":{}},{"cell_type":"code","source":"probs = model(**token_input)[0].softmax(1)\nprobs,type(probs)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:14:20.813361Z","iopub.execute_input":"2021-11-03T15:14:20.813637Z","iopub.status.idle":"2021-11-03T15:14:20.848882Z","shell.execute_reply.started":"2021-11-03T15:14:20.813606Z","shell.execute_reply":"2021-11-03T15:14:20.848025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we convert the torch.Tensor object into a numpy object. Since the torch.Tensor probe is attached to a cuda device (in fact attached to GPU memory), we path it through the CPU memory and convert it into a numpy object.","metadata":{}},{"cell_type":"code","source":"probs_numpy = probs.cpu().detach().numpy()\nprobs_numpy,type(probs_numpy)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:33:33.747916Z","iopub.execute_input":"2021-11-03T15:33:33.748205Z","iopub.status.idle":"2021-11-03T15:33:33.758111Z","shell.execute_reply.started":"2021-11-03T15:33:33.748175Z","shell.execute_reply":"2021-11-03T15:33:33.75722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we create a python dictionary to convert an encoded class into a class name.","metadata":{}},{"cell_type":"code","source":"encoder_to_class_name = dict(zip(list(range(0,len(dataset_fetch_20newsgroups['target_labels'].unique()))),dataset_fetch_20newsgroups['target_labels'].unique().tolist()))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T15:47:38.3356Z","iopub.execute_input":"2021-11-03T15:47:38.336229Z","iopub.status.idle":"2021-11-03T15:47:38.342873Z","shell.execute_reply.started":"2021-11-03T15:47:38.336193Z","shell.execute_reply":"2021-11-03T15:47:38.3421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In a last step, we display the predicted class of the new text. We use the numpy.argmax function.","metadata":{}},{"cell_type":"code","source":"encoder_to_class_name[np.argmax(probs_numpy)]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T16:04:02.586185Z","iopub.execute_input":"2021-11-03T16:04:02.586442Z","iopub.status.idle":"2021-11-03T16:04:02.594411Z","shell.execute_reply.started":"2021-11-03T16:04:02.586413Z","shell.execute_reply":"2021-11-03T16:04:02.59368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"9) In this part we explain how to predict the classes of a whole test dataset. A huggingFace PyTorch is GPU-memory consuming. When we want to predict on a whole dataset with it, we have to clean the memory with the gc package (garbage collection).<br><br>\nIn the above cell, we do a loop over the whole dataset.  At each step of this loop we call the tokenizer and the model. At each step of the loop, We also convert a PyTorch tensor object into a numpy array to release some GPU memory.<br><br>\n**CUDA** (or Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing unit (GPU) for general purpose processing.<br><br>\n\nY_test_proba in the above cell is a list of numpy probability vectors.","metadata":{}},{"cell_type":"code","source":"Y_true = test_dataset['label']\nY_test_email = test_dataset['text']\n\nimport gc\n\nY_test_proba = []\n\nfor text in test_dataset['text']:\n    token_output = tokenizer(text, padding=\"max_length\", truncation=True,return_tensors=\"pt\").to(\"cuda\")\n    Y_test_proba.append(model(**token_output)[0].softmax(1).cpu().detach().numpy())\n    del token_output\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T16:17:10.82089Z","iopub.execute_input":"2021-11-03T16:17:10.821429Z","iopub.status.idle":"2021-11-03T16:22:31.958444Z","shell.execute_reply.started":"2021-11-03T16:17:10.82139Z","shell.execute_reply":"2021-11-03T16:22:31.957629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ny_pred = [np.argmax(vect_proba) for vect_proba in Y_test_proba]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T16:25:58.409882Z","iopub.execute_input":"2021-11-03T16:25:58.410135Z","iopub.status.idle":"2021-11-03T16:25:58.421414Z","shell.execute_reply.started":"2021-11-03T16:25:58.410105Z","shell.execute_reply":"2021-11-03T16:25:58.420702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We comput the accuracy score:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(Y_true,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T16:26:19.967949Z","iopub.execute_input":"2021-11-03T16:26:19.9682Z","iopub.status.idle":"2021-11-03T16:26:19.979188Z","shell.execute_reply.started":"2021-11-03T16:26:19.968171Z","shell.execute_reply":"2021-11-03T16:26:19.978223Z"},"trusted":true},"execution_count":null,"outputs":[]}]}