{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "introduction-to-hugging-face-transformers.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabnancyuhp/DEEP-LEARNING/blob/main/NOTEBOOKS/introduction-to-hugging-face-transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ref : https://huggingface.co/transformers/training.html<br>\n",
        "ref : https://huggingface.co/docs/datasets/loading_datasets.html<br>\n",
        "ref : https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html<br>\n",
        "In this notebook, \n",
        "* we will see a text classification implementation on the IMDB dataset with the Transfer Learning technique. \n",
        "* we will show how to fine-tune a pretrained model from the Transformers library. \n",
        "* we will fine-tune BERT on the IMDB dataset\n",
        "\n",
        "BERT stands for Bidirectional Encoder Representations from Transformers. In TensorFlow, models can be directly trained using Keras and the fit method.<br><br>\n",
        "\n",
        "**Hugging Face** is an NLP-focused startup with a large open-source community, in particular around the **Transformers library.** 🤗/Transformers is a python-based library that exposes an API to use many well-known transformer architectures, such as BERT, RoBERTa, GPT-2 or DistilBERT, that obtain state-of-the-art results on a variety of NLP tasks like text classification, information extraction, question answering, and text generation. Those architectures come pre-trained with several sets of weights.<br><br>\n",
        "\n",
        "The **Transformers library** has seen super-fast growth in PyTorch and has recently been ported to TensorFlow 2.0, offering an API that now works with Keras’ fit API, TensorFlow Extended, and TPUs<br><br>\n",
        "\n",
        "**Hugging Face transformers** is a collection of state-of-the-art NLP (Natural Language Processing) models. They offer a wide variety of architectures to choose from (BERT, GPT-2, RoBERTa etc) as well as a hub of pre-trained models uploaded by users and organisations. <br><br>\n",
        "**Hugging Face Datasets** is a wrapper library that provides some tools to load and process data in many commonly used formats (CSV, JSON etc)."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-15T07:26:44.471907Z",
          "iopub.execute_input": "2021-09-15T07:26:44.472211Z",
          "iopub.status.idle": "2021-09-15T07:27:36.05847Z",
          "shell.execute_reply.started": "2021-09-15T07:26:44.472134Z",
          "shell.execute_reply": "2021-09-15T07:27:36.057214Z"
        },
        "id": "DQJU0IsabaVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Datasets\n",
        "**Hugging Face Datasets** is a wrapper library that provides some tools to load and process data in many commonly used formats (CSV, JSON etc).<br><br>\n",
        "Before we can fine-tune a model, we need a dataset. In this tutorial, we will show you how to fine-tune BERT on the IMDB dataset: the task is to classify whether movie reviews are positive or negative. Bellow, we load the IMBD data structure from the hugging-face hub. Row_datasets is DatasetDict object."
      ],
      "metadata": {
        "id": "A_lUHWMFbaVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "S1yLLk9AbaVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we show how a hugging-face DatasetDict is modelized. When we run the cell below we see that a hugging-face DatasetDict is a data structure close to a python dictionary. The raw_datasets DatasetDict has 3 sub-datasets: train, test and unsupervised. The unsupervised dataset is the union of train and test datasets.<br>\n",
        "To access the movies reviews from the training dataset, we have to run raw_datasets['train']['text']. To get the labels of the training dataset we run raw_datasets['train']['label']"
      ],
      "metadata": {
        "id": "l-VvOg7hbaVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "trusted": true,
        "id": "KJUu-rkDbaV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train']['text'][0:2]"
      ],
      "metadata": {
        "trusted": true,
        "id": "51Iyu2qJbaV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train']['label'][0:2]"
      ],
      "metadata": {
        "trusted": true,
        "id": "cL5V-6cebaV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = [len(x) for x in raw_datasets['train']['text']]"
      ],
      "metadata": {
        "trusted": true,
        "id": "YMVqXQsrbaV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess/Tokenize a hugging-face dataset\n",
        "Remind the goal of this notebook is to fin-tuned a Bert cased model. To preprocess our data, we will need a tokenizer. A tokenizer splits the data in tokens (these can be characters, words, part of words). For this task, we are using the tokenizer from the pre-trained model we selected (bert-base-cased)."
      ],
      "metadata": {
        "id": "W7oRqDZ1baV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZPW-S70LbaV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most importants features made during the tokenize step  are 'attention_mask', 'input_ids', 'label', 'token_type_ids'. Now, we don't need the text feature anymore. As a consequence, We remove the text columns from the huggingface dataset later in this notebook."
      ],
      "metadata": {
        "id": "zdRYpAZFbaV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "#tokenized_datasets"
      ],
      "metadata": {
        "trusted": true,
        "id": "oKByQ06gbaV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning the hugging-face bert-case model with tensorflow"
      ],
      "metadata": {
        "id": "F1gl_4o7baV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the dataset usable by a tensorflow model\n",
        "* First step: we define the content of the training set and the test set.\n",
        "* Second step: Second step: we make these 2 datasets compatibles with tensorflow \n",
        "* Third step: we make the training and test dataset suitables for a tensorflow model"
      ],
      "metadata": {
        "id": "FK_lWkOEbaV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "full_test_dataset = tokenized_datasets[\"test\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "xBbuhbAnbaWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Second step: Second step: we make these 2 datasets compatibles with tensorflow\n",
        "tf_train_dataset = full_train_dataset.with_format(\"tensorflow\")\n",
        "tf_test_dataset = full_test_dataset.with_format(\"tensorflow\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "tAPsMy7EbaWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Third step: we make the training and test dataset suitables for a tensorflow model\n",
        "import tensorflow as tf\n",
        "train_features = {x: tf_train_dataset[x].to_tensor() for x in tokenizer.model_input_names}\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
        "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
        "\n",
        "test_features = {x: tf_test_dataset[x].to_tensor() for x in tokenizer.model_input_names}\n",
        "test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_features, tf_test_dataset[\"label\"]))\n",
        "test_tf_dataset = test_tf_dataset.batch(8)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6iyRO2_HbaWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We import the tensorflow pretrained bert-base-cased model from the huggingface hub.\n",
        "We import the pretrained bert-base-cased model from the huggingface hub. The model we will import is paired with the tokenizer we imported earlier in this notebook. We import the bert-base-cased in the cell below. We set num_labels=2 because we have 2 distincts values in the label columns from our dataset. In the cell below, model is a tensorflow model."
      ],
      "metadata": {
        "id": "QKGe5MzLbaWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "LciVrC62baWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We fine-tune the pretrained model\n",
        "We compile the tensorflow model as we always do when we use tensorflow. Here we use the\n",
        "* the adam  optimizer with learning_rate=5e-5\n",
        "* SparseCategoricalCrossentropy loss fucntion\n",
        "* SparseCategoricalAccuracy() metrics\n",
        "\n",
        "To fine-tune properly a tensorflow hugging-face model with an adam optimizer, it recomded to set the leraning_rate to 5e-5."
      ],
      "metadata": {
        "id": "vPpEuGQobaWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "model.fit(train_tf_dataset, validation_data=test_tf_dataset, epochs=2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "63hMMlBibaWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the compile step, model is a fine-tuned transformer model. Then, we can apply model on a new movie review."
      ],
      "metadata": {
        "id": "AzgDLtJ3baWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_movie_review = [\"I was extraordinarily impressed by this film. It's one of the best sports films \\\n",
        "                    I've every seen. The visuals in this film are outstanding. I love the sequences \\\n",
        "                    in which the camer tracks the ball as it flies through the air or into the cup. \\\n",
        "                    The film moves well, offering both excitement and drama. The cinematography was fantastic.\\\n",
        "                    <br /><br />The acting performances are great. I was surprised by young Shia LaBeouf.\\\n",
        "                    He does well in this role. Stephen Dillane is also good as the brooding Harry Vardon. \\\n",
        "                    Peter Firth, Justin Ashforth, and Elias Koteas offer able support. \\\n",
        "                    The film is gripping and entertaining and for the first time in my \\\n",
        "                    life actually made me want to watch a golf tournament.\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "HYF0yRIAbaWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to apply the model to the nes review, We have to:\n",
        "* vectorize the new movie review\n",
        "* put the vectorized text into a tf.data.Dataset\n",
        "* to apply the .batch(1) method to make review_data suitable for the fine-tuned model."
      ],
      "metadata": {
        "id": "tO0Dp-HqbaWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we vectorize\n",
        "token_new_review = tokenizer(new_movie_review, padding=\"max_length\", truncation=True)\n",
        "\n",
        "#put the vectorized text into a tf.data.Dataset\n",
        "import tensorflow as tf\n",
        "review_data = tf.data.Dataset.from_tensor_slices((dict(token_new_review)))\n",
        "\n",
        "#make review_data suitable for the fine-tuned model.\n",
        "review_data_batch = review_data.batch(1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "31FgUMeRbaWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can apply the fine-tuned model to review_data_batch. Then, we apply a softmax function to the result to get a probability vector."
      ],
      "metadata": {
        "id": "TzQlgf2NbaWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "review_sentiment = ['negatif review','positif review']\n",
        "\n",
        "#we apply the fine-tuned model to review_data_batch\n",
        "pred = model.predict(review_data_batch).logits\n",
        "#we apply a sofmax function to get a probability vector\n",
        "prob_vector = tf.nn.softmax(pred, axis=1).numpy()\n",
        "prob_vector"
      ],
      "metadata": {
        "trusted": true,
        "id": "K6AO3H92baWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we display the predicted class of the new review"
      ],
      "metadata": {
        "id": "s3dkicTCbaWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "review_sentiment = ['negatif review','positif review']\n",
        "review_sentiment[np.argmax(prob_vector)]"
      ],
      "metadata": {
        "trusted": true,
        "id": "hz4wTEXBbaWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.DataFrame({'text':tokenized_datasets[\"test\"]['text'],'sentiment':tokenized_datasets[\"test\"]['label']})\n",
        "data[data['sentiment']==1].iloc[12495,0]"
      ],
      "metadata": {
        "trusted": true,
        "id": "x9YT4GKpbaWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning in PyTorch with the Trainer API\n",
        "**In this section, we deal with a PyTorch pretrained huggingface transformer. You might need to restart your notebook at this stage to free some memory.**\n",
        "In order to use the hugging-face pytorch Trainer API, you have to set up WANDB_DISABLED to true. Wandb means weights & Biases integration.  We set WANDB_DISABLED to “true” to disable wandb entirely. "
      ],
      "metadata": {
        "id": "RL3kSuwObaWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-04T15:46:26.945187Z",
          "iopub.execute_input": "2021-11-04T15:46:26.945475Z",
          "iopub.status.idle": "2021-11-04T15:46:26.949546Z",
          "shell.execute_reply.started": "2021-11-04T15:46:26.945443Z",
          "shell.execute_reply": "2021-11-04T15:46:26.948375Z"
        },
        "trusted": true,
        "id": "p0YpEtDcbaWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the huggingface IMDB dataset:"
      ],
      "metadata": {
        "id": "NwQObmZJbaWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-04T15:46:29.599457Z",
          "iopub.execute_input": "2021-11-04T15:46:29.599889Z",
          "iopub.status.idle": "2021-11-04T15:46:31.154279Z",
          "shell.execute_reply.started": "2021-11-04T15:46:29.599851Z",
          "shell.execute_reply": "2021-11-04T15:46:31.153563Z"
        },
        "trusted": true,
        "id": "ZCycGAIJbaWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We vectorize the movie reviews with a bert-case- tokenizer. Then we create the training dataset and the test dataset.**<br> In the cell , full train_dataset and full_text dataset are 2 huggingface datasets."
      ],
      "metadata": {
        "id": "mj5-CLWTbaWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "full_test_dataset = tokenized_datasets[\"test\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-04T15:46:35.863716Z",
          "iopub.execute_input": "2021-11-04T15:46:35.864286Z",
          "iopub.status.idle": "2021-11-04T15:47:58.211784Z",
          "shell.execute_reply.started": "2021-11-04T15:46:35.864248Z",
          "shell.execute_reply": "2021-11-04T15:47:58.211039Z"
        },
        "trusted": true,
        "id": "8l300wlfbaWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in order to fine-tune a bert huggingface model with the trainer API,\n",
        "* We have to load the transfromer model we want to use. \n",
        "* We have to set the TrainingArguments\n",
        "* We set the Trainer\n",
        "\n",
        "The model we load is paired with the tokenizer we loaded earlier in this exemple. The model in the cell below is a Pytorch model. In the case of a pretrained pytorch model we use AutoModelForSequenceClassification instead of TFAutoModelForSequenceClassification.  "
      ],
      "metadata": {
        "id": "rO3W8x02baWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We load the transfromer model we want to use**<br>\n",
        "In the cell below, model is a PyTorch model."
      ],
      "metadata": {
        "id": "5rqkhi4obaWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "pytorch_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-04T15:49:07.933993Z",
          "iopub.execute_input": "2021-11-04T15:49:07.934796Z",
          "iopub.status.idle": "2021-11-04T15:49:10.333776Z",
          "shell.execute_reply.started": "2021-11-04T15:49:07.934738Z",
          "shell.execute_reply": "2021-11-04T15:49:10.333011Z"
        },
        "trusted": true,
        "id": "kfd6FMAkbaWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We set the TrainingArguments**<br>\n",
        "The Trainer API uses some training arguments we call with a TrainingArguments object. You can use the default configuration with TrainingArguments(\"test_trainer\"). Else, you can set some arguments such that:\n",
        "* output_dir='./results',     the output directory\n",
        "* num_train_epochs the total number of training epochs\n",
        "* per_device_train_batch_size the batch size per device during training\n",
        "* per_device_eval_batch_size=20 the batch size for evaluation\n",
        "* weight_decay=0.01 the strength of weight decay\n",
        "* logging_dir='./logs' the directory for storing logs"
      ],
      "metadata": {
        "id": "kA_I3n2vbaWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We set some arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")"
      ],
      "metadata": {
        "id": "gdhI2XNkbaWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we use the default configuration\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\"test_trainer\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-04T15:49:21.592064Z",
          "iopub.execute_input": "2021-11-04T15:49:21.592884Z",
          "iopub.status.idle": "2021-11-04T15:49:21.648374Z",
          "shell.execute_reply.started": "2021-11-04T15:49:21.592835Z",
          "shell.execute_reply": "2021-11-04T15:49:21.647636Z"
        },
        "trusted": true,
        "id": "R4-t7KGxbaWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We set the Trainer and begin the training stage with Trainer.train().**<br>\n",
        "The Trainer object takes some arguments such that:\n",
        "* model : a huggingface pretrained PyTorch model\n",
        "* args : a TrainingArguments defined earlier \n",
        "* train_dataset : a huggingface dataset made from a huggingface tokenizer step  \n",
        "* eval_datase : a huggingface dataset made from a huggingface tokenizer step\n",
        "\n",
        "The target value of a huggingface dataset used by the Trainer should always be named label else the Train API doesn't work."
      ],
      "metadata": {
        "id": "wiaJDRAMbaWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(model=pytorch_model, args=training_args, train_dataset=full_train_dataset)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-04T15:49:28.709915Z",
          "iopub.execute_input": "2021-11-04T15:49:28.710594Z",
          "iopub.status.idle": "2021-11-04T16:58:50.348165Z",
          "shell.execute_reply.started": "2021-11-04T15:49:28.710554Z",
          "shell.execute_reply": "2021-11-04T16:58:50.346663Z"
        },
        "trusted": true,
        "id": "ZTwD6K7rbaWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the Trainer step, pytorch_model is a fine-tuned transformer model. Then, we can apply pytorch_model on a new movie review. We have to\n",
        "* vectorize the new movie review\n",
        "* Apply the pytorch on the vectorized new text \n",
        "* use softmax function to get a PyTorch tensor probability vector\n",
        "* convert the previous PyTorch tensor into a numpy array"
      ],
      "metadata": {
        "id": "59TRb4ycbaWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_movie_review = [\"I was extraordinarily impressed by this film. It's one of the best sports films \\\n",
        "                    I've every seen. The visuals in this film are outstanding. I love the sequences \\\n",
        "                    in which the camer tracks the ball as it flies through the air or into the cup. \\\n",
        "                    The film moves well, offering both excitement and drama. The cinematography was fantastic.\\\n",
        "                    <br /><br />The acting performances are great. I was surprised by young Shia LaBeouf.\\\n",
        "                    He does well in this role. Stephen Dillane is also good as the brooding Harry Vardon. \\\n",
        "                    Peter Firth, Justin Ashforth, and Elias Koteas offer able support. \\\n",
        "                    The film is gripping and entertaining and for the first time in my \\\n",
        "                    life actually made me want to watch a golf tournament.\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-04T17:03:11.685229Z",
          "iopub.execute_input": "2021-11-04T17:03:11.685848Z",
          "iopub.status.idle": "2021-11-04T17:03:11.691661Z",
          "shell.execute_reply.started": "2021-11-04T17:03:11.685808Z",
          "shell.execute_reply": "2021-11-04T17:03:11.690861Z"
        },
        "trusted": true,
        "id": "DPr9ehb3baWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CUDA** (or Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing unit (GPU) for general purpose processing. **To(\"cuda\") in tokenizer step means we put the vectorized text in the GPU memory.**"
      ],
      "metadata": {
        "id": "wUaFyLJGbaWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we vectorize the new movie review\n",
        "review_token =tokenizer(new_movie_review, padding=\"max_length\", truncation=True,return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "#get a PyTorch tensor probability vector\n",
        "prob_pytorch_tensor = pytorch_model(**review_token )[0].softmax(1)\n",
        "\n",
        "#convert the previous PyTorch tensor into a numpy array\n",
        "prob_numpy_array = prob_pytorch_tensor.cpu().detach().numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-04T17:04:44.563041Z",
          "iopub.execute_input": "2021-11-04T17:04:44.563758Z",
          "iopub.status.idle": "2021-11-04T17:04:44.597777Z",
          "shell.execute_reply.started": "2021-11-04T17:04:44.563723Z",
          "shell.execute_reply": "2021-11-04T17:04:44.597086Z"
        },
        "trusted": true,
        "id": "ZF0uJhIvbaWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we display the predicted class of the new review"
      ],
      "metadata": {
        "id": "Q56ELsNIbaWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "review_sentiment = ['negatif review','positif review']\n",
        "review_sentiment[np.argmax(prob_numpy_array)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-04T17:04:51.096027Z",
          "iopub.execute_input": "2021-11-04T17:04:51.096316Z",
          "iopub.status.idle": "2021-11-04T17:04:51.102843Z",
          "shell.execute_reply.started": "2021-11-04T17:04:51.096287Z",
          "shell.execute_reply": "2021-11-04T17:04:51.102015Z"
        },
        "trusted": true,
        "id": "Uxl_MFfWbaWr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}