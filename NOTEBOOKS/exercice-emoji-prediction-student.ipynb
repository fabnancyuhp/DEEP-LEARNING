{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data cleaning functions\n",
    "\n",
    "here, we provide some useful functions to clean text data. You have to call these functions during preprocessing text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T14:36:35.010429Z",
     "iopub.status.busy": "2021-11-21T14:36:35.010049Z",
     "iopub.status.idle": "2021-11-21T14:36:35.019087Z",
     "shell.execute_reply": "2021-11-21T14:36:35.017703Z",
     "shell.execute_reply.started": "2021-11-21T14:36:35.010400Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "import re\n",
    "\n",
    "#Removes HTML syntaxes\n",
    "def remove_html(data):\n",
    "    html_tag=re.compile(r'<.*?>')\n",
    "    data=html_tag.sub(r'',data)\n",
    "    return data\n",
    "\n",
    "#Removes URL data\n",
    "def _remove_urls(x):\n",
    "    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n",
    "\n",
    "#Removes emails\n",
    "def _remove_emails(x):\n",
    "    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\n",
    "\n",
    "def _remove_rt(x):\n",
    "    return re.sub(r'\\brt\\b', '', x).strip()\n",
    "\n",
    "def _remove_special_chars(x):\n",
    "    x = re.sub(r'[^\\w ]+', \"\", x)\n",
    "    x = ' '.join(x.split())\n",
    "    return x\n",
    "\n",
    "def remove_digit(x):\n",
    "    return re.sub(r'[0-9]', '', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise: Emojis prediction\n",
    "Before doing this exercise, I recomand you to read carefully  the notebooks about RNN applications.\n",
    "* https://nbviewer.ipython.org/github/fabnancyuhp/DEEP-LEARNING/blob/main/NOTEBOOKS/text-mining-with-recurrent-neural-networks.ipynb\n",
    "* https://nbviewer.ipython.org/github/fabnancyuhp/DEEP-LEARNING/blob/main/NOTEBOOKS/simples-applications-of-recurrent-neural-networks.ipynb\n",
    "\n",
    "\n",
    "The goal of this exercise is to construct LSTM neural network to predict the emoji from tweets. In other words, given a tweet, we want to predict the emoji chosen by the author of the tweet. This exercise is a textmining problem close to sentiment analysis. You have to run the above cell \n",
    "* to import the data used for this exercise\n",
    "* to creat a python dictionary making a correspondance between numbers and emojis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:13:35.504320Z",
     "iopub.status.busy": "2021-11-21T15:13:35.503971Z",
     "iopub.status.idle": "2021-11-21T15:13:35.876930Z",
     "shell.execute_reply": "2021-11-21T15:13:35.875658Z",
     "shell.execute_reply.started": "2021-11-21T15:13:35.504289Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "import requests, io, zipfile, pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "number_emoji = {0:'ðŸ‡ºðŸ‡¸',1:'ðŸŽ„',2:'ðŸ˜œ'}\n",
    "emoji_to_number = {'ðŸ‡ºðŸ‡¸':0,'ðŸŽ„':1,'ðŸ˜œ':2}\n",
    "\n",
    "fil=\"https://raw.githubusercontent.com/fabnancyuhp\\\n",
    "/DEEP-LEARNING/main/DATA/Twitter_emoji_simplified.csv\"\n",
    "\n",
    "tweet = pd.read_csv(fil)\n",
    "tweet = tweet[['TEXT','emoji']]\n",
    "tweet.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1/ How many distinct emojis does the tweet dataset have? Create a column tweet['class'] that encode tweet['emoji'] using emoji_to_number dictionary. Tweet['class'] will be the target values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:21:02.480753Z",
     "iopub.status.busy": "2021-11-21T15:21:02.480350Z",
     "iopub.status.idle": "2021-11-21T15:21:02.484976Z",
     "shell.execute_reply": "2021-11-21T15:21:02.483863Z",
     "shell.execute_reply.started": "2021-11-21T15:21:02.480718Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "#distinct emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:31:02.521980Z",
     "iopub.status.busy": "2021-11-21T15:31:02.521421Z",
     "iopub.status.idle": "2021-11-21T15:31:02.531320Z",
     "shell.execute_reply": "2021-11-21T15:31:02.530197Z",
     "shell.execute_reply.started": "2021-11-21T15:31:02.521943Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "#The emoji_to_number dictionary\n",
    "emoji_to_number = {'ðŸ‡ºðŸ‡¸':0,'ðŸŽ„':1,'ðŸ˜œ':2}\n",
    "\n",
    "#use a lambda function with emoji_to_number dictionary\n",
    "tweet['class']  = tweet['emoji'].apply(#lamnda function here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:31:11.049347Z",
     "iopub.status.busy": "2021-11-21T15:31:11.048793Z",
     "iopub.status.idle": "2021-11-21T15:31:11.189800Z",
     "shell.execute_reply": "2021-11-21T15:31:11.188800Z",
     "shell.execute_reply.started": "2021-11-21T15:31:11.049312Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "#Quick view of the class repartition\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "sns.countplot(x='class',data=tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2/ Print the emoji and class of the row number 101 of the tweet dataset? hint : use pandas iloc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:22:47.392758Z",
     "iopub.status.busy": "2021-11-21T15:22:47.392233Z",
     "iopub.status.idle": "2021-11-21T15:22:47.397398Z",
     "shell.execute_reply": "2021-11-21T15:22:47.396658Z",
     "shell.execute_reply.started": "2021-11-21T15:22:47.392724Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "#Your answer here\n",
    "row_101 = tweet.iloc[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3/ Make a word cloud with all tweets that have an emoji ðŸŽ„."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:31:26.307918Z",
     "iopub.status.busy": "2021-11-21T15:31:26.307476Z",
     "iopub.status.idle": "2021-11-21T15:31:30.223645Z",
     "shell.execute_reply": "2021-11-21T15:31:30.222860Z",
     "shell.execute_reply.started": "2021-11-21T15:31:26.307880Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "#your code here\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "tweet_tree = tweet.loc[tweet['emoji']=='ðŸŽ„']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4/ The goal of this question is to clean the column tweet['TEXT']. For this purpose use sequentially the following functions we defined at the beginning of this notebook.\n",
    "* remove_html\n",
    "* _remove_urls\n",
    "* _remove_emails\n",
    "* _remove_rt\n",
    "* _remove_special_chars\n",
    "* remove_digit\n",
    "\n",
    "Convert Tweet['TEXT'] into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:51:44.931391Z",
     "iopub.status.busy": "2021-11-21T15:51:44.930487Z",
     "iopub.status.idle": "2021-11-21T15:51:45.118177Z",
     "shell.execute_reply": "2021-11-21T15:51:45.117114Z",
     "shell.execute_reply.started": "2021-11-21T15:51:44.931346Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "#your code to clean the column \n",
    "tweet['TEXT'] = tweet['TEXT'].apply(lambda x:remove_html(str(x)))\n",
    "tweet['TEXT'] = tweet['TEXT'].apply(lambda x:_remove_urls(str(x)))\n",
    "#Apply _remove_emails, _remove_rt, _remove_special_chars, remove_digit\n",
    "\n",
    "\n",
    "#Convert Tweet['TEXT'] into lowercase with apply function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5/ Tokenize tweet['TEXT'] using tensorflow.keras. Store the result in X. Compute Y = tweet['class'].values to have the target values. What is the vocabulary size of the tokenization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:32:37.009754Z",
     "iopub.status.busy": "2021-11-21T15:32:37.009070Z",
     "iopub.status.idle": "2021-11-21T15:32:37.167081Z",
     "shell.execute_reply": "2021-11-21T15:32:37.166088Z",
     "shell.execute_reply.started": "2021-11-21T15:32:37.009701Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "X_token = [d.split() for d in list(tweet['TEXT'])]\n",
    "tokenizer = Tokenizer()\n",
    "#code here \n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-04T14:09:46.998319Z",
     "iopub.status.busy": "2021-09-04T14:09:46.99799Z",
     "iopub.status.idle": "2021-09-04T14:09:47.006933Z",
     "shell.execute_reply": "2021-09-04T14:09:47.004788Z",
     "shell.execute_reply.started": "2021-09-04T14:09:46.998288Z"
    }
   },
   "source": [
    "6/ X is a list of sequences as the result of tokenization. Plot a histogram of the sequence lengths. Make the padding process of X with maxlen=10 and truncating='post'. Store it in X_padd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:32:48.276669Z",
     "iopub.status.busy": "2021-11-21T15:32:48.275959Z",
     "iopub.status.idle": "2021-11-21T15:32:48.495597Z",
     "shell.execute_reply": "2021-11-21T15:32:48.494720Z",
     "shell.execute_reply.started": "2021-11-21T15:32:48.276618Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "#Plot the histogram of the sequence lengths\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist([len(x) for x in X],bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:32:59.685251Z",
     "iopub.status.busy": "2021-11-21T15:32:59.684767Z",
     "iopub.status.idle": "2021-11-21T15:32:59.740563Z",
     "shell.execute_reply": "2021-11-21T15:32:59.739522Z",
     "shell.execute_reply.started": "2021-11-21T15:32:59.685220Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 10\n",
    "X_pad = #code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7/ Apply to_categorical to Y with num_class=3 and store the result in Y_vect. After, Split X_padd and Y_vect into a train set and a test set using train_test_split from sklearn. You have to set test_size=0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:33:18.881222Z",
     "iopub.status.busy": "2021-11-21T15:33:18.880605Z",
     "iopub.status.idle": "2021-11-21T15:33:18.887844Z",
     "shell.execute_reply": "2021-11-21T15:33:18.886936Z",
     "shell.execute_reply.started": "2021-11-21T15:33:18.881188Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y_vect = \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, Y_vect, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "9) Create an LSTM network with the following layers :\n",
    "* an Embedding layer with output_dim=50. Output_dim is the embedding dimention. input_length argument should equal to the maxlen you used in question 6. The input is equal to the vocabulary size you finded in question 5\n",
    "* An LSTM layer with 64 units\n",
    "* A dropout with a rate 0.3\n",
    "* The output layer is a Dense layer\n",
    "\n",
    "Compile this LSTM with optimizer = 'adam' and metrics=['acc']. Train this LSTM with epochs=4 and validation_split=0.1. Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:33:31.266776Z",
     "iopub.status.busy": "2021-11-21T15:33:31.266421Z",
     "iopub.status.idle": "2021-11-21T15:33:40.606314Z",
     "shell.execute_reply": "2021-11-21T15:33:40.605652Z",
     "shell.execute_reply.started": "2021-11-21T15:33:31.266745Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "from tensorflow.keras.layers import Dense,LSTM, Embedding,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "embedding_dim = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:33:47.825909Z",
     "iopub.status.busy": "2021-11-21T15:33:47.825375Z",
     "iopub.status.idle": "2021-11-21T15:33:47.999159Z",
     "shell.execute_reply": "2021-11-21T15:33:47.997980Z",
     "shell.execute_reply.started": "2021-11-21T15:33:47.825873Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=4,validation_split=0.1)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "10) Use the model you've just implemented to predict the emojis of the following tweets :\n",
    "  \n",
    "  * tweet_5 = 'Merry Christmas you filthy little animals. Wearing a @user ugly sweater featuring Santaâ€¦'\n",
    "  * tweet_11 = \"let's see those votes @ Merica\"\n",
    "  * tweet_0 = \"#Halloween Gig 2! E-Ticket will be going crazy tonight. I'll be there too I guess Sandy fromâ€¦\"\n",
    "  \n",
    "For this purpose you can use the function handle we define in the cell bellow. Before use this function you have to fite the model in question 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:33:59.912990Z",
     "iopub.status.busy": "2021-11-21T15:33:59.912464Z",
     "iopub.status.idle": "2021-11-21T15:34:00.353005Z",
     "shell.execute_reply": "2021-11-21T15:34:00.352005Z",
     "shell.execute_reply.started": "2021-11-21T15:33:59.912956Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "tweet_5 = 'Merry Christmas you filthy little animals. Wearing a @user ugly sweater featuring Santaâ€¦'\n",
    "tweet_11 = \"let's see those votes @ Merica\"\n",
    "tweet_0 = \"#Halloween Gig 2! E-Ticket will be going crazy tonight. I'll be there too I guess Sandy fromâ€¦\"\n",
    "\n",
    "number_emoji = {0:'ðŸ‡ºðŸ‡¸',1:'ðŸŽ„',2:'ðŸ˜œ'}\n",
    "\n",
    "import numpy as np\n",
    "def handle(tweet):\n",
    "    clean_tweet = remove_html(str(tweet))\n",
    "    clean_tweet = _remove_urls(str(clean_tweet))\n",
    "    clean_tweet = _remove_emails(str(clean_tweet))\n",
    "    clean_tweet = _remove_rt(str(clean_tweet))\n",
    "    clean_tweet = _remove_special_chars(str(clean_tweet))\n",
    "    clean_tweet = remove_digit(str(clean_tweet))\n",
    "    clean_tweet = str(clean_tweet).lower()\n",
    "    clean_tweet_token=[clean_tweet.split()]\n",
    "    clean_tweet_sequence = tokenizer.texts_to_sequences(clean_tweet_token)\n",
    "    clean_tweet_sequence_pad = pad_sequences(clean_tweet_sequence,maxlen=10,truncating='post' )\n",
    "    clean_tweet_proba_vect = model.predict(clean_tweet_sequence_pad)\n",
    "    clean_tweet_class = np.argmax(clean_tweet_proba_vect)\n",
    "    #emojibis = le.inverse_transform([clean_tweet_class])[0]\n",
    "    emoji = number_emoji[clean_tweet_class]\n",
    "    return(emoji)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-04T14:14:01.334826Z",
     "iopub.status.busy": "2021-09-04T14:14:01.334509Z",
     "iopub.status.idle": "2021-09-04T14:14:13.779659Z",
     "shell.execute_reply": "2021-09-04T14:14:13.778836Z",
     "shell.execute_reply.started": "2021-09-04T14:14:01.334799Z"
    }
   },
   "source": [
    "11/ Build a Neural network model2 with the following layers:\n",
    "* an Embedding layer with output_dim=50. Output_dim is the embedding dimention. The input_dim is equal to the vocabulary size you finded in question 5\n",
    "* An LSTM layer with 64 units. You have to set return_sequences=True\n",
    "* A dropout with a rate 0.3\n",
    "* An LSTM layer with 32 units\n",
    "* A dropout with a rate 0.2\n",
    "* A dense layer with 10 units and a relu activation function.\n",
    "* The output layer is a Dense layer\n",
    "\n",
    "Compile model2. Don't train model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:36:23.518142Z",
     "iopub.status.busy": "2021-11-21T15:36:23.517767Z",
     "iopub.status.idle": "2021-11-21T15:36:24.379718Z",
     "shell.execute_reply": "2021-11-21T15:36:24.378697Z",
     "shell.execute_reply.started": "2021-11-21T15:36:23.518105Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "12/ Build a bidirectional LSTM to handle the emoji prediction. You can retake the LSTM you built at the question 9. You only have to make a little change. Compile this bidirectional LSTM. Train this LSTM with 4 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T15:36:37.104352Z",
     "iopub.status.busy": "2021-11-21T15:36:37.103868Z",
     "iopub.status.idle": "2021-11-21T15:36:49.174811Z",
     "shell.execute_reply": "2021-11-21T15:36:49.173712Z",
     "shell.execute_reply.started": "2021-11-21T15:36:37.104322Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "from tensorflow.keras.layers import Dense,LSTM, Embedding,Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "embedding_dim = 50\n",
    "\n",
    "#the number of units of the last dense layer\n",
    "output_size = len(np.unique(y_train)) #=20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "13/ **This question is not compulsory. This question is solved.** So far we have dealt with LSTM to solve NLP problems. It is possible to solve NLP tasks using a CNN (convolutional neural networks). Build a CNN with the following layers:\n",
    "* an Embedding layer with output_dim=50. Output_dim is the embedding dimention. input_length argument should equal to the maxlen you used in question 6. The input is equal to the vocabulary size you finded in question 5\n",
    "* a Conv1D layer with filters=32, kernel_size=3, activation='relu'\n",
    "* a GlobalMaxPooling1D()\n",
    "* a Dense layer with 10 units and activation='relu'\n",
    "* the output layer is a dense layer\n",
    "\n",
    "We call this model model_cnn. Compile this model. Train this model with 8 epochs and validation_split=0.1. Evaluate your model on the test set.<br>\n",
    "\n",
    "How many trainable parameters does model_cnn have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T14:56:37.200156Z",
     "iopub.status.busy": "2021-11-21T14:56:37.199774Z",
     "iopub.status.idle": "2021-11-21T14:56:44.633308Z",
     "shell.execute_reply": "2021-11-21T14:56:44.632350Z",
     "shell.execute_reply.started": "2021-11-21T14:56:37.200126Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv1D, Embedding, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "embedding_dim = 50\n",
    "maxlen = 10\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(vocab_size,output_dim = embedding_dim,input_length=maxlen))\n",
    "model_cnn.add(Conv1D(32, 3, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(10, activation='relu'))\n",
    "model_cnn.add(Dense(3,activation='softmax'))\n",
    "\n",
    "model_cnn.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model_cnn.fit(X_train,y_train,epochs=8,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T14:56:49.894502Z",
     "iopub.status.busy": "2021-11-21T14:56:49.894142Z",
     "iopub.status.idle": "2021-11-21T14:56:50.005047Z",
     "shell.execute_reply": "2021-11-21T14:56:50.003911Z",
     "shell.execute_reply.started": "2021-11-21T14:56:49.894470Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "model_cnn.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-21T14:57:08.001348Z",
     "iopub.status.busy": "2021-11-21T14:57:08.000714Z",
     "iopub.status.idle": "2021-11-21T14:57:08.015142Z",
     "shell.execute_reply": "2021-11-21T14:57:08.013774Z",
     "shell.execute_reply.started": "2021-11-21T14:57:08.001310Z"
    },
    "trusted": true
   },
   "outputs": [
   ],
   "source": [
    "model_cnn.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}