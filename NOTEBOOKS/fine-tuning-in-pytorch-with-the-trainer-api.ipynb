{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning in PyTorch with the Trainer API\n* https://huggingface.co/blog/sentiment-analysis-python\n**In this section, we deal with a PyTorch pretrained huggingface transformer. You might need to restart your notebook at this stage to free some memory.**\nIn order to use the hugging-face pytorch Trainer API, you have to set up WANDB_DISABLED to true. Wandb means weights & Biases integration.  We set WANDB_DISABLED to “true” to disable wandb entirely. ","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nimport gc\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:17:44.397725Z","iopub.execute_input":"2022-12-27T15:17:44.398356Z","iopub.status.idle":"2022-12-27T15:17:44.504845Z","shell.execute_reply.started":"2022-12-27T15:17:44.398263Z","shell.execute_reply":"2022-12-27T15:17:44.503835Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"We download the huggingface IMDB dataset:","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nraw_datasets = load_dataset(\"imdb\")\n\nfull_train_dataset = raw_datasets[\"train\"].shuffle(seed=42).select([i for i in list(range(3000))])\nfull_test_dataset = raw_datasets[\"test\"].shuffle(seed=42).select([i for i in list(range(300))])\n\nimport gc\ndel raw_datasets\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:17:50.742784Z","iopub.execute_input":"2022-12-27T15:17:50.743149Z","iopub.status.idle":"2022-12-27T15:18:24.016333Z","shell.execute_reply.started":"2022-12-27T15:17:50.743116Z","shell.execute_reply":"2022-12-27T15:18:24.015104Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ba7550a5c4436d8d4efd57608c4728"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05d1a3ed4b54f4384a53d066d0d5e5a"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f15d352122840d9bd8dc587d3c3b230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"530e008776dd4ef1a11a6d3b0a23e7b1"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"75"},"metadata":{}}]},{"cell_type":"markdown","source":"**We vectorize the movie reviews with a bert-case- tokenizer. Then we create the training dataset and the test dataset.**<br> In the cell , full train_dataset and full_text dataset are 2 huggingface datasets.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n#tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n\n#full_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))\n#full_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(3000))\nfull_train_dataset = full_train_dataset.map(tokenize_function, batched=True)\nfull_test_dataset = full_test_dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:18:24.021494Z","iopub.execute_input":"2022-12-27T15:18:24.022314Z","iopub.status.idle":"2022-12-27T15:18:32.015816Z","shell.execute_reply.started":"2022-12-27T15:18:24.022278Z","shell.execute_reply":"2022-12-27T15:18:32.014684Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3837e44d3945a4b3d78b2ddaf11738"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64442d07023a467aa87094d164efdbff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7897eef1a9c47879f381e65e3f63b7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f45fe3e7f3f4193be46943560b563da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288306afa10d4567babfc74c503325aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b17130de4984e1ba93c7f9435ffd906"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now, in order to fine-tune a bert huggingface model with the trainer API,\n* We have to load the transfromer model we want to use. \n* We have to set the TrainingArguments\n* We set the Trainer\n\nThe model we load is paired with the tokenizer we loaded earlier in this exemple. The model in the cell below is a Pytorch model. In the case of a pretrained pytorch model we use AutoModelForSequenceClassification instead of TFAutoModelForSequenceClassification.  ","metadata":{}},{"cell_type":"markdown","source":"**We load the transfromer model we want to use**<br>\nIn the cell below, model is a PyTorch model.","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nfrom transformers import AutoModelForSequenceClassification\n\n#pytorch_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\npytorch_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:18:39.608955Z","iopub.execute_input":"2022-12-27T15:18:39.609680Z","iopub.status.idle":"2022-12-27T15:18:50.410590Z","shell.execute_reply.started":"2022-12-27T15:18:39.609645Z","shell.execute_reply":"2022-12-27T15:18:50.409605Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed956bc9e9a43d59b54d0db516e16a4"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**We set the TrainingArguments**<br>\nThe Trainer API uses some training arguments we call with a TrainingArguments object. You can use the default configuration with TrainingArguments(\"test_trainer\"). Else, you can set some arguments such that:\n* output_dir='./results',     the output directory\n* num_train_epochs the total number of training epochs\n* per_device_train_batch_size the batch size per device during training\n* per_device_eval_batch_size=20 the batch size for evaluation\n* weight_decay=0.01 the strength of weight decay\n* logging_dir='./logs' the directory for storing logs","metadata":{}},{"cell_type":"code","source":"#We set some arguments\nfrom transformers import TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=20,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:21:01.067802Z","iopub.execute_input":"2022-12-27T15:21:01.068456Z","iopub.status.idle":"2022-12-27T15:21:01.077509Z","shell.execute_reply.started":"2022-12-27T15:21:01.068416Z","shell.execute_reply":"2022-12-27T15:21:01.076262Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"#we use the default configuration. We change the number of epochs to 1\nfrom transformers import TrainingArguments\ntraining_args = TrainingArguments(\"test_trainer\")\ntraining_args.num_train_epochs = 1\n#training_args.per_device_eval_batch_size = 2\n#training_args.per_device_train_batch_size = 2\n#training_args","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:20:42.141502Z","iopub.execute_input":"2022-12-27T15:20:42.141852Z","iopub.status.idle":"2022-12-27T15:20:46.252641Z","shell.execute_reply.started":"2022-12-27T15:20:42.141821Z","shell.execute_reply":"2022-12-27T15:20:46.251669Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**We set the Trainer and begin the training stage with Trainer.train().**<br>\nThe Trainer object takes some arguments such that:\n* model : a huggingface pretrained PyTorch model\n* args : a TrainingArguments defined earlier \n* train_dataset : a huggingface dataset made from a huggingface tokenizer step  \n* eval_datase : a huggingface dataset made from a huggingface tokenizer step\n\nThe target value of a huggingface dataset used by the Trainer should always be named label else the Train API doesn't work.","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\n\nfrom transformers import Trainer\n\ntrainer = Trainer(model=pytorch_model, args=training_args, train_dataset=full_train_dataset)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:22:05.228298Z","iopub.execute_input":"2022-12-27T15:22:05.228648Z","iopub.status.idle":"2022-12-27T15:26:13.644982Z","shell.execute_reply.started":"2022-12-27T15:22:05.228617Z","shell.execute_reply":"2022-12-27T15:26:13.644060Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 3000\n  Num Epochs = 3\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 564\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [564/564 04:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.355000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nModel weights saved in ./results/checkpoint-500/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=564, training_loss=0.3428031130039946, metrics={'train_runtime': 243.078, 'train_samples_per_second': 37.025, 'train_steps_per_second': 2.32, 'total_flos': 1192206587904000.0, 'train_loss': 0.3428031130039946, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"After the Trainer step, pytorch_model is a fine-tuned transformer model. Then, we can apply pytorch_model on a new movie review. We have to\n* vectorize the new movie review\n* Apply the pytorch on the vectorized new text \n* use softmax function to get a PyTorch tensor probability vector\n* convert the previous PyTorch tensor into a numpy array","metadata":{}},{"cell_type":"code","source":"new_movie_review = [\"I was extraordinarily impressed by this film. It's one of the best sports films \\\n                    I've every seen. The visuals in this film are outstanding. I love the sequences \\\n                    in which the camer tracks the ball as it flies through the air or into the cup. \\\n                    The film moves well, offering both excitement and drama. The cinematography was fantastic.\\\n                    <br /><br />The acting performances are great. I was surprised by young Shia LaBeouf.\\\n                    He does well in this role. Stephen Dillane is also good as the brooding Harry Vardon. \\\n                    Peter Firth, Justin Ashforth, and Elias Koteas offer able support. \\\n                    The film is gripping and entertaining and for the first time in my \\\n                    life actually made me want to watch a golf tournament.\"]","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:27:36.832223Z","iopub.execute_input":"2022-12-27T15:27:36.832863Z","iopub.status.idle":"2022-12-27T15:27:36.838436Z","shell.execute_reply.started":"2022-12-27T15:27:36.832829Z","shell.execute_reply":"2022-12-27T15:27:36.837061Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**CUDA** (or Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing unit (GPU) for general purpose processing. **To(\"cuda\") in tokenizer step means we put the vectorized text in the GPU memory.**","metadata":{}},{"cell_type":"code","source":"#we vectorize the new movie review\nreview_token =tokenizer(new_movie_review, padding=\"max_length\", truncation=True,return_tensors=\"pt\").to(\"cuda\")\n\n#get a PyTorch tensor probability vector\nprob_pytorch_tensor = pytorch_model(**review_token )[0].softmax(1)\n\n#convert the previous PyTorch tensor into a numpy array\nprob_numpy_array = prob_pytorch_tensor.cpu().detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:28:15.996965Z","iopub.execute_input":"2022-12-27T15:28:15.997892Z","iopub.status.idle":"2022-12-27T15:28:16.018642Z","shell.execute_reply.started":"2022-12-27T15:28:15.997856Z","shell.execute_reply":"2022-12-27T15:28:16.017706Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Then, we display the predicted class of the new review","metadata":{}},{"cell_type":"code","source":"import numpy as np\nreview_sentiment = ['negatif review','positif review']\nreview_sentiment[np.argmax(prob_numpy_array)]","metadata":{"execution":{"iopub.status.busy":"2022-12-27T15:29:11.340423Z","iopub.execute_input":"2022-12-27T15:29:11.340767Z","iopub.status.idle":"2022-12-27T15:29:11.347999Z","shell.execute_reply.started":"2022-12-27T15:29:11.340737Z","shell.execute_reply":"2022-12-27T15:29:11.347071Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'positif review'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}