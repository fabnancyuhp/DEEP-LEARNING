{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabnancyuhp/DEEP-LEARNING/blob/main/NOTEBOOKS/fine-tuning-in-pytorch-with-the-trainer-api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning in PyTorch with the Trainer API\n",
        "This notebook is inspired by https://huggingface.co/blog/sentiment-analysis-python. In this notebook, we deal with a PyTorch pretrained hugging-face transformer. More precisely, we use a distilbert transformer over the IMDB dataset.<br> \n",
        "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark."
      ],
      "metadata": {
        "id": "eDqZpo0OqFq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the huggingface IMDB dataset. full_train_dataset and full_test_dataset are hugging-face datasets objects."
      ],
      "metadata": {
        "id": "iu3hNvtGqFrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install datasets\n",
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "\n",
        "full_train_dataset = raw_datasets[\"train\"].shuffle(seed=42).select([i for i in list(range(3000))])\n",
        "full_test_dataset = raw_datasets[\"test\"].shuffle(seed=42).select([i for i in list(range(300))])\n",
        "\n",
        "import gc\n",
        "del raw_datasets\n",
        "gc.collect()\n",
        "\n",
        "#full_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))\n",
        "#full_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(3000))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-27T15:17:50.742784Z",
          "iopub.execute_input": "2022-12-27T15:17:50.743149Z",
          "iopub.status.idle": "2022-12-27T15:18:24.016333Z",
          "shell.execute_reply.started": "2022-12-27T15:17:50.743116Z",
          "shell.execute_reply": "2022-12-27T15:18:24.015104Z"
        },
        "trusted": true,
        "id": "0H9ROaBOqFrC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We vectorize the movie reviews with a distilbert-base-uncased tokenizer. Then we create the training dataset and the test dataset.\n",
        " In the cell , full_train_dataset and full_test_dataset are 2 huggingface datasets."
      ],
      "metadata": {
        "id": "o0MOeTW6qFrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "full_train_dataset = full_train_dataset.map(tokenize_function, batched=True)\n",
        "full_test_dataset = full_test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-27T15:18:24.021494Z",
          "iopub.execute_input": "2022-12-27T15:18:24.022314Z",
          "iopub.status.idle": "2022-12-27T15:18:32.015816Z",
          "shell.execute_reply.started": "2022-12-27T15:18:24.022278Z",
          "shell.execute_reply": "2022-12-27T15:18:32.014684Z"
        },
        "trusted": true,
        "id": "LojTA12GqFrD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in order to fine-tune a bert huggingface model with the trainer API,\n",
        "* We have to load the transfromer model we want to use. \n",
        "* We have to set the TrainingArguments\n",
        "* We set the Trainer\n",
        "\n",
        "The model we load is paired with the tokenizer we loaded earlier in this exemple. The model in the cell below is a Pytorch model. In the case of a pretrained pytorch model we use AutoModelForSequenceClassification instead of TFAutoModelForSequenceClassification.  "
      ],
      "metadata": {
        "id": "kbWyfzfZqFrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We load the distilbert-base-uncased transfromer we want to use\n",
        "In the cell below, model is a PyTorch hugging-face model.  "
      ],
      "metadata": {
        "id": "TC-FeIeyqFrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "#pytorch_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "pytorch_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-27T15:18:39.608955Z",
          "iopub.execute_input": "2022-12-27T15:18:39.609680Z",
          "iopub.status.idle": "2022-12-27T15:18:50.410590Z",
          "shell.execute_reply.started": "2022-12-27T15:18:39.609645Z",
          "shell.execute_reply": "2022-12-27T15:18:50.409605Z"
        },
        "trusted": true,
        "id": "Jm2uBuaLqFrF",
        "outputId": "c63ab62c-a87b-425b-f1e7-ad7e77de921b",
        "colab": {
          "referenced_widgets": [
            "aed956bc9e9a43d59b54d0db516e16a4"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aed956bc9e9a43d59b54d0db516e16a4"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We set the TrainingArguments**<br>\n",
        "The Trainer API uses some training arguments we call with a TrainingArguments object. You can use the default configuration with TrainingArguments(\"test_trainer\"). Else, you can set some arguments such that:\n",
        "* output_dir='./results',     the output directory\n",
        "* num_train_epochs the total number of training epochs\n",
        "* per_device_train_batch_size the batch size per device during training\n",
        "* per_device_eval_batch_size=20 the batch size for evaluation\n",
        "* weight_decay=0.01 the strength of weight decay\n",
        "* logging_dir='./logs' the directory for storing logs"
      ],
      "metadata": {
        "id": "Awqu4XEtqFrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We set some arguments\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-27T15:21:01.067802Z",
          "iopub.execute_input": "2022-12-27T15:21:01.068456Z",
          "iopub.status.idle": "2022-12-27T15:21:01.077509Z",
          "shell.execute_reply.started": "2022-12-27T15:21:01.068416Z",
          "shell.execute_reply": "2022-12-27T15:21:01.076262Z"
        },
        "trusted": true,
        "id": "3bTPpR1QqFrG",
        "outputId": "4945e98a-f339-4cb3-cc4c-37a18e2d5434"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we use the default configuration. We change the number of epochs to 1\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\"test_trainer\")\n",
        "training_args.num_train_epochs = 1\n",
        "#training_args.per_device_eval_batch_size = 2\n",
        "#training_args.per_device_train_batch_size = 2\n",
        "#training_args"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-27T15:20:42.141502Z",
          "iopub.execute_input": "2022-12-27T15:20:42.141852Z",
          "iopub.status.idle": "2022-12-27T15:20:46.252641Z",
          "shell.execute_reply.started": "2022-12-27T15:20:42.141821Z",
          "shell.execute_reply": "2022-12-27T15:20:46.251669Z"
        },
        "trusted": true,
        "id": "6_2NPljcqFrG",
        "outputId": "e49c05b9-59cf-48f9-b79d-64fc74511e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We set the Trainer and begin the training stage with Trainer.train().**<br>\n",
        "The Trainer object takes some arguments such that:\n",
        "* model : a huggingface pretrained PyTorch model\n",
        "* args : a TrainingArguments defined earlier \n",
        "* train_dataset : a huggingface dataset made from a huggingface tokenizer step  \n",
        "* eval_datase : a huggingface dataset made from a huggingface tokenizer step\n",
        "\n",
        "The target value of a huggingface dataset used by the Trainer should always be named label else the Train API doesn't work."
      ],
      "metadata": {
        "id": "f-yYanrfqFrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(model=pytorch_model, args=training_args, train_dataset=full_train_dataset)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-27T15:22:05.228298Z",
          "iopub.execute_input": "2022-12-27T15:22:05.228648Z",
          "iopub.status.idle": "2022-12-27T15:26:13.644982Z",
          "shell.execute_reply.started": "2022-12-27T15:22:05.228617Z",
          "shell.execute_reply": "2022-12-27T15:26:13.644060Z"
        },
        "trusted": true,
        "id": "tzSeT0b5qFrH",
        "outputId": "90ce6bc3-97be-44e8-a908-52cd1b6fe2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 3000\n  Num Epochs = 3\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 564\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [564/564 04:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.355000</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Saving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nModel weights saved in ./results/checkpoint-500/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n",
          "output_type": "stream"
        },
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=564, training_loss=0.3428031130039946, metrics={'train_runtime': 243.078, 'train_samples_per_second': 37.025, 'train_steps_per_second': 2.32, 'total_flos': 1192206587904000.0, 'train_loss': 0.3428031130039946, 'epoch': 3.0})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the Trainer step, pytorch_model is a fine-tuned transformer model. Then, we can apply pytorch_model on a new movie review. We have to\n",
        "* vectorize the new movie review\n",
        "* Apply the pytorch on the vectorized new text \n",
        "* use softmax function to get a PyTorch tensor probability vector\n",
        "* convert the previous PyTorch tensor into a numpy array"
      ],
      "metadata": {
        "id": "wSDkaVkMqFrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_movie_review = [\"I was extraordinarily impressed by this film. It's one of the best sports films \\\n",
        "                    I've every seen. The visuals in this film are outstanding. I love the sequences \\\n",
        "                    in which the camer tracks the ball as it flies through the air or into the cup. \\\n",
        "                    The film moves well, offering both excitement and drama. The cinematography was fantastic.\\\n",
        "                    <br /><br />The acting performances are great. I was surprised by young Shia LaBeouf.\\\n",
        "                    He does well in this role. Stephen Dillane is also good as the brooding Harry Vardon. \\\n",
        "                    Peter Firth, Justin Ashforth, and Elias Koteas offer able support. \\\n",
        "                    The film is gripping and entertaining and for the first time in my \\\n",
        "                    life actually made me want to watch a golf tournament.\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-27T15:27:36.832223Z",
          "iopub.execute_input": "2022-12-27T15:27:36.832863Z",
          "iopub.status.idle": "2022-12-27T15:27:36.838436Z",
          "shell.execute_reply.started": "2022-12-27T15:27:36.832829Z",
          "shell.execute_reply": "2022-12-27T15:27:36.837061Z"
        },
        "trusted": true,
        "id": "ibAYZRvjqFrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CUDA** (or Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing unit (GPU) for general purpose processing. **To(\"cuda\") in tokenizer step means we put the vectorized text in the GPU memory.**"
      ],
      "metadata": {
        "id": "dS4NQzKfqFrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we vectorize the new movie review\n",
        "review_token =tokenizer(new_movie_review, padding=\"max_length\", truncation=True,return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "#get a PyTorch tensor probability vector\n",
        "prob_pytorch_tensor = pytorch_model(**review_token )[0].softmax(1)\n",
        "\n",
        "#convert the previous PyTorch tensor into a numpy array\n",
        "prob_numpy_array = prob_pytorch_tensor.cpu().detach().numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-27T15:28:15.996965Z",
          "iopub.execute_input": "2022-12-27T15:28:15.997892Z",
          "iopub.status.idle": "2022-12-27T15:28:16.018642Z",
          "shell.execute_reply.started": "2022-12-27T15:28:15.997856Z",
          "shell.execute_reply": "2022-12-27T15:28:16.017706Z"
        },
        "trusted": true,
        "id": "QmtyOPgcqFrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we display the predicted class of the new review"
      ],
      "metadata": {
        "id": "gcjgDM7VqFrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "review_sentiment = ['negatif review','positif review']\n",
        "review_sentiment[np.argmax(prob_numpy_array)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-27T15:29:11.340423Z",
          "iopub.execute_input": "2022-12-27T15:29:11.340767Z",
          "iopub.status.idle": "2022-12-27T15:29:11.347999Z",
          "shell.execute_reply.started": "2022-12-27T15:29:11.340737Z",
          "shell.execute_reply": "2022-12-27T15:29:11.347071Z"
        },
        "trusted": true,
        "id": "_MnL99FhqFrI",
        "outputId": "8304414c-6913-4c51-b942-8dd07f4ad2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'positif review'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "jHUcmGPQqFrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JBqNn8XnqFrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}